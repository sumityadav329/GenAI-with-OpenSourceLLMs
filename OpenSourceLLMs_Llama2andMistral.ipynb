{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7cbbe9e549bd49159be82cc2d5bba379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6268df3dce414dd89a447af4da09b590",
              "IPY_MODEL_3ce98b861123405fba6b5b4cb70caa2d",
              "IPY_MODEL_1183d4b3680e4f7faeac601da5f00bc0"
            ],
            "layout": "IPY_MODEL_f7bcbcc5e09e4dcba6c1c02722a5dc67"
          }
        },
        "6268df3dce414dd89a447af4da09b590": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_967e200267ac493c96188c3c8019a739",
            "placeholder": "​",
            "style": "IPY_MODEL_8ba2542b120641bfaa40d41f8f19b49a",
            "value": "config.json: 100%"
          }
        },
        "3ce98b861123405fba6b5b4cb70caa2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b24a4c94ecff467482511cd27a67602f",
            "max": 614,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_618741b0cd22419aa4e14d198867a6e4",
            "value": 614
          }
        },
        "1183d4b3680e4f7faeac601da5f00bc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df62a5c02d514fa2a53043d5024ff434",
            "placeholder": "​",
            "style": "IPY_MODEL_8128bbfb2e3542c68ac19cb04a8b237b",
            "value": " 614/614 [00:00&lt;00:00, 33.7kB/s]"
          }
        },
        "f7bcbcc5e09e4dcba6c1c02722a5dc67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "967e200267ac493c96188c3c8019a739": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ba2542b120641bfaa40d41f8f19b49a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b24a4c94ecff467482511cd27a67602f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "618741b0cd22419aa4e14d198867a6e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df62a5c02d514fa2a53043d5024ff434": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8128bbfb2e3542c68ac19cb04a8b237b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3aebbeacf5e04e13851161c911f25169": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_912c73b0844e4ce696e6898ee3472570",
              "IPY_MODEL_3deb489a669e4dbeba1a821ae1d314f0",
              "IPY_MODEL_01e3c80f29834de78859d54726383c4a"
            ],
            "layout": "IPY_MODEL_7fc4c920fe1d429a93410ab927630a1d"
          }
        },
        "912c73b0844e4ce696e6898ee3472570": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48bb0dc16ee446a0a334f00e0b247dba",
            "placeholder": "​",
            "style": "IPY_MODEL_e8a374b9bef04b22ab7049c0055684e0",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "3deb489a669e4dbeba1a821ae1d314f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7e35cd39480477e9a277d7092eca4f0",
            "max": 26788,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5204421c1ee942d9bd6552bff668612b",
            "value": 26788
          }
        },
        "01e3c80f29834de78859d54726383c4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7a0089e51c24a0ea32ebc672217df35",
            "placeholder": "​",
            "style": "IPY_MODEL_a334fc0e5e2241a6845158b23e24a4a5",
            "value": " 26.8k/26.8k [00:00&lt;00:00, 1.39MB/s]"
          }
        },
        "7fc4c920fe1d429a93410ab927630a1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48bb0dc16ee446a0a334f00e0b247dba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8a374b9bef04b22ab7049c0055684e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7e35cd39480477e9a277d7092eca4f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5204421c1ee942d9bd6552bff668612b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7a0089e51c24a0ea32ebc672217df35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a334fc0e5e2241a6845158b23e24a4a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cada55b815714a13b94d20d58271061f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_013c0bcdacc84da99870533d581c1768",
              "IPY_MODEL_e2b85ef9df43481d8c6bec58fdd92e9a",
              "IPY_MODEL_84d22453b3984b18936f2a34b34dba7d"
            ],
            "layout": "IPY_MODEL_9403214d851a4c6c8eff2dd0e46004ad"
          }
        },
        "013c0bcdacc84da99870533d581c1768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cf8174e32c1425d83bc9af0e34f4e17",
            "placeholder": "​",
            "style": "IPY_MODEL_c07220e54b694c4cb05c455ab17684a8",
            "value": "Downloading shards: 100%"
          }
        },
        "e2b85ef9df43481d8c6bec58fdd92e9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e2e6386d7724308b3b9e4d1eaded719",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5bfb3f09f3e045e38605c969e90a47be",
            "value": 2
          }
        },
        "84d22453b3984b18936f2a34b34dba7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2828b5e42f9475b9ccb7fc9fd574596",
            "placeholder": "​",
            "style": "IPY_MODEL_577f6cce8fa543e1b406de9a208cbe61",
            "value": " 2/2 [01:36&lt;00:00, 42.85s/it]"
          }
        },
        "9403214d851a4c6c8eff2dd0e46004ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cf8174e32c1425d83bc9af0e34f4e17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c07220e54b694c4cb05c455ab17684a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e2e6386d7724308b3b9e4d1eaded719": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bfb3f09f3e045e38605c969e90a47be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2828b5e42f9475b9ccb7fc9fd574596": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "577f6cce8fa543e1b406de9a208cbe61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a5612c45a3e42548c220368f036174a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_446e96d4d7214553b03b243b5f39819f",
              "IPY_MODEL_9fea9ace7fb649be888619958a5f14c4",
              "IPY_MODEL_6f03570810284c62ba6c14f680e6cc8a"
            ],
            "layout": "IPY_MODEL_6968bbb2ff434bea8be81cbe54748cba"
          }
        },
        "446e96d4d7214553b03b243b5f39819f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ecf381e226749e8bce8a4dde52c0644",
            "placeholder": "​",
            "style": "IPY_MODEL_7e15d664ca074a7ab6123d2f699f7c51",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "9fea9ace7fb649be888619958a5f14c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fa3a152edfc4801a8574086d981bcc8",
            "max": 9976576152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b1139674596f4a08b610b135f762d9ad",
            "value": 9976576152
          }
        },
        "6f03570810284c62ba6c14f680e6cc8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9590ef6708c4d98b33ef6ab09b3dccd",
            "placeholder": "​",
            "style": "IPY_MODEL_8d20c314b8f34c2cb60d51e2713da621",
            "value": " 9.98G/9.98G [01:17&lt;00:00, 186MB/s]"
          }
        },
        "6968bbb2ff434bea8be81cbe54748cba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ecf381e226749e8bce8a4dde52c0644": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e15d664ca074a7ab6123d2f699f7c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fa3a152edfc4801a8574086d981bcc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1139674596f4a08b610b135f762d9ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d9590ef6708c4d98b33ef6ab09b3dccd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d20c314b8f34c2cb60d51e2713da621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4292d1ae31be4a22bec049849b393d25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_984cf51a1f5d415cb540e5375922308d",
              "IPY_MODEL_3c75c2291d6d44bdb9ea9c8730edc699",
              "IPY_MODEL_416c279d22814cb3ac8518db4a18efbf"
            ],
            "layout": "IPY_MODEL_a7388451dcc246b191cd4af3650510d5"
          }
        },
        "984cf51a1f5d415cb540e5375922308d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33bd8f3e20484b91b263f1e7591e10d4",
            "placeholder": "​",
            "style": "IPY_MODEL_0db6b1d8b9c44200a20b5c87bb5ad2c4",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "3c75c2291d6d44bdb9ea9c8730edc699": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dad41b454414cf285fe46ad1bbabdad",
            "max": 3500296424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fcbc7cdb377f4227b3dfc5a5eac084f9",
            "value": 3500296424
          }
        },
        "416c279d22814cb3ac8518db4a18efbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e42b7a91f5db457cab7dfc89f28016d3",
            "placeholder": "​",
            "style": "IPY_MODEL_a7759ab63ace477b8778468fbdc762a8",
            "value": " 3.50G/3.50G [00:18&lt;00:00, 239MB/s]"
          }
        },
        "a7388451dcc246b191cd4af3650510d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33bd8f3e20484b91b263f1e7591e10d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0db6b1d8b9c44200a20b5c87bb5ad2c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3dad41b454414cf285fe46ad1bbabdad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcbc7cdb377f4227b3dfc5a5eac084f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e42b7a91f5db457cab7dfc89f28016d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7759ab63ace477b8778468fbdc762a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc97d12f3e814cc2a437cc5ad69699e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ddbbbc633e54d74b1581f8323435238",
              "IPY_MODEL_9485b734e8454e5582928bd94506811b",
              "IPY_MODEL_75a010d615fe41d2a2ad856b2f212e05"
            ],
            "layout": "IPY_MODEL_9e75032464c341738c6f6ea01f32887c"
          }
        },
        "5ddbbbc633e54d74b1581f8323435238": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04594ad2f37b4c0ebe133c74760cee33",
            "placeholder": "​",
            "style": "IPY_MODEL_dee14fdc02e7497ab575b06896be5efa",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9485b734e8454e5582928bd94506811b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3534393dc1234c1594d6ac84b266a1f4",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ebca62fa14de4e93b3b26f08caff5c48",
            "value": 2
          }
        },
        "75a010d615fe41d2a2ad856b2f212e05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65cee2e9c6514d93a217e2afdbb61ad8",
            "placeholder": "​",
            "style": "IPY_MODEL_1f7bbd4a3971437f85310246204eece2",
            "value": " 2/2 [00:59&lt;00:00, 27.43s/it]"
          }
        },
        "9e75032464c341738c6f6ea01f32887c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04594ad2f37b4c0ebe133c74760cee33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dee14fdc02e7497ab575b06896be5efa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3534393dc1234c1594d6ac84b266a1f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebca62fa14de4e93b3b26f08caff5c48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65cee2e9c6514d93a217e2afdbb61ad8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f7bbd4a3971437f85310246204eece2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3def84fdcf5b471695488cef56714e02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_474d8a2baf1946768c4779e8098d022c",
              "IPY_MODEL_69fd5823ab42459b827035f2c88289da",
              "IPY_MODEL_18fbc696b67149ae9a98ec2e7e57574c"
            ],
            "layout": "IPY_MODEL_5d843f4e673247acb6835227a34da926"
          }
        },
        "474d8a2baf1946768c4779e8098d022c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c444b8cded324b29aef02a5ac81995a9",
            "placeholder": "​",
            "style": "IPY_MODEL_a36e49507977421f949785cba5f8178a",
            "value": "generation_config.json: 100%"
          }
        },
        "69fd5823ab42459b827035f2c88289da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99763dc642134a1493952ad5d7c81e69",
            "max": 188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e64e06556754a49a12969dee9b64599",
            "value": 188
          }
        },
        "18fbc696b67149ae9a98ec2e7e57574c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62ec4c13bccb431290c192bb9132cfed",
            "placeholder": "​",
            "style": "IPY_MODEL_69b891e613994f4b81bda14c7f5b4b82",
            "value": " 188/188 [00:00&lt;00:00, 11.8kB/s]"
          }
        },
        "5d843f4e673247acb6835227a34da926": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c444b8cded324b29aef02a5ac81995a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a36e49507977421f949785cba5f8178a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99763dc642134a1493952ad5d7c81e69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e64e06556754a49a12969dee9b64599": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62ec4c13bccb431290c192bb9132cfed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69b891e613994f4b81bda14c7f5b4b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdee1c16a95c403c80b6c2f2fba17c79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a6c5c87564bc4713863aa7db3e128f56",
              "IPY_MODEL_dc6bf8d3c8114941908f1e4d5ad97910",
              "IPY_MODEL_29423beb8fb94e5580b8348d5742d88b"
            ],
            "layout": "IPY_MODEL_0e8a4fc8c5164977914a51e7d463bff8"
          }
        },
        "a6c5c87564bc4713863aa7db3e128f56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_194cac9e751549eea840cffdc6460149",
            "placeholder": "​",
            "style": "IPY_MODEL_2c797bc18da34b2d8a1cb30dcc9e1647",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "dc6bf8d3c8114941908f1e4d5ad97910": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1382fec77fcc4478ab6f8862a04bd6cc",
            "max": 1618,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5dbcf5586a3442da8bb30e5d2ee0adc0",
            "value": 1618
          }
        },
        "29423beb8fb94e5580b8348d5742d88b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f2c1ce527c243c0b21495e97f32aa06",
            "placeholder": "​",
            "style": "IPY_MODEL_033b67383235422eac4a43e2355722c0",
            "value": " 1.62k/1.62k [00:00&lt;00:00, 129kB/s]"
          }
        },
        "0e8a4fc8c5164977914a51e7d463bff8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "194cac9e751549eea840cffdc6460149": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c797bc18da34b2d8a1cb30dcc9e1647": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1382fec77fcc4478ab6f8862a04bd6cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dbcf5586a3442da8bb30e5d2ee0adc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f2c1ce527c243c0b21495e97f32aa06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "033b67383235422eac4a43e2355722c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6485864cad0b47c8b67e45a11fecd7d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c2d1a34dfa5402dbeb0752d76d8ee42",
              "IPY_MODEL_4cfb4c3b07e54394afec57afa06e5666",
              "IPY_MODEL_77a942b230c74004aca2906db77eaf3f"
            ],
            "layout": "IPY_MODEL_c9a5848c4fec43c39eca27da3b9f351e"
          }
        },
        "2c2d1a34dfa5402dbeb0752d76d8ee42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bad5a981e939415f8e404564e8f4ac2a",
            "placeholder": "​",
            "style": "IPY_MODEL_e7b206c9aed140adb9cfd0aeab61d258",
            "value": "tokenizer.model: 100%"
          }
        },
        "4cfb4c3b07e54394afec57afa06e5666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_772b43333c98428d997ea11fe8096731",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d2bdf3790b03437eaf5a014112ae0d76",
            "value": 499723
          }
        },
        "77a942b230c74004aca2906db77eaf3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_232ad7113fab4522a4edd8e80e4b15e4",
            "placeholder": "​",
            "style": "IPY_MODEL_3aa6ed29a45a44eb9cfaade19b8af43e",
            "value": " 500k/500k [00:00&lt;00:00, 23.7MB/s]"
          }
        },
        "c9a5848c4fec43c39eca27da3b9f351e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bad5a981e939415f8e404564e8f4ac2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7b206c9aed140adb9cfd0aeab61d258": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "772b43333c98428d997ea11fe8096731": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2bdf3790b03437eaf5a014112ae0d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "232ad7113fab4522a4edd8e80e4b15e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3aa6ed29a45a44eb9cfaade19b8af43e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2ec137769c54dc38469ca1bba8f0809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e069d93550a44492ac741c66461aacf9",
              "IPY_MODEL_76727604503f4217ad482b10f88f2dd1",
              "IPY_MODEL_6937fdce33d940c09fdda69dd0dc7b9d"
            ],
            "layout": "IPY_MODEL_d6f79736397344438c5fa905abdec69e"
          }
        },
        "e069d93550a44492ac741c66461aacf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8107738c03bd4970861460d868d4c300",
            "placeholder": "​",
            "style": "IPY_MODEL_cf4d7c1070e649c5ad04fdcbfe48cbb2",
            "value": "tokenizer.json: 100%"
          }
        },
        "76727604503f4217ad482b10f88f2dd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_787a792e2cec449294db6e8a428be186",
            "max": 1842767,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4391f9ef18140bea154cfb1035ce974",
            "value": 1842767
          }
        },
        "6937fdce33d940c09fdda69dd0dc7b9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2c125cb343c41829e5404e73658250a",
            "placeholder": "​",
            "style": "IPY_MODEL_78158c4c10074bd38d3c73b1e04b1ab8",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 5.71MB/s]"
          }
        },
        "d6f79736397344438c5fa905abdec69e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8107738c03bd4970861460d868d4c300": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf4d7c1070e649c5ad04fdcbfe48cbb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "787a792e2cec449294db6e8a428be186": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4391f9ef18140bea154cfb1035ce974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2c125cb343c41829e5404e73658250a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78158c4c10074bd38d3c73b1e04b1ab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fe9a1c3c9814bfb817f1d1cbeae3f04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26b0d1dbdaf645d28b7eaac2d2d923b0",
              "IPY_MODEL_5de6998ffabc4ba5b73fafdc0b4ffee1",
              "IPY_MODEL_6f392d5b548b4c2693f283a850f6d952"
            ],
            "layout": "IPY_MODEL_4418d4774e4449f197fc614404dd49a1"
          }
        },
        "26b0d1dbdaf645d28b7eaac2d2d923b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f96f1b791e74b9fab331df15d8b9ad3",
            "placeholder": "​",
            "style": "IPY_MODEL_c163db1b52994e318ed80cc93082e559",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "5de6998ffabc4ba5b73fafdc0b4ffee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c77de697fbd44432a918608f8fce42c7",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8aa2a040e9464b2383d93804b4b81071",
            "value": 414
          }
        },
        "6f392d5b548b4c2693f283a850f6d952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8b84922d40548319b4e387fbd88ee2a",
            "placeholder": "​",
            "style": "IPY_MODEL_f4ab8ce4242b4993be21a4ae136db94e",
            "value": " 414/414 [00:00&lt;00:00, 23.8kB/s]"
          }
        },
        "4418d4774e4449f197fc614404dd49a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f96f1b791e74b9fab331df15d8b9ad3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c163db1b52994e318ed80cc93082e559": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c77de697fbd44432a918608f8fce42c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aa2a040e9464b2383d93804b4b81071": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8b84922d40548319b4e387fbd88ee2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4ab8ce4242b4993be21a4ae136db94e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bd9e18eb2c348f8949811e1a242a7c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_155476404cda4347bdcb948827ffe309",
              "IPY_MODEL_983e589cf3134df98b9cdf84a7bb8d29",
              "IPY_MODEL_0ecd28eafb8943aab07b8f9f7d1767da"
            ],
            "layout": "IPY_MODEL_6caf33a8567f4c818b0ed9ff1a10e4bd"
          }
        },
        "155476404cda4347bdcb948827ffe309": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53f0f0c8af584db0a2c1fea9f4b4c9f9",
            "placeholder": "​",
            "style": "IPY_MODEL_3a2dafff10814f67a0ff42823ed0f89b",
            "value": "modules.json: 100%"
          }
        },
        "983e589cf3134df98b9cdf84a7bb8d29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e75a4316c2834663ba59d1624df86f9a",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d731d9f70aeb437298dd24e968a2c19a",
            "value": 349
          }
        },
        "0ecd28eafb8943aab07b8f9f7d1767da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d812a71e5c441c88f7061c55ca33ba9",
            "placeholder": "​",
            "style": "IPY_MODEL_a9777758358041efbcf41daa738df0cd",
            "value": " 349/349 [00:00&lt;00:00, 13.6kB/s]"
          }
        },
        "6caf33a8567f4c818b0ed9ff1a10e4bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53f0f0c8af584db0a2c1fea9f4b4c9f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a2dafff10814f67a0ff42823ed0f89b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e75a4316c2834663ba59d1624df86f9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d731d9f70aeb437298dd24e968a2c19a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d812a71e5c441c88f7061c55ca33ba9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9777758358041efbcf41daa738df0cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad87365f8e4846568932b206aa994eee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4fcc4f722b5d43b7aadb4e936070be9c",
              "IPY_MODEL_08e9f66a33d94b7b9f980f2163f184d0",
              "IPY_MODEL_32aff72d3a7643c280547cd3d2250c22"
            ],
            "layout": "IPY_MODEL_a0217e03e52f4c3b80b6be3972e4015d"
          }
        },
        "4fcc4f722b5d43b7aadb4e936070be9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_228b3349e73146a68a25bd225e342fb0",
            "placeholder": "​",
            "style": "IPY_MODEL_faae43aa8c074283be7bc55631de6884",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "08e9f66a33d94b7b9f980f2163f184d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5db2e952aa044ca9962db2200b126683",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23d8266addf4486ca3617e7a64ff5db7",
            "value": 116
          }
        },
        "32aff72d3a7643c280547cd3d2250c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5de15bb3e56b4a9bb24c7b80dff8cf6f",
            "placeholder": "​",
            "style": "IPY_MODEL_acb54cb6d612400282037daa0f7d286e",
            "value": " 116/116 [00:00&lt;00:00, 7.65kB/s]"
          }
        },
        "a0217e03e52f4c3b80b6be3972e4015d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "228b3349e73146a68a25bd225e342fb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faae43aa8c074283be7bc55631de6884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5db2e952aa044ca9962db2200b126683": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23d8266addf4486ca3617e7a64ff5db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5de15bb3e56b4a9bb24c7b80dff8cf6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acb54cb6d612400282037daa0f7d286e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e212c4bcec0f4edbbfb2e1d27a382424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e751f565c48c45858af4901822aaaa70",
              "IPY_MODEL_cb37b3dd008d40a890758a48702209de",
              "IPY_MODEL_f85230f3fec241e086ad0e5129644761"
            ],
            "layout": "IPY_MODEL_7f06a7e46e474c5d959be8215d34c113"
          }
        },
        "e751f565c48c45858af4901822aaaa70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88754644da1d43af9c959acb5def0897",
            "placeholder": "​",
            "style": "IPY_MODEL_8e77b1cab64c43689051d4ce76469d61",
            "value": "README.md: 100%"
          }
        },
        "cb37b3dd008d40a890758a48702209de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e08aa031db1149d6a733bee403df8e9a",
            "max": 10621,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e5e37fb980e490094c08f9aebd2e493",
            "value": 10621
          }
        },
        "f85230f3fec241e086ad0e5129644761": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf967e6723df4c608fd12fda9be8d2e4",
            "placeholder": "​",
            "style": "IPY_MODEL_77505cbfa9a24754b20a88118e223007",
            "value": " 10.6k/10.6k [00:00&lt;00:00, 512kB/s]"
          }
        },
        "7f06a7e46e474c5d959be8215d34c113": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88754644da1d43af9c959acb5def0897": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e77b1cab64c43689051d4ce76469d61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e08aa031db1149d6a733bee403df8e9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e5e37fb980e490094c08f9aebd2e493": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf967e6723df4c608fd12fda9be8d2e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77505cbfa9a24754b20a88118e223007": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cf91330c0f740be9b984c3b0d16854a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ea2eef481f64176ad195ede7c1d0db4",
              "IPY_MODEL_b543ed67fb0b47e9a7990214dfbe6457",
              "IPY_MODEL_d17172d8c3784b45a5311d6fd79b4f47"
            ],
            "layout": "IPY_MODEL_104a156af21048b09d8fb8bcd2d1e529"
          }
        },
        "3ea2eef481f64176ad195ede7c1d0db4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6e19febd35040a693cad645a40b5673",
            "placeholder": "​",
            "style": "IPY_MODEL_eef0f9396a97492db8c02094ba9d17c3",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "b543ed67fb0b47e9a7990214dfbe6457": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f02de5225ecc4c198c7da7a2136c9a47",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_64e50042a9124a94af05c47b0c91f5a8",
            "value": 53
          }
        },
        "d17172d8c3784b45a5311d6fd79b4f47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3594d498c6054721a9b470dc03c357ac",
            "placeholder": "​",
            "style": "IPY_MODEL_1b4387d5eb7b47508ca909600bdc863f",
            "value": " 53.0/53.0 [00:00&lt;00:00, 2.66kB/s]"
          }
        },
        "104a156af21048b09d8fb8bcd2d1e529": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6e19febd35040a693cad645a40b5673": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eef0f9396a97492db8c02094ba9d17c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f02de5225ecc4c198c7da7a2136c9a47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64e50042a9124a94af05c47b0c91f5a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3594d498c6054721a9b470dc03c357ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b4387d5eb7b47508ca909600bdc863f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27d97baa684e4372ac6ddb565310a678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4740ae97df8f4539978ccac20bd01009",
              "IPY_MODEL_4553b20d654243c89c490e7160d7b019",
              "IPY_MODEL_b9fce70d534840bdbaf5f3cd12e9c4ad"
            ],
            "layout": "IPY_MODEL_fdd170312e0c403899c22bc8fcfddba8"
          }
        },
        "4740ae97df8f4539978ccac20bd01009": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_731c89aef8d840bdbd27287da37e6832",
            "placeholder": "​",
            "style": "IPY_MODEL_22ff04166293409a8eaf9fe3230a8781",
            "value": "config.json: 100%"
          }
        },
        "4553b20d654243c89c490e7160d7b019": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63fbfd3180084e69a3cd2f6787e65bf2",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9056e8a331a1406aa3bbd5d843b31c9a",
            "value": 571
          }
        },
        "b9fce70d534840bdbaf5f3cd12e9c4ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b25f69b3a2e429089f58b65a4c9ad86",
            "placeholder": "​",
            "style": "IPY_MODEL_d5e429ded0bf4d479d844a030f52cec1",
            "value": " 571/571 [00:00&lt;00:00, 33.4kB/s]"
          }
        },
        "fdd170312e0c403899c22bc8fcfddba8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "731c89aef8d840bdbd27287da37e6832": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22ff04166293409a8eaf9fe3230a8781": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63fbfd3180084e69a3cd2f6787e65bf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9056e8a331a1406aa3bbd5d843b31c9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b25f69b3a2e429089f58b65a4c9ad86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5e429ded0bf4d479d844a030f52cec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba49cfb7b61a488da1da7df01ff84a2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_545ddf85ff9c4500b95c1642f2498c6f",
              "IPY_MODEL_8c5ab7ad4aaf44f99ed9bd8b97ec1290",
              "IPY_MODEL_96f7ddbfe4d641c89dd97878c2f7aa75"
            ],
            "layout": "IPY_MODEL_5d95bb888b6d4e839b8e82104cc10b58"
          }
        },
        "545ddf85ff9c4500b95c1642f2498c6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_588398204dfd4533af06587f4f372b8e",
            "placeholder": "​",
            "style": "IPY_MODEL_a50bc4918d7c4d2abd5511aeace5d2ae",
            "value": "model.safetensors: 100%"
          }
        },
        "8c5ab7ad4aaf44f99ed9bd8b97ec1290": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c791822afe944b486285b380a9cff0b",
            "max": 437971872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7578bbff24f48a0860a82512185166b",
            "value": 437971872
          }
        },
        "96f7ddbfe4d641c89dd97878c2f7aa75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1644898f5b9241b8bda8d3b9fe690b27",
            "placeholder": "​",
            "style": "IPY_MODEL_442d80fdeb5047c6a9f83acb18d0f3f6",
            "value": " 438M/438M [00:01&lt;00:00, 227MB/s]"
          }
        },
        "5d95bb888b6d4e839b8e82104cc10b58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "588398204dfd4533af06587f4f372b8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a50bc4918d7c4d2abd5511aeace5d2ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c791822afe944b486285b380a9cff0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7578bbff24f48a0860a82512185166b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1644898f5b9241b8bda8d3b9fe690b27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "442d80fdeb5047c6a9f83acb18d0f3f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f7f39ba00dc443fb1513a1bbeb96594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_57b09635215943bfb0563c791d9797e4",
              "IPY_MODEL_83c01cb8b0fb45abab66c2a2fe4e48ee",
              "IPY_MODEL_56048865724441e3afcdcab2db05bc34"
            ],
            "layout": "IPY_MODEL_23e60f766a8b48aaa4fdb4b8b17775a7"
          }
        },
        "57b09635215943bfb0563c791d9797e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dda43013c1942cfabcbeb2aa3ba9a1e",
            "placeholder": "​",
            "style": "IPY_MODEL_c6ad5b0c5ee7435595015da8aab8a9e3",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "83c01cb8b0fb45abab66c2a2fe4e48ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a22eb13d7d824f8990a96ea4983dc9ff",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d549b393fcc42cba337e5e3b3684719",
            "value": 363
          }
        },
        "56048865724441e3afcdcab2db05bc34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc6c1ab08a49427a82a41c9338881dec",
            "placeholder": "​",
            "style": "IPY_MODEL_e5489d19fe934321b728d538299666ab",
            "value": " 363/363 [00:00&lt;00:00, 28.2kB/s]"
          }
        },
        "23e60f766a8b48aaa4fdb4b8b17775a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dda43013c1942cfabcbeb2aa3ba9a1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6ad5b0c5ee7435595015da8aab8a9e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a22eb13d7d824f8990a96ea4983dc9ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d549b393fcc42cba337e5e3b3684719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc6c1ab08a49427a82a41c9338881dec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5489d19fe934321b728d538299666ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c64a4de49b92439d89e87079899b9727": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e111bdec0c0a44009090cb4e4ba27a1f",
              "IPY_MODEL_e917a82641b745f1abc7da6fcf9ae82a",
              "IPY_MODEL_8d08c8fb1ee94cd3809715a672e30299"
            ],
            "layout": "IPY_MODEL_173ce66019b2437684040c3810f92f9e"
          }
        },
        "e111bdec0c0a44009090cb4e4ba27a1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ea5f982d6784fec889e0b07eb6f2cb6",
            "placeholder": "​",
            "style": "IPY_MODEL_a21673aed82a4d169c8d6a92f4354acd",
            "value": "vocab.txt: 100%"
          }
        },
        "e917a82641b745f1abc7da6fcf9ae82a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b31d5eddd2a84606a57f085a6b948ae3",
            "max": 231536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_31df46a871bd4770adc513726fe6a27b",
            "value": 231536
          }
        },
        "8d08c8fb1ee94cd3809715a672e30299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ddd001fca7d4bdb9220ecf8c9185899",
            "placeholder": "​",
            "style": "IPY_MODEL_07af7b4353f848d785e80af2fe5dbcf1",
            "value": " 232k/232k [00:00&lt;00:00, 1.83MB/s]"
          }
        },
        "173ce66019b2437684040c3810f92f9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ea5f982d6784fec889e0b07eb6f2cb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a21673aed82a4d169c8d6a92f4354acd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b31d5eddd2a84606a57f085a6b948ae3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31df46a871bd4770adc513726fe6a27b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ddd001fca7d4bdb9220ecf8c9185899": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07af7b4353f848d785e80af2fe5dbcf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c0d98d7696d42d7a904db8c9136266b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad7aea75b60145c798f0ed55948c1533",
              "IPY_MODEL_31bda88745b942e49d21a43c824589cc",
              "IPY_MODEL_19e1d58bb441401f83136d049cf08264"
            ],
            "layout": "IPY_MODEL_6dee69b6333e4462a30f98f174f41b75"
          }
        },
        "ad7aea75b60145c798f0ed55948c1533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dd183a012a94cabb8bc917819539dab",
            "placeholder": "​",
            "style": "IPY_MODEL_d0f17f57ee1e4df4b4b4770c8981a7eb",
            "value": "tokenizer.json: 100%"
          }
        },
        "31bda88745b942e49d21a43c824589cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee9096b7be6a43dd8e33595e1a4b0e80",
            "max": 466021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d46304da37c243faa159f008acbb058e",
            "value": 466021
          }
        },
        "19e1d58bb441401f83136d049cf08264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_570e6e2c317d4a04a6e3c25f0488cbcd",
            "placeholder": "​",
            "style": "IPY_MODEL_7e2a90712afa4687a85f2c343595a6fd",
            "value": " 466k/466k [00:00&lt;00:00, 19.6MB/s]"
          }
        },
        "6dee69b6333e4462a30f98f174f41b75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dd183a012a94cabb8bc917819539dab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0f17f57ee1e4df4b4b4770c8981a7eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee9096b7be6a43dd8e33595e1a4b0e80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d46304da37c243faa159f008acbb058e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "570e6e2c317d4a04a6e3c25f0488cbcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e2a90712afa4687a85f2c343595a6fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e0e92deaccb4a28bac2b96aa8365a29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8e014e09b2549108002c9777ba3e628",
              "IPY_MODEL_b430b09dd8184fc8a62992c5427caf0f",
              "IPY_MODEL_511a5d77019042b1aa9f9a2631b6da1a"
            ],
            "layout": "IPY_MODEL_76eef1897f3e422eb4c00631d92955cf"
          }
        },
        "e8e014e09b2549108002c9777ba3e628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9be4914e5cfa4f54a4d5ac19018deb8c",
            "placeholder": "​",
            "style": "IPY_MODEL_505019e0f70141ef8b9ff40800fae0ec",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "b430b09dd8184fc8a62992c5427caf0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f809021de5d4a82908d5e65b71b358c",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_140e288669c14d1e93c3cba403310a0c",
            "value": 239
          }
        },
        "511a5d77019042b1aa9f9a2631b6da1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e07e002ddbf6450791a866f434794b77",
            "placeholder": "​",
            "style": "IPY_MODEL_b6bc9de3b8994b0ea9a1851f6ccb7894",
            "value": " 239/239 [00:00&lt;00:00, 12.1kB/s]"
          }
        },
        "76eef1897f3e422eb4c00631d92955cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9be4914e5cfa4f54a4d5ac19018deb8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "505019e0f70141ef8b9ff40800fae0ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f809021de5d4a82908d5e65b71b358c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "140e288669c14d1e93c3cba403310a0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e07e002ddbf6450791a866f434794b77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6bc9de3b8994b0ea9a1851f6ccb7894": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a301f58049e44206803949c03151cc3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d754e5e06fbf4b4091af56e97d3abc88",
              "IPY_MODEL_2d63f13124024f4dafd17af590b16dd6",
              "IPY_MODEL_2790bd7501e741fba96e489ab2bc7ba5"
            ],
            "layout": "IPY_MODEL_b341540b8aaf4c59925d6c34c3506751"
          }
        },
        "d754e5e06fbf4b4091af56e97d3abc88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00d5bb7967974585a861fe943b00d6e5",
            "placeholder": "​",
            "style": "IPY_MODEL_00f993bcb1fe4fb3ba2df34f024daab0",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "2d63f13124024f4dafd17af590b16dd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59a44eb97ff046cda2a2cb1879d4a6d4",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c6d211edd834529b0a030ae5affcaee",
            "value": 190
          }
        },
        "2790bd7501e741fba96e489ab2bc7ba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84e28c346c8840bbbc9a27da72f5130e",
            "placeholder": "​",
            "style": "IPY_MODEL_67da5cb7da714ab59323f2ceb8213f9f",
            "value": " 190/190 [00:00&lt;00:00, 10.0kB/s]"
          }
        },
        "b341540b8aaf4c59925d6c34c3506751": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00d5bb7967974585a861fe943b00d6e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00f993bcb1fe4fb3ba2df34f024daab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59a44eb97ff046cda2a2cb1879d4a6d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c6d211edd834529b0a030ae5affcaee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84e28c346c8840bbbc9a27da72f5130e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67da5cb7da714ab59323f2ceb8213f9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7JdGQgMQl1M",
        "outputId": "2b87d63c-58af-489b-c35e-14e670126793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/290.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/290.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n",
            "Installing collected packages: pypdf\n",
            "Successfully installed pypdf-4.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers einops accelerate langchain bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy5jtw83SWNf",
        "outputId": "8f06abcc-2300-44ba-9b47-394ff7d5267f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m722.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdFjur9BSQ0J",
        "outputId": "8a2137ef-4252-4e07-df2a-1c3d0bfe3c55"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/171.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.40.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: sentence_transformers\n",
            "Successfully installed sentence_transformers-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index==0.9.39"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv_f1QZ_SQw4",
        "outputId": "1a1884a5-4773-431d-b328-23c07c34d1ad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index==0.9.39\n",
            "  Downloading llama_index-0.9.39-py3-none-any.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.39) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.39) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.39) (0.6.5)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index==0.9.39)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.39) (2023.6.0)\n",
            "Collecting httpx (from llama-index==0.9.39)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.39) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.39) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.39) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.39) (1.25.2)\n",
            "Collecting openai>=1.1.0 (from llama-index==0.9.39)\n",
            "  Downloading openai-1.25.0-py3-none-any.whl (312 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.9/312.9 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.39) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.39) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.39) (8.2.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index==0.9.39)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.39) (4.11.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.39) (0.9.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.39) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.39) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.39) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.39) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.39) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.39) (4.0.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index==0.9.39) (1.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.39) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.39) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.39) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.39) (4.66.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index==0.9.39) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index==0.9.39) (1.7.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index==0.9.39) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index==0.9.39) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index==0.9.39) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx->llama-index==0.9.39)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index==0.9.39) (3.7)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index==0.9.39)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index==0.9.39) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index==0.9.39) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index==0.9.39) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index==0.9.39) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index==0.9.39) (3.21.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index==0.9.39) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index==0.9.39) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index==0.9.39) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index==0.9.39) (1.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index==0.9.39) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index==0.9.39) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index==0.9.39) (2.18.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index==0.9.39) (1.16.0)\n",
            "Installing collected packages: h11, deprecated, tiktoken, httpcore, httpx, openai, llama-index\n",
            "Successfully installed deprecated-1.2.14 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 llama-index-0.9.39 openai-1.25.0 tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "from llama_index.prompts.prompts import SimpleInputPrompt"
      ],
      "metadata": {
        "id": "U-H57dgdSQtY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir Data\n"
      ],
      "metadata": {
        "id": "J0hMcxuHSQqQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(\"/content/Data/\").load_data()"
      ],
      "metadata": {
        "id": "srF12w4gSQmt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-V0J6jF-SQjN",
        "outputId": "3c40e3df-bad7-459f-e2e9-4caeebf02c4e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id_='54423cf5-7243-4661-8152-af8843f3290e', embedding=None, metadata={'page_label': '1', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n \\n \\n \\n \\n \\nData  Science  \\nInterview  Questions  \\n(30 days of Interview  Preparation)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0bacb366-d04c-4243-901d-f7140d913c5e', embedding=None, metadata={'page_label': '2', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INEURON.AI  \\n \\n Page 2 \\n  \\nQ1.  What is the difference between AI,  Data  Science , ML, and DL ? \\n \\nAns 1 :  \\n         \\n  \\n \\nArtificial Intelligence : AI is purely math and scientific exercis e, but when it became computa tional , it \\nstarted to solve human problems formalized into a subset of computer science. A rtificial intelligence has \\nchanged the original computational statistics paradigm to the modern idea that machines could mimic \\nactual human capabilities, such as deci sion making and perfo rming more “human” tasks. Modern AI into \\ntwo categories  \\n1. General AI - Planning, decision making, identifying objects, recognizing sounds, social & \\nbusiness transactions  \\n2. Applied AI - driverless/ Autonomous car or machine smartly trade st ocks \\n \\nMachine Learning : Instead of engineers “teaching” or programming computers to have what they need \\nto carry out tasks, that perhaps computers could teach themselves – learn something without being \\nexplicitly programmed to do so. ML is a form of AI where  based on more data , and they can change \\nactions and response, which will make more effic ient, ad aptable and scalable. e .g., navigation apps and \\nrecommendation engine s. Classified into: - \\n1. Supervised  \\n2. Unsupervised  \\n3. Reinforcement learning  \\n \\nData Science : Data science ha s many tools, techniques, and algorithms called from these fields, plus \\nothers –to handle big data  \\nThe goal of data science, somewhat similar to machine learning, is to make accurate predictions and to \\nautomate and perform transactions in real -time, such as purchasing internet traffic or automatically \\ngenerating content.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='01ef3fba-4a2b-40e7-9ab9-7b9c7769865f', embedding=None, metadata={'page_label': '3', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INEURON.AI  \\n \\n Page 3 \\n Data science relies less on math and coding and more on data and building new systems to process the \\ndata. Relying on the fields of data integration, distributed architecture, aut omated machine learning, data \\nvisualization, data engineering, and automated data -driven decisions, data science can cover an entire \\nspectrum of data processing, not only the algorithms or statistics related to data.  \\n \\nDeep Learning : It is a technique for i mplementing ML.  \\nML provides the desired output from a given input , but DL reads the input and applies it to another data.  \\nIn ML , we can easily classify the flower  based upon the features . Suppose you want a machine to look at \\nan image and determine what  it represents to the human eye , whether a face, flower, landscape, truck, \\nbuilding, etc.  \\nMachine learning is not sufficient for this task because machine learning can only produce an output from \\na data set – whether according to a known algorithm or based on the inherent structure of the data. Y ou \\nmight be able to use machine learning to determine whether an image was of an “X” – a flower, say – and \\nit would learn and get more accurate. But that output is binary (yes/no) and is dependent on the \\nalgorithm, not the data. In the image recognition ca se, the outcome is not binary and not dependent on \\nthe algorithm.  \\nThe n eural network performs MICRO calculations with computational on many layers . Neural networks \\nalso support weighting data for ‘confidence . These results in a probabilistic system , vs. deterministic, and \\ncan handle tasks that we think of as requiring more ‘human -like’ judg ment.  \\n \\n \\nQ2. What is the difference  between Supervised learning, U nsupervised  learning and \\nReinforcement  learning ? \\n \\nAns 2 :  \\nMachine Learning  \\nMachine learning is the scient ific study of algorithms and statistical models that computer systems use to \\neffectively perform a specific task without using explicit instructions, relying on patterns and inference \\ninstead.  \\nBuilding a model by learning the patterns of historical data wi th some relationship between data to make \\na data -driven prediction.  \\n \\nTypes of Machine Learning  \\n• Supervised Learning  \\n• Unsupervised Learning  \\n• Reinforcement Learning  \\n \\nSupervised learning  \\nIn a supervised learning model, the algorithm learns on a labe led da taset, to generate reasonable \\npredictions for the response to new data.  (Forecasting outcome of new data)  \\n• Regression  \\n• Classification  \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0163590c-d892-40e8-a8c4-92e397c2d8d1', embedding=None, metadata={'page_label': '4', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"INEURON.AI  \\n \\n Page 4 \\n  \\nUnsupervised learning  \\nAn unsupervised model, in contrast , provides unlabelled data that the algorithm tries to make sens e of by \\nextracting features, co -occur rence and underlying patterns on its own. We use unsupervised learning for  \\n• Clustering  \\n• Anomaly detection  \\n• Association  \\n• Autoencoders  \\nReinforcement Learning  \\nReinforcement learning is less supervised and depends on  the learning agent in determining the output \\nsolutions by arriving at different possible ways to achieve the best possible solution.  \\n \\nQ3. Describe the general architecture of Machine learning . \\n \\n          \\n  \\n \\n \\nBusiness understanding : Understand the give use cas e, and also , it's good to know more about the \\ndomain for which the use cases are buil t. \\n \\nData Acquisition and Understanding : Data gathering from different sources and understanding the \\ndata. Cleaning the data , handling the missing data if any , data wrangli ng, and EDA( Exp loratory data \\nanalysis) . \\n \\n \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b1d9177b-a4a5-44db-9b3d-558c1e1e8863', embedding=None, metadata={'page_label': '5', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"INEURON.AI  \\n \\n Page 5 \\n Modeling:  Feature Engineering  - scaling the data , feature selection - not all feature s are important. We \\nuse the backward elimination method , correlation factors, PCA and domain knowledge to select the \\nfeatures.  \\nModel Training  based on trial and error method or by experience , we select the al gorithm and train with \\nthe selected features.  \\nModel evaluation  Accuracy of the model , confusion matrix and cross -validation.  \\nIf accuracy is not high , to achi eve higher accuracy , we tune  the model...either by changing the algo rithm \\nused or by feature selection or by g athering more data , etc. \\nDeployment  - Once the model has good accuracy , we deplo y the model either in the cloud or Rasberry \\npy or any other place. Once we deploy , we monitor the performa nce of the model.if its good...we go live \\nwith the model or re iterate the all process unti l our model performa nce is good.  \\nIt's not done yet!!!  \\nWhat if , after a few day s, our model performs bad ly because of new data . In that case , we do all the \\nprocess a gain by collecting new data and redeploy the model.  \\n \\nQ4. What is Linear Regression ? \\n \\nAns 4: \\nLinear Regression tends to establish a relationship between a depend ent variable(Y) and one or more \\nindepend ent variable(X) by finding the best fit of the straight line.  \\nThe equation for the Linear model is Y = mX+c, where m is the slope and c is the intercept  \\n \\n \\nIn the above diagram, the blue dots we see are the distribution of 'y' w.r.t 'x .' There is no straight line that \\nruns through all the data points. So, the objective here is to fit the best fit of a straight line that will try to \\nminimi ze the error between the expected and actual value . \\n \\n \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1103957e-5d74-4758-b2da-05017cf08cd4', embedding=None, metadata={'page_label': '6', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"INEURON.AI  \\n \\n Page 6 \\n  \\nQ5. OLS Stats Model (Ordinary Least Square)  \\n \\nAns 5: \\nOLS is a stats model, which will help us in identifying the more significan t features that can has an \\ninfluence on the ou tput. OLS model in python is  executed  as: \\nlm = smf.ols(formula = 'Sales ~ am+constant', data = data).fit() lm.conf_int() lm.summary()  \\nAnd we get the output as below,  \\n \\n \\nThe higher the t -value for the feature, the more significant the feature is to the output variable . And \\nalso, the p -value plays a rule in rejecting the Null hypothesis(Null hypothesis stating the features has zero \\nsignificance on the target variable.).  If the p -value is less than 0.05(95% confidence interval) for a \\nfeature, then we  can consider the feature to be significant.  \\n \\n \\nQ6. What is L1 Regularization (L1 = lasso)  ? \\n \\nAns 6: \\nThe main objective of creating a model(training data) is mak ing sure it fits the data properly and reduce \\nthe loss. Sometimes the model that is trained which will fit the data b ut it may fail and give a poor \\nperformance during analyzing of data (test data) . This leads to overfitting. Regularization came to \\novercome overfitting.  \\nLasso Regression ( Least Absolute Shrinkage and Selection Operator ) adds “ Absolute value of \\nmagnit ude” of coefficient , as penalty term to the loss function.  \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2d8b05f6-98ca-4d54-9eaa-70eb7a0b000a', embedding=None, metadata={'page_label': '7', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INEURON.AI  \\n \\n Page 7 \\n Lasso shrinks the less important feature’s coefficient to zero ; thus, removing some feature altogether. So, \\nthis works well for feature selection in case we have a huge number of features.  \\n \\n \\n \\nMethods like Cross-validation, Stepwise Regression are there  to handle overfitting  and perform feature \\nselection work well with a small set of features . These techniques are  good when we are dealing with a \\nlarge set of features.  \\nAlong with sh rinking coefficients,  the lasso performs feature selection , as well. (Remember the \\n‘selection‘ in the lasso full -form?) Because some of the coefficients become exactly zero, which is \\nequivalent to the particular feature being excluded from the model.  \\n \\nQ7. L2 Re gularization(L2 = Ridge Regression)  \\n \\nAns 7: \\n \\n \\nOverfitting happens when the model learns signal as well as noise in the training data and wouldn’t \\nperform well on new/unseen data on which model wasn’t trained on.  \\nTo avoid overfitting your model on training data like  cross -validation sampling , reducing the number \\nof features, pruning , regularization , etc. \\nSo to  avoid overfitting , we perform  Regularization . \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4b7ba45a-2fe5-459d-b402-3c63bad6970a', embedding=None, metadata={'page_label': '8', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INEURON.AI  \\n \\n Page 8 \\n  \\n \\nThe Regression model that uses  L2 regularization is called Ridge Regression.  \\nThe f ormula for Ridge Regression :-  \\n \\n \\nRegula rization adds the penalty as model complexity increases. The r egularization parameter \\n(lambda) penalizes all the parameters except intercept so that the model generalizes the data and \\nwon’t overfit.  \\nRidge regression adds  “squared magnitude of the coefficient \" as penalty term to the loss \\nfunction. Here the box part in the above image represents the L2 regularization element/term.  \\n \\n \\nLambda is a hyperparameter.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2dcd3952-ba7c-423e-9b94-d6d0e278b766', embedding=None, metadata={'page_label': '9', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INEURON.AI  \\n \\n Page 9 \\n If lambda is zero , then it is equivalent to OLS. But if lambda is very large , then it will add too much \\nweight , and it will lead to under -fitting . \\nRidge regularization  forces the weights to be small but does not make them zero  and does no t give \\nthe sparse solution . \\nRidge is not robust to outliers  as square terms blow  up the error differences of the outlie rs, and the \\nregularization term tries to fix it by penalizing the weights  \\nRidge regression performs better when all the input features influence the output , and all with  weights \\nare of roughly equal size . \\nL2 regularization can learn complex data patte rns. \\n \\nQ8. What is R square(where to use and where not) ? \\nAns 8. \\nR-squared  is a statistical measure of how close the data are to the fitted regression line. It is also \\nknown as the  coefficient of determination, or the coefficient of multiple determination for multiple \\nregre ssion.  \\nThe definition of R -squared is the  percentage of the response variable variation that is explained by a \\nlinear model.  \\nR-squared = Explained variation / Total variation  \\nR-squared is always between 0 and 100%.  \\n0% indicates that the model explains none  of the variability of the response data around its mean.  \\n100% indicates that the model explains all the variability of the response data around its mean.  \\nIn general, the higher the R -squared, the better the model fits your data.  \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='005a8b00-04f6-4953-8599-4122a13f0826', embedding=None, metadata={'page_label': '10', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INEURON.AI  \\n \\n Page 10 \\n There is a  problem w ith the R -Square. The problem arises when we ask this question to ourselves.** Is it \\ngood to help as many independent variables as possible?**  \\nThe answer is No  because we understood that each independent variable should have a meaningful \\nimpact. But, even**  if we add independent variables which are not meaningful**, will it improve R -Square \\nvalue?  \\nYes, this is the basic problem with R -Square. How many junk independent variables or important \\nindependent variable or impactful independent variable you add to y our model, the R -Squared value will \\nalways increase. It will never decrease with the addition of a newly independent variable , whether it could \\nbe an impactful, non -impactful , or bad variable, so we need another way to measure equivalent R -\\nSquare , which penalizes our model with any junk independent variable.   \\nSo, we calculate the  Adjusted R -Square  with a better adjustment in the formula of generic R -square.  \\n \\n \\nQ9. What is Mean Square  Error? \\nThe mean squared error tells you how close a regression line is to a set of points. It does this by \\ntaking the distances from the points to the regression line (these distances are the “errors”) and \\nsquaring them.  \\n \\nGiving an intuition  \\n \\n \\n \\nThe line equation is y=Mx+B . We want to find M (slope)  and B (y-intercept)  that minimizes the \\nsquared e rror. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6bb1e2f4-8a90-4617-b884-e2649cc9eea7', embedding=None, metadata={'page_label': '11', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INEURON.AI  \\n \\n Page 11 \\n  \\n \\n \\n \\nQ10. Why S upport  Vector Regre ssion ? Difference between SVR and a simple regression \\nmodel?  \\n \\nAns 10: \\nIn simple linear regression , try to minimi ze the error rate. But in SVR , we try to  fit the er ror within \\na certain threshold . \\n \\nMain Concep ts:- \\n1. Boundary  \\n2. Kernel  \\n3. Support Vector  \\n4. Hyper Plane  \\n \\n \\n \\n \\nBlue line: Hyper Plane; Red Line: Boundary -Line \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='88f790db-3b7f-45e2-a9dd-7a84d7fcb9cf', embedding=None, metadata={'page_label': '12', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INEURON.AI  \\n \\n Page 12 \\n Our best fit line is the one w here th e hyperplane  has the maximum number of points.  \\nWe are trying to do here is trying to decide a decision boundary at ‘ e’ distance from the original \\nhyper plane such that data points closest to the hyper plane or the support vectors are within that \\nboundary line  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4489ecaf-c6c1-4695-ac89-94a34a98c0c9', embedding=None, metadata={'page_label': '13', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  1 | 22 \\n \\n \\n \\n \\n \\n \\nDATA SCIE NCE  \\nINTE RVIEW PREPARATION  \\n(30 Days o f Interview \\nPreparation) \\n \\n \\n#DAY  02  \\n  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ca7d8d27-7a41-4a11-80d3-05892e2b9e9a', embedding=None, metadata={'page_label': '14', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  2 | 22 \\n \\nQ1. What is Logistic Regression ? \\n \\nAnswer: \\nThe logistic regression technique involves the dependent variable , which can be represent ed in the \\nbinary (0 or 1, true or false, yes or no) values, which means that the outcome could only be in either \\none form of two. For example, it can be utilized when we need to find the probability of a \\nsuccessful or fail event.  \\n \\n Logistic Regression is used when  the dependent variable ( target) is categorical.  \\n \\n \\nModel  \\nOutput = 0 or 1  \\nZ = WX + B  \\nhΘ(x) = sigmoid (Z)  \\nhΘ(x) = log(P(X) / 1 - P(X) ) = WX +B  \\n \\n  \\nIf ‘Z’ goes to infinity, Y(predicted) will become 1 , and if ‘Z’ goes to negative infinity, Y(predicted) \\nwill beco me 0.  \\nThe output from the hypothesis is the estimated probability. This is used to infer how confident can \\npredicted value be actual value when given an input X.  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0ad0469a-9e18-4b3c-8a5c-216dcbb34b8d', embedding=None, metadata={'page_label': '15', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  3 | 22 \\n \\nCost Function  \\n \\nCost ( hΘ(x) , Y(Actual)) =    -log (hΘ(x)) if y=1 \\n                                             -log (1 - hΘ(x)) if y=0 \\n \\nThis implementation  is for binary  logistic  regression.  For data with more  than 2 classes,  softmax  re\\ngression  has to be used.  \\n \\n \\n \\nQ2. Difference between logistic and linear regression ? \\n \\nAnswer: \\nLinear and Logistic regression are the most basic form of regression which are commonly used. The \\nessential difference between these two is that Logistic regression is used when the dependent variable \\nis binary . In contrast, Linear regression is used when the dependent variable is continuous , and the \\nnature of the regression  line is linear.  \\n \\nKey Differences between  Linear and Logistic Regression  \\n \\nLinear regression models data using continuous numeric value. As against, logistic regression models \\nthe data in the binary values.  \\nLinear regression requires to establish the line ar relationship among dependent and independent \\nvariable s, whereas it is not necessary for logistic regression.  \\nIn linear regression, the independent variable can be correlated with each other. On the contrary, in \\nthe logistic regression, the variable mu st not be correlated with each other.  \\n \\n \\n \\n \\nQ3. Why we can ’t do a classifica tion problem using Regression ? \\n \\nAnswer :- \\nWith linear regression you fit a polynomial through the data - say, like on the example below , we fit \\na straight line through {tumor size, tumor type} sample set:  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bda3fb20-6bf4-43e8-a9ca-21534fa38ddf', embedding=None, metadata={'page_label': '16', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\n \\nP a g e  4 | 22 \\n \\n \\n \\nAbove, malignant tumors get 1 , and non -malignant ones get 0, and the green line is our hypothesis \\nh(x). To make predictions , we may say that for any given tumor size x, if h(x) gets bigger than 0.5 , \\nwe predict malignant tumor s. Otherwise , we predi ct benign ly. \\nIt looks like this way , we could correctly predict every single training set sample, but now let's change \\nthe task a bit.  \\n \\nIntuitively it's clear that all tumors larger certain threshold are malignant. So let's add another sample \\nwith huge tumor si ze, and run linear regression again:  \\n \\n \\n \\nNow our h(x)>0.5 →malignant doesn't work anymore. To keep making correct predictions , we need \\nto change it to h(x)>0.2 or something - but that not how the algorithm should work.  \\n \\nWe cannot change the hypothesis each time  a new sample arrives. Instead, we should learn it off the \\ntraining set data, and then (using the hypothesis we've learned) make correct predictions for the data \\nwe haven't seen before.  \\nLinear regression is unbounded . \\n \\nQ4. What is Decision Tree ? \\n \\nA decision tree is a  type of supervised learning algorithm that can be used in classification as well as \\nregressor problems. The input to a decision tree can be both continuous as well as categorical. The \\ndecision tree works on a n if-then statement. Decision tree tries to solve a problem by using tree \\nrepresentation ( Node and Leaf)  \\nAssumptions while creating a decision tree: 1) Initially all the training set is considered as a root 2) \\nFeature values are preferred to be categorical, if continuous then they are discretized 3) Records are \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='27c75f31-3525-45b6-9d88-6ab53d65dead', embedding=None, metadata={'page_label': '17', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  5 | 22 \\n \\ndistributed recursively on the basis of attribute values 4) Which attributes are considered to be in root \\nnode or internal node is done by using a statistical approach . \\n \\n \\n \\n \\n \\nQ5. Entropy, Information Gain, Gini Index, Reducing Impurity ? \\n \\nAnswer: \\nThere are diffe rent attributes which define the split of nodes in a decision tree. There are few \\nalgorithms to find the optimal split . \\n  \\n1) ID3(Iterative Dichotomiser 3) :  This solution uses Entropy and Information gain as metric s \\nto form a better decision tree. The attribute  with the highest information gain is used as a root \\nnode , and a similar approach is followed after that . Entropy is the measure that characterizes \\nthe impurity of an arbitrary collection of examples.  \\n \\n \\n \\n \\nEntropy varies from 0 to 1. 0  if all the data belong t o a single class and 1  if the class distribution is \\nequal. In this way , entropy will give a measure of impurity in the dataset . \\nSteps to decide which attribute to split:  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='af19ef91-e407-44e2-81fd-62c996644ee0', embedding=None, metadata={'page_label': '18', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  6 | 22 \\n \\n1. Compute the entropy for the dataset  \\n2. For every attribute:  \\n \\n2.1 Calculate entropy for all categorical values . \\n \\n2.2 Take average information entropy for the attribute . \\n \\n2.3 Calculate gain for the current attribute . \\n3. Pick the attribute with the highest information gain . \\n4. Repeat until we get the desired tree . \\n \\nA leaf node is decided when entropy is zero  \\nInforma tion Ga in = 1 - ∑ (Sb/S)*Entropy  (Sb) \\nSb - Subset, S - entire data  \\n \\n2) CART Algorithm  (Classification and Regression trees):  In CART , we use the GINI index as \\na metric. Gini index is used as a cost function to evaluate split in a dataset  \\nSteps to calculate Gini for a spl it: \\n1. Calculate Gini for subnodes, using formula  sum of the square of probability for success \\nand failure (p2+q2) . \\n2. Calculate Gini for split using weighted Gini score of each node of that split . \\n \\n \\n \\n \\nChoose the split based on higher Gini value  \\n \\n \\n \\nSplit  on Gender:  \\nGini for sub-node  Female  = (0.2)*(0.2)+(0.8)*(0.8)=0.68  \\n     Gini for sub-node  Male  = (0.65)*(0.65)+(0.35)*(0.35)=0.55  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ed1d8eeb-308e-467f-bb29-01ef57cdff93', embedding=None, metadata={'page_label': '19', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  7 | 22 \\n \\n     Weighted  Gini for Split  Gender  = (10/30)*0.68+(20/30)*0.55  = 0.59 \\n \\n \\nSimilar  for Split  on Class:  \\nGini for sub-node  Class  IX = (0.43)*(0.43)+( 0.57)*(0.57)=0.51  \\n     Gini for sub-node  Class  X = (0.56)*(0.56)+(0.44)*(0.44)=0.51  \\n     Weighted  Gini for Split  Class  = (14/30)*0.51+(16/30)*0.51  = 0.51 \\n     \\n    Here  Weighted  Gini is high for gender,  so we consider  splitting based  on gender  \\n \\nQ6. How to control leaf height and Pruning ? \\n \\nAnswer : \\nTo control the leaf size , we can set the parameters: - \\n \\n1. Maximum depth :  \\nMaximum tree depth is a limit to stop the further splitting of nodes when the specified tree depth has \\nbeen reached during the building of the initial dec ision tree.  \\nNEVER use maximum depth to limit the further splitting of nodes. In other words: use the \\nlargest possible value.  \\n \\n \\n2. Minimum split size: \\nMinimum split  size is a limit to stop the further splitting of nodes when the number of observations \\nin the node is lower tha n the minimum split  size. \\nThis is a good way to limit the grow th of the tree. When a leaf contains t oo few observations, further \\nsplitting will result in overfitting (modeling of noise in the data).  \\n \\n3. Minimum leaf size  \\nMinimum leaf  size is a limit to split a n ode when the number of observations in one of the child nodes \\nis lower than the minimum leaf  size. \\nPruning  is mostly done to reduce the chances of overfitting the tree to the training data and reduce \\nthe overall complexity of the tree.  \\n \\nThere are two types o f pruning:  Pre-pruning  and Post-pruning . \\n \\n1. Pre-pruning is also known as  the early stopping criteria . As the name suggests, the criteria \\nare set as parameter values while building the model. The tree stops growing when it meets \\nany of these pre -pruning criteria, o r it discovers the pure classes.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='46d85d31-3325-4016-b6b4-159f555d0927', embedding=None, metadata={'page_label': '20', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  8 | 22 \\n \\n2. In Post -pruning, the idea is to allow the decision tree to grow fully and observe the CP value. \\nNext, we prune/cut the tree with the optimal  CP(Complexity Parameter) value as the \\nparameter.  \\n \\nThe CP (complexity parameter) is u sed to control tree growth. If the cost of adding a variable is \\nhigher , then the value of CP, tree growth stops.  \\n \\n \\n \\n \\n \\n \\n \\nQ7. How to handle a decision tree for numerical and categorical \\ndata? \\nAnswer: \\nDecision trees can handle both categorical and numerical variab les at the same time as features . There \\nis not any problem in doing that.  \\nEvery split in a decision tree is based on a feature.  \\n \\n1. If the feature is categorical, the split is done with the elements belonging to a particular \\nclass . \\n2. If the feature is conti nuous,  the split is done with the elements higher than a threshold.  \\n \\nAt every split, the decision tree will take the best variable at that moment. This will be done according \\nto an impurity measure with the split  branches. And the fact that the variable used t o do split is \\ncategorical or continuous is irrelevant (in fact, decision trees categorize contin uous variables by \\ncreating binary regions with the threshold).  \\nAt last, the good approach is to always convert your categoricals to contin uous \\nusing   LabelEncoder  or OneHotEncoding . \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ee9d579a-a85d-4607-aa43-5fc16feb3ba1', embedding=None, metadata={'page_label': '21', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  9 | 22 \\n \\n \\n \\nQ8. What is the Random Forest  Algorit hm? \\n \\nAnswer: \\nRandom Forest is an ensemble machine learning algorithm that follows the bagging technique. The \\nbase estimators in the random forest are decision trees. Random forest randomly selects a set of \\nfeatures that are us ed to decide the best split at each node of the decision tree.  \\nLooking at it step -by-step, this is what a random forest model does:  \\n1. Random subsets are created from the original dataset ( bootstrapping ). \\n2. At each node in the decision tree, only a random set o f features are considered to decide the \\nbest split.  \\n3. A decision tree model is fitted on each of the subsets.  \\n4. The final prediction is calculated by averaging the predictions from all decision trees.  \\nTo sum up, the Random forest randomly selects data points and f eatures  and builds multiple trees \\n(Forest) . \\nRandom  Forest is used for feature importance selection. The attribute  (.feature_importances_ ) is \\nused to find feature importance.  \\n \\n \\n \\n \\n \\nSome Important Parameters :- \\n1. n_estimators :- It defines the number of decision tre es to be created in a random forest.  \\n2.  criterion :- \"Gini \" or \" Entropy .\" \\n3. min_samples_split :- Used to define the minimum number of samples required in a leaf \\nnode before a split is attempted  \\n4. max_features : -It defines the maximum number of features allowed for  the split in each \\ndecision tree.  \\n5. n_jobs :- The number of jobs to run in parallel for both fit and predict.  Always keep ( -1) to \\nuse all the cores for parallel processing . \\n \\n \\nQ9. What is Variance and Bias tradeoff ? \\n \\nAnswer : \\nIn predicting models, the prediction error is composed of two different errors  \\n1. Bias \\n2. Variance  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='61f8c004-a165-4cec-a853-8c45301ff5ba', embedding=None, metadata={'page_label': '22', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\n \\nP a g e  10 | 22 \\n \\n \\nIt is important to understand the variance and bias trade -off which tells about to minimize the Bias \\nand Variance in the predi ction and avoids overfitting & under fitting  of the model . \\n \\nBias: It is the difference between the  expected or average prediction of the model and the correct \\nvalue which we are trying to predict. Imagine  if we are trying to build more than one model by \\ncollecting different data sets , and later on , evaluating the prediction , we may end up by different \\npredic tion for all the models. So, bias is something which measures how far these model prediction \\nfrom the correct prediction. It always leads to a high error in training and test data.  \\n \\nVariance : Variability of a model prediction for a given data point. We can bu ild the model multiple \\ntimes , so the variance is how much the predictions for a given point vary between different \\nrealizations of the model.  \\n \\n \\nFor example : Voting Republican - 13 Voting Democratic - 16 Non -Respondent - 21 Total - 50 \\nThe probability of voting  Republican is 13/(13+16), or 44.8%. We put out our press release that the \\nDemocrats are going to win by over 10 points; but, when the election comes around, it turns out they \\nlose by 10 points. That certainly reflects poorly on us. Where did we g o wrong in our model?  \\nBias scenario's : using a phonebook to select participants in our survey is one of our sources of bias. \\nBy only surveying certain classes of people, it skews the results in a way that will be consistent if we \\nrepeated the entire model  building exercise. Similarly, not following up with respondents is another \\nsource of bias, as it consistently changes the mixture of responses we get. On our bulls -eye diagram , \\nthese move us away from the center of the target, but they would not result in an increased scatter of \\nestimates.  \\nVariance scenario s: the small sample size is a source of variance. If we increased our sample size, \\nthe results would be more consistent each time we repeated the survey and prediction. The results \\nstill might be highly inaccurate due to our large sources of bias, but the variance of predictions will \\nbe reduced  \\n \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3e4b0f8d-cb12-4771-9c29-bd4950940bae', embedding=None, metadata={'page_label': '23', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  11 | 22 \\n \\n \\n \\n \\n \\nQ10. What are Ensemble Methods ? \\n \\nAnswer  \\n1. Bagging  and Boosting  \\nDecision trees have been around for a long time and also known to suffer from bias and variance. \\nYou will have a large b ias with simple trees and a large variance with complex trees.  \\n \\nEnsemble methods  - which combines several decision trees to produce better predictive \\nperformance than utilizing a single decision tree. The main principle behind the ensemble model is \\nthat a g roup of weak learners come together to form a strong learner.  \\n \\nTwo techniques to perform ensemble decision trees:  \\n1. Bagging  \\n2. Boosting  \\n \\nBagging (Bootstrap Aggregation)  is used when our goal is to reduce the variance of a decision tree. \\nHere the idea is to create sev eral subsets of data from the training sample chosen randomly with \\nreplacement. Now, each collection of subset data is used to train their decision trees. As a result, we \\nend up with an ensemble of different models. Average of all the predictions from differen t trees are \\nused which is more robust than a single decision tree.  \\n \\nBoosting  is another ensemble technique to create a collection of predictors. In this technique, learners \\nare learned sequentially with early learners fitting simple models to the data and t hen analyzing data \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a93202e2-47e2-4bc4-82c8-0c7a5999cc02', embedding=None, metadata={'page_label': '24', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  12 | 22 \\n \\nfor errors. In other words, we fit consecutive trees (random sample) , and at every step, the goal is to \\nsolve for net error from the prior tree.  \\nWhen a  hypothesis misclassifies an input , its weight is increased , so that the next hypothes is is more \\nlikely to classify it correctly. By combining the whole set at the end converts weak learners into a \\nbetter performing model.  \\n \\nThe different types of boosting algorithms are:  \\n1. AdaBoost  \\n2. Gradient Boosting  \\n3. XGBoost  \\n \\n \\nQ11. What is SVM Classification ? \\n \\nAnswer: \\nSVM or L arge margin classifier is a supervised learning algorithm that uses a powerful technique \\ncalled SVM for classification.  \\nWe have two types of SVM classifier s:  \\n1) Linear SVM : In Linear SVM, the data  points are expected to be sep arated by some appar ent \\ngap. Th erefore, the SVM algorithm predicts a straight hyperplane dividing the two classes. The \\nhyperplane is also called as maximum margin hyperplane  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4039594b-34f0-41b8-83d1-5d9c534ccd39', embedding=None, metadata={'page_label': '25', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  13 | 22 \\n \\n \\n \\n2) Non -Linear SVM:  It is possible that our data  points are not linearly sep arable in a p -\\ndimensional space, bu t can be linearly sep arable in a higher dimension. Kernel tricks make it \\npossible to draw nonlinear hyperplanes. Some standard kern els are  a) Polynomial Kernel  b) RBF \\nkernel(mostly used) . \\n \\n \\nAdvantages of SVM classifier :    \\n1) SVMs are effective when the num ber of features is quite large.  \\n2) It works effectively even if the number of features is greater than the number of samples.  \\n3) Non -Linear data can also be classified using customized hyperplanes built by using kernel trick.  \\n4) It is a robust model to so lve prediction problems since it maximizes margin.  \\n \\nDisadvantages of SVM classifier:   \\n1) The biggest limitation of the Support Vector Machine is the choice of the kernel. The wrong choice \\nof the kernel can lead to an increase in error percentage.  \\n2) With a great er number of samples, it starts giving poor performances.  \\n3) SVMs have good generalization performance , but they can be extremely slow in the test phase.  \\n4) SVMs have high algorithmic complexity and extensive memory requirements due to the use of \\nquadratic programming.  \\n \\nQ11. What is Naive Ba yes Classification and Gaussian Naive Ba yes \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b11f283e-1cb8-4bf3-aafa-1d6fb452ff6c', embedding=None, metadata={'page_label': '26', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  14 | 22 \\n \\n \\nAnswer : \\nBayes’ Theorem  finds the probability of an event occurring given the probability of another event \\nthat has already occurred. Bayes’ theorem is stated mathematically as the following  equation:  \\n \\n  \\nNow, with regards to our dataset, we can apply Bayes’ theorem in following way:  \\nP(y|X) = {P(X|y) P(y)}/{P(X)}  \\nwhere, y is class variable and X is a dependent feature vector (of size n) where:  \\nX = (x_1,x_2,x_3,.....,x_n)  \\n  \\n \\nTo clear, an e xample of a feature vector and corresponding class variable can be: (refer 1st row of \\nthe dataset)  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fb71b8cb-fa71-41d6-8ea5-2ceb7d48d984', embedding=None, metadata={'page_label': '27', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  15 | 22 \\n \\nX = (Rainy, Hot, High, False) y = No So basically, P(X|y) here means, the probability of “Not \\nplaying golf” given that the weather conditions are “Rainy outlook” , “Temperature is hot”, “high \\nhumidity” and “no wind”.  \\n \\nNaive Ba yes Classification : \\n1. We assume that no pair of features are dependent. For example, the temperature being ‘Hot’ \\nhas nothing to do with the humidity , or the outlook being ‘Rainy’ does not affect  the winds. \\nHence, the features are assumed to be independent.  \\n2. Secondly, each feature is given the same weight  (or importance). For example, knowing the \\nonly temperature and humidity alone can’t predict the outcome accurate ly. None of the \\nattributes is irrelevan t and assumed to be contributing equally to the outcome  \\nGaussian Naive Bayes  \\nContinuous values associated with each feature are assumed to be distributed according to a \\nGaussian distribution. A Gaussian distribution is also called Normal distribution. When  plotted, it \\ngives a bell -shaped curve which is symmetric about the mean of the feature values as shown below:  \\n This is as simple as calculating the mean and standard deviation values of each input variable (x) for \\neach class value.  \\nMean (x) = 1/n * sum(x)  \\nWhere n is the number of instances , and x is the values for an input variable in your training data.  \\nWe can calculate the standard deviation using the following equation:  \\nStandard  deviation(x) = sqrt  (1/n * sum(xi -mean(x)^2 ))  \\nWhen to use what ? Standard N aive Bayes only supports categorical features, while Gaussian Naive \\nBayes only supports continuously valued features.  \\n \\n \\nQ12. What is the Confusion Matrix ? \\n \\nAnswer : \\nA confusion matrix is a table that is often used to describe the performance of a classification model \\n(or “classif ier”) on a set of test data for which the true values are known. It allows the visualization \\nof the performance of an algorithm.  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9581a810-bd68-4714-a05e-78aeadade0b1', embedding=None, metadata={'page_label': '28', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  16 | 22 \\n \\nA confusion matrix is a  summary of prediction results  on a classification problem. The number of \\ncorrect and incorrect predicti ons are summarized with count values and broken down by each class.  \\n \\nThis is the key to the confusion matrix.  \\nIt gives us insight not only into the errors being made by a classifier  but, more importantly , the types \\nof errors that are being made.  \\n Here,  \\n\\uf0b7 Class  1: Positive  \\n\\uf0b7 Class 2 : Negative  \\nDefinition of the Terms:  \\n1. Positive (P) : Observation is positive (for example: is an apple).  \\n2. Negative (N) : Observation is not positive (for example: is not an apple).  \\n3. True Positive (TP) : Observation is positive, and is pre dicted to be positive.  \\n4. False Negative (FN) : Observation is positive, but is predicted negative.  \\n5. True Negative (TN) : Observation is negative, and is predicted to be negative.  \\n6. False Positive (FP) : Observation is negative, but is predicted positive.  \\n \\n \\n \\nQ13. What is Accuracy and  Misclassification Rate ? \\n \\nAnswer : \\n \\nAccuracy  \\nAccuracy is defined as the ratio of  the sum of True Positive and True \\nNegative  by Total(TP+TN+FP+FN)  \\n  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='05ea126a-d648-4526-b236-32ebacbab6b6', embedding=None, metadata={'page_label': '29', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  17 | 22 \\n \\nHowever,  there are problems  with accuracy. It assumes equal costs for both kinds of \\nerrors.  A 99% accuracy can be excellent, good, mediocre, poor , or terrible depending upon \\nthe problem.  \\n \\n \\n \\n \\nMisclassification Rate  \\n \\nMisclassification Rate is defined as the ratio of  the sum of False Positive and False \\nNegative  by Total(TP+TN+FP+FN)  \\nMisclassification Rate is also called  Error Rate.  \\n \\n \\n \\n \\n \\n \\nQ14. True Positive Rate & True Negative Rate  \\n \\nAnswer : \\n \\nTrue Positive Rate : \\nSensitivity (SN)  is calculated as the number of correct positive predictions divided by the \\ntotal number of positives. It is also called  Recall (REC)  or true positive rate (TPR) . The best \\nsensitivity is 1.0, whereas the worst is 0.0.  \\n \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='58b40387-a520-446b-8c8d-41a4ff05ddb2', embedding=None, metadata={'page_label': '30', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  18 | 22 \\n \\n \\n \\n \\nTrue Negative Rate  \\n \\nSpecificity (SP)  is calculated as the number of correct negative predictions divided by the \\ntotal number of negatives. It is also called a true negative rate (TNR). The best speci ficity is \\n1.0, whereas the worst is 0.0.  \\n \\n \\nQ15. What is False Positive Rate & False negative Rate ? \\nFalse Positive Rate  \\nFalse positive rate (FPR) is calculated as the number of incorrect positive predictions divided by the \\ntotal number of negatives. The best false positive rate is 0.0 , whereas the worst is 1.0. It can also be \\ncalculated as 1 – specificity.  \\n \\nFalse Negative Rate  \\nFalse Negative rate (FPR) is calculated as the number of incorrect positive predictions divided by \\nthe total number of positives. The best fa lse negative rate is 0.0 , whereas the worst is 1.0.  \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='de3f5a48-305d-4af1-814a-d7782929b8ac', embedding=None, metadata={'page_label': '31', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  19 | 22 \\n \\nQ16. What are F1 Score, precision and recall ? \\n \\nRecall :- \\nRecall can be defined as the ratio of the total number of correctly classified positive examples \\ndivide to the total number of positive examples.  \\n1. High Recal l indicates the class is correctly recognized (small number of FN).  \\n2. Low Recall indicates the class is incorrectly recognized (large number of FN).  \\n \\nRecall is given by the relation:  \\n  \\n \\nPrecision:  \\nTo get the value of precision , we divide the total number of cor rectly classified positive examples \\nby the total number of predicted positive examples.  \\n1. High Precision indicates an example labeled as positive is indeed positive ( a small number \\nof FP).  \\n2. Low Precision indicates an example labeled as positive is indeed positi ve (large number of \\nFP). \\n \\nThe relation gives precis ion: \\n  \\n \\nRemember :- \\nHigh recall, low precision:  This means that most of the positive examples are correctly recognized \\n(low FN) , but there are a lot of false positives.  \\nLow recall, high precision:  This s hows that we miss a lot of positive examples (high FN) , but those \\nwe predict as positive are indeed positive (low FP).  \\n \\n \\n \\n \\nF-measure/F1 -Score : \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='17336ff0-8742-456f-a418-425496c74026', embedding=None, metadata={'page_label': '32', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  20 | 22 \\n \\nSince we have two measures (Precision and Recall) , it helps to have a measurement that represents \\nboth of them. We cal culate an  F-measure , which uses Harmonic Mean in place of Arithmetic \\nMean as it punishes the extreme values more.  \\n \\n \\nThe F -Measure will always be nearer to the smaller value of Precision or Recall.  \\n \\n \\n \\n \\nQ17. What is RandomizedSearchCV ? \\n \\nAnswer : \\nRandomized search CV is used to per form a random  search on hyperparameters. Randomized \\nsearch CV uses a fit and score method, predict proba, decision_func, transform , etc..,  \\nThe parameters of the estimator used to apply these methods are optimized by cross -validated \\nsearch over parameter sett ings.  \\n \\nIn contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of \\nparameter settings is sampled from the specified distributions. The number of parameter settings that \\nare tried is given by n_iter.  \\n \\nCode Example :  \\n \\nclass sklearn.model_ selection.RandomizedSearchCV(estimator, param_distributions, \\nn_iter=10, scoring=None, fit_params=None, n_jobs=None, iid=’warn’, refit=True, \\ncv=’warn’, verbose=0, pre_dispatch=‘2 n_jobs’, random_state=None, error_score=’raise -\\ndeprecating’, return_train_score =’warn’)  \\n \\n \\n \\n \\n \\nQ18. What is GridSearchCV ? \\n \\nAnswer : \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cba7b4e3-f27b-4b34-9251-9d88ae7ff033', embedding=None, metadata={'page_label': '33', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\n \\nP a g e  21 | 22 \\n \\nGrid search is the process of performing hyper parameter tuning to determine the optimal values for \\na given model.  \\nCODE  Example: - \\n \\nfrom sklearn.model_selection import GridSearchCV from sklearn.svm import SVR gsc = \\nGridSearchCV ( estimator=SVR(kernel='rbf'), param_grid={ 'C': [0.1, 1, 100, 1000], 'epsilon': \\n[0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10], 'gamma': [0.0001, 0.001, 0.005, 0.1, 1, \\n3, 5] }, cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs= -1) \\n \\nGrid search runs the model on all the possible range of hyperparameter values and outputs the best \\nmodel  \\n \\n \\nQ19. What is BaysianSearch CV? \\n \\nAnswer : \\nBayesian search , in contrast to the grid and random search, keep s track of past evaluation results , \\nwhich they use to form a probabilis tic model mapping hyperparameters to a probability of a score on \\nthe objective function.  \\n \\n \\n \\n \\n \\nCode:  \\nfrom  skopt  import  BayesSearchCV  \\nopt = BayesSearchCV(  \\n    SVC(),  \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c1c8fd0c-94d0-47bf-a67d-5753c702eb0c', embedding=None, metadata={'page_label': '34', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\n \\nP a g e  22 | 22 \\n \\n    { \\n        'C': (1e-6, 1e+6,  'log-uniform'),    \\n        'gamma':  (1e-6, 1e+1,  'log-uniform'),  \\n        'degree':  (1, 8),  # integer  valued  parameter  \\n        'kernel':  ['linear',  'poly',  'rbf'] \\n    }, \\n    n_iter=32,  \\n    cv=3 ) \\n \\n \\n \\nQ20. What is ZCA Whitening ? \\n \\nAnswer : \\nZero Component Analysis:  \\nMaking the co -variance matrix as the Identity matrix is called whitening. T his will remove the first \\nand second -order statistical structure  \\nZCA  transforms  the data to zero mean s and makes  the features  linearly  independ ent of each other  \\nIn some  image  analysis  applications,  especially  when  working  with images  of the color  and tiny typ\\ne, it is frequently  interesting  to apply  some  whitening  to the data before , e.g. training  a classifier.  \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='80ef77aa-4064-40e1-9fd7-eaba28d2f078', embedding=None, metadata={'page_label': '35', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  1 | 18 \\n \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW  PREPARATION  \\n(30 Days of Interview  \\nPreparation)  \\n \\n# DAY 03  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2a9b2858-a330-44ca-9765-6a34f2c8f81b', embedding=None, metadata={'page_label': '36', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  2 | 18 \\n \\n  \\nQ1. How do you treat heteroscedasticity in regression ? \\nHeteroscedasticity  means unequal scatter ed distribution . In regression analysis, we generally talk about \\nthe heteroscedas ticity in the contex t of the error term. H eteroscedasticity is the  systematic change in the \\nspread of the residuals  or errors  over the range of measured  values. Heteroscedasticity is the  problem \\nbecause Ordinary least squares (OLS)  regression assumes that all residuals are drawn from a random \\npopulation that has a cons tant variance . \\n  \\nWhat c auses Heteroscedasticity?  \\nHeteroscedasticity  occurs more often in datasets , where we  have a large range between the  largest and \\nthe smallest observed values. T here are many  reasons why heteroscedasticity can exist,  and a generic  \\nexplanation is that the error variance change s proportionally with a factor.  \\n \\nWe can categorize  Heterosced asticity into two general types :- \\n \\nPure heteroscedasticity :-  It refers to cases where we  specify the correct model and let us  observe the \\nnon-constant variance in  residual plots.  \\n \\nImpure heteroscedasticity :-  It refers to cases where you incorrectly specify  the model, and that causes \\nthe non -constant variance. When you leave an important variable out of a model, the omitted effect is \\nabsorbed into the error term. If the effect of the omitted variable varies throughout the observed range of \\ndata, it can produ ce the telltale signs of heteroscedasticity in the residual plots.  \\n \\nHow to Fix Heteroscedasticity  \\n \\nRedefining the variables : \\nIf your model is a cross -sectional model that includes large differences between the sizes of the \\nobservations, you can find different  ways to specify the model that reduces the impact of the size \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='da14c4ed-5e15-44d5-9756-d996ee34ffcc', embedding=None, metadata={'page_label': '37', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  3 | 18 \\n \\ndifferential. To do this, change the model from using the raw measure to using rates and per capita values. \\nOf course, this type of model answers a slightly different kind of question. You’ll n eed to determine \\nwhether this approach is suitable for both your data and what you need to learn.  \\n \\nWeighted regression : \\nIt is a method that assigns each data point to a weight based on the variance of its fitted value. The idea \\nis to give small  weights to observations associated with higher variances to shrink their squared residuals. \\nWeighted regression minimizes the sum of the weighted squared residuals. When you use the correct \\nweights, heteroscedasticity is replaced by homoscedasticity.  \\n \\nQ2. What is multicollinearity , and how do you treat it ?  \\n \\nMulticollinearity  means independent variables are highly correlated to each other. In regression \\nanalysis, it\\'s an important assumption that the regression model should not be faced with a problem of \\nmultico llinearity.  \\nIf two explanatory variables are highly correlated, it\\'s hard to tell , which affects  the dependent variable.  \\nLet\\'s say  Y is regressed against X1 and X2 and where X1 and X2 are highly correlated. Then the effect \\nof X1 on Y is hard to dis tinguish from the effect of X2 on Y because any increase in X1 tends to be \\nassociated with an increase in X2.  \\nAnother way to look at the multicollinearity problem is : Individual t -test P values can be misleading. It \\nmeans a P -value can be high , which means the variable is not important, even though the variable is \\nimportant.  \\n \\nCorrecting Multicollinearity : \\n1) Remove one of the highly correlated independent variable s from the model. If you have two or more \\nfactors with a high VIF, remove one from the model.  \\n2) Principle Com ponent Analysis (PCA) - It cut the number of interdependent variables to a smaller set \\nof uncorrelated components. Instead of using highly correlated variables, use components in the model \\nthat have eigenvalue greater than 1.  \\n3) Run PROC VARCLUS and choose  the variable that has a minimum (1 -R2) ratio within a cluster.  \\n4) Ridge Regression - It is a technique for analyzing multiple regression data that suffer from \\nmulticollinearity.  \\n5) If you include an interaction term (the product of two independent variables), y ou can also reduce \\nmulticollinearity by \"centering\" the variables. By \"centering ,\" it means subtracting the mean from the \\nvalues of the independent variable  before creating the products.  \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ec839d7c-4699-4b09-9de5-f0796684a409', embedding=None, metadata={'page_label': '38', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  4 | 18 \\n \\n \\nWhen is multicollinearity  not a problem?  \\n \\n1) If your goal is to predi ct Y from a set of X variables, then multicollinearity is not a problem. The \\npredictions will still be accurate, and the overall R2 (or adjusted R2) quantifies how well the model \\npredicts the Y values.  \\n2) Multiple dummy (binary) variables that represent a categorical variable with three or more categories.  \\n \\n \\nQ3. What is market basket analysis? How would you do it in Python ? \\n \\nMarket basket analysis is the study of items that are purchased or grouped in a single transaction or \\nmultiple, sequential tr ansactions. Understanding the relationships and the strength of those relationships \\nis valuable information that can be used to make recommendations, cross -sell, up -sell, offer coupons, \\netc. \\nMarket Basket Analysis is one of the key techniques used by large  retailers to uncover associations \\nbetween items. It works by looking for combinations of items that occur together frequently in \\ntransactions. To put it another way, it allows retailers to identify relationships between the items that \\npeople buy.  \\n \\nQ4. Wh at is Associati on A nalysis? Where is it used ? \\n \\nAssociation analysis uses a set of transactions to discover rules that indicate the likely occurrence of an \\nitem based on the occurrences of o ther items in the transaction.  \\n \\nThe technique of association rules  is widely used for  retail basket analysis . It can also be used for \\nclassification by using rules with class labels on the right -hand side. It is even used for outlier detection \\nwith rules indicating infrequent/abnormal association.  \\n \\nAssociation analysis al so helps us to identify cross -selling opportunities, for example , we can use the \\nrules resulting from the analysis to place associated products together in a catalog, in the supermarket, \\nor the Web shop, or apply them when targeting a marketing campaign  for product B at customers who \\nhave already purchased product A.  \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e83c58ce-b609-45a8-9ac5-34d90b170c45', embedding=None, metadata={'page_label': '39', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  5 | 18 \\n \\nAssociation rules are given in the form as below:  \\n \\nA=>B[Support,Confidence] The part before => is referred to as if (Antecedent) and the part after => is \\nreferred to as then (Consequent).  \\nWhe re A and B are sets of items in the transaction data , a and B are disjoint sets.  \\nComputer=>Anti−virusSoftware[Support=20%,confidence=60%] Above rule says:  \\n1. 20% transaction show Anti -virus software is bought with purchase of a Computer  \\n2. 60% of customers who p urchase Anti -virus software is bought with purchase of a Computer  \\nAn example of Association Rules * Assume there are 100 customers  \\n1. 10 of them bought milk, 8 bought butter and 6 bought both of them 2 .bought milk => bought \\nbutter  \\n2. support = P(Milk & Butt er) = 6/100 = 0.06  \\n3. confidence = support/P(Butter) = 0.06/0.08 = 0.75  \\n4. lift = confidence/P(Milk) = 0.75/0.10 = 7.5  \\n \\nQ5. What is KNN Classifier  ? \\n \\nKNN means  K-Nearest Neighbour  Algorithm. It can be used for both classification and regression.  \\n \\n  \\nIt is the simplest machi ne learning algorithm. Also known as  lazy learning  (why? Because it does not \\ncreate a generalized model during the time of training, so the testing phase is very important where it \\ndoes the actual job. Hence Testing is very costly - in terms of time & money). A lso called a n instance -\\nbased or memory -based learning  \\nIn k-NN classification , the output is a class membership. An object is classified by a plurality vote of its \\nneighbors, with the object being assigned to the class most common among its k nearest ne ighbors (k is \\na positive integer, typically small). If k = 1, then the object is assigned to the class of that single nearest \\nneighbor.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='82f78304-8db4-47a8-89fc-87d404223396', embedding=None, metadata={'page_label': '40', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  6 | 18 \\n \\n \\nIn k-NN regression , the output is the property value for the object. This value is the average of the \\nvalues of k  nearest neighbors.  \\n All three distance measures are only valid for continuous variables. In the instance of categorical \\nvariables , the Hamming distance must be used . \\n  \\nHow to choose the value of K:  K value is a  hyperparameter which needs to choose  during  the time of \\nmodel building  \\nAlso, a small number  of neighbors are most flexible fit , which will have a low bias , but the high variance \\nand a large number of neighbors will have a smoother decision boundary , which means lower variance \\nbut higher bias.  \\nWe should choo se an odd number if the number of classes is even. It is said the most common values are \\nto be 3 & 5.  \\n \\nQ6. What is Pipeline in sklearn  ? \\n \\nA pipeline is what chains several steps together, once the initial exploration is done. For example, some \\ncodes are meant to trans form features \\u200a—\\u200anormali ze numerical ly, or turn text into vectors, or fill up \\nmissing data,  and they are transformers; other codes are meant to predict variables by fitting an algorithm, \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fa45da6c-9347-43c4-a25e-ddc8839b59da', embedding=None, metadata={'page_label': '41', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\nP a g e  7 | 18 \\n \\nsuch as random forest or support vector machine, they are estimators. Pipel ine chains all these together , \\nwhich can th en be applied to training data i n bloc k. \\nExample of a pipeline that imputes data with the most frequent value of each column, and then fit a \\ndecision tree classifier.  \\nFrom sklearn.pipeline  import  Pipeline  \\n steps  = [('imputation',  Imputer(missing_values='NaN',  strategy  = 'most_frequent',  axis=0)),  \\n          ('clf', DecisionTreeClassifier())]  \\n pipeline  = Pipeline(steps)  \\n clf = pipeline.fit(X_train,y_train)```  \\n  \\n Instead  of fitting  to one model,  it can be looped  over several  models  to find the best one. \\nclassifiers = [ KNeighborsClassifier(5), RandomForestClassifier(), GradientBoostingClassifier()]  \\nfor clf in classifiers:  \\nsteps  = [('imputation',  Imputer(missing_values='NaN',  strategy  = 'most_frequent',  axis=0)),  \\n      ('clf',  clf)] \\n  \\npipeline  = Pipeline(steps)  \\n \\nI also learn ed the pipeline itself can be used as an estimator and passed to cross -validation or grid  \\nsearch.  \\nfrom  sklearn .model_selection  import  KFold  \\n from  sklearn.model_sel ection  import  cross_val_score  \\n kfold  = KFold(n_ splits=10,  random_state=seed)  \\n results  = cross_val_score(pipeline , X_train,  y_train,  cv=kfold)  \\n print(results.mean())  \\n \\nQ7. What is Principal Component Analysis(PCA) , and why we do?  \\n \\nThe main idea of principal component analysis (PCA) is to reduce the dimensionality of a data set \\nconsisting of many variables correlated with each other, either heavily or lightly, while retaining the \\nvariation present in the dataset, up to the maximum extent. The same is done by transforming the \\nvariables to a new set  of variables, which are known as the principal components (or simply, the PCs) \\nand are orthogonal, ordered such that the retention of variation present in the original variables decreases \\nas we move down in the order. So, in this way, the 1st principal co mponent retains maximum variation \\nthat was present in the original components. The principal components are the eigenvectors of a \\ncovariance matrix, and hence they are orthogonal.  \\n \\n \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4e82a8f8-f785-4db0-a46d-e62dcfec42d6', embedding=None, metadata={'page_label': '42', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\nP a g e  8 | 18 \\n \\n \\n \\nMain important points to be considered:  \\n1. Normalize the data  \\n2. Calculate the cov ariance matrix  \\n3. Calculate the eigenvalues and eigenvectors  \\n4. Choosing components and forming a feature vector  \\n5. Forming Principal Components  \\n \\nQ8. What is t -SNE?  \\n \\n(t-SNE) t -Distributed Stochastic Neighbor Embedding is a non -linear dimens ionality reduction algorithm \\nused for exploring high -dimensional data. It maps multi -dimensional data to two or more dimensions \\nsuitable for human observation. With the help of the t -SNE algorithms, you may have to plot fewer \\nexploratory data analysis plots ne xt time you work with high dimensional data.  \\n \\nQ9. VIF(Variation Inflation Factor),Weight of Evidence & Information  \\n        Value.  Why and when to use ? \\n \\nVariation Inflation Factor  \\nIt provides an index that measures how much the variance (the square of the estimate's standard \\ndeviation) of an estimated regression coefficient is increased because of collinearity.  \\nVIF = 1 / (1 -R-Square of j -th variable) where R2 of jth vari able is the coefficient of determination of the \\nmodel that includes all independent variables except  the jth predictor.  \\nWhere R -Square of j -th variable is the multiple R2 for the regression of Xj on the other independent \\nvariables (a regression that does not involve the dependent variable Y).  \\nIf VIF > 5 , then there is a problem with multicollinearity.  \\n \\nUnderstanding VIF  \\nIf the variance inflation factor of a predictor variable is 5 this means that variance for the coefficient of \\nthat predictor variable is 5 times as large as it would be if that predictor variable were uncorrelated with \\nthe other predictor va riables.  \\nIn other words, if the variance inflation factor of a predictor variable is 5 this means that the standard \\nerror for the coefficient of that predictor variable is 2.23 times ( √5 = 2.23) as large as it would be if that \\npredictor variable were uncor related with the other predictor variables.  \\n \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8c469d82-c645-4813-b12a-80bc4d2c24a2', embedding=None, metadata={'page_label': '43', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  9 | 18 \\n \\nWeight of evidence (WOE) and information value (IV)  are simple, yet powerful techniques to \\nperform variable transformation and selection.  \\n  \\nThe formula  to create  WOE  and IV is  \\n \\n \\n  \\nHere is a simple table that sho ws how to calculate these values.  \\n  \\nThe IV value can be used to select variables quickly.  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8b13e87e-c9c5-4f00-8344-18bae1228324', embedding=None, metadata={'page_label': '44', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  10 | 18 \\n \\n \\n \\nQ10: How to evaluate  that d ata does not have any outliers ? \\n \\nIn statistics, outliers are data points that don’t belong to a certain population. It is an abnormal observation \\nthat lies far away from other values. An outlier is an observation that diverges from otherwise well -\\nstructured data.  \\nDetection:  \\n \\nMethod 1 — Standard Deviation:  In statistics, If a data distribution is approximately normal , then \\nabout  68% of the data values lie within one standard deviation of the mean , and about 95% are within \\ntwo standard deviations, and about 99.7% lie within three standard deviations.  \\n Therefore, if you have any data point that is more than 3 times the standard dev iation, then those points \\nare very likely to be anomalous or outliers.  \\n \\nMethod 2 — Boxplots : Box plots are a graphical depiction of numerical data through their quantiles. It \\nis a very simple but effective way to visualize outliers. Think about the lower an d upper whiskers as the \\nboundaries of the data distribution. Any data points that show above or below the whiskers  can be \\nconsidered outliers or anomalous.  \\n  \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5abbea1f-3b79-42c9-8654-ee33f4cc5728', embedding=None, metadata={'page_label': '45', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  11 | 18 \\n \\nMethod 3 - Violin Plots : Violin plots are similar to box plots, except that they also show the p robability \\ndensity of the data at different values, usually smoothed by a kernel density estimator. Typically a violin \\nplot will include all the data that is in a box plot: a marker for the median of the data , a box or marker \\nindicating the interquartile r ange , and possibly all sample points  if the number of samples is not too high.  \\n  \\nMethod 4 - Scatter  Plots : A scatter plot  is a type of plot or mathematical diagram using Cartesian \\ncoordinates to display values for typically two variables for a set of da ta. The data are displayed as a \\ncollection of points, each having the value of one variable determining the position on the horizontal axis \\nand the value of the other variable determining the position on the vertical axis.  \\n \\n  \\nThe points  which  are very far away from  the general  spread  of data and have  a very few neighbo rs are \\nconsidered  to be outliers  \\n \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f967d2d5-c248-48e1-a48e-cd6c7ee4086a', embedding=None, metadata={'page_label': '46', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  12 | 18 \\n \\n \\nQ11: What you do if there are outliers ? \\n \\nFollowing are the approaches to handle the outliers:  \\n1. Drop the outlier records  \\n2. Assign a new value: If an outlier seems to  be due to a mistake in your data, you try imputing a \\nvalue.  \\n3. If percentage -wise the number of outliers is less , but when we see numbers , there are several , \\nthen, in that case,  dropping them might cause a loss in insight. We should group them in that \\ncase and run our analysis separately on them.  \\n \\nQ12: What are the encoding  techniques you hav e applied with  \\n          Examples  ? \\n \\nIn many practical data science activities, the data set will contain categorical variables. These variables \\nare typically stored as text values\". Sinc e machine learning is based on mathematical equations, it would \\ncause a problem when we keep categorical variables as is.  \\n \\nLet\\'s consider the following dataset of fruit names and their weights.  \\nSome of the common encoding techniques are:  \\nLabel encoding:  In label encoding, we map each category to a number or a label. The labels chosen for \\nthe categories have no relationship. So categories that have some ties or are close to each other lose such \\ninformation after encoding.  \\nOne - hot encoding:  In this method, w e map each category to a vector that contains 1 and 0 denoting \\nthe presence of the feature or not. The number of vectors depends on the categories which we want to \\nkeep. For high cardinality features, this method produces a lot of columns that slows down t he learning \\nsignificantly.  \\n \\n \\nQ13: Tradeoff  between bias and variances, the relationship between  \\n         them. \\n \\nWhenever we discuss model prediction, it’s important to understand prediction errors (bias and variance). \\nThe prediction error for any machine learning algorithm  can be broken down into three parts:  \\n\\uf0b7 Bias Error  \\n\\uf0b7 Variance Error  \\n\\uf0b7 Irreducible Error  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='290e4c71-68ee-4d9e-8755-9f630802e480', embedding=None, metadata={'page_label': '47', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  13 | 18 \\n \\nThe irreducible error cannot be reduced regardless of what algorithm is used. It is the error introduced \\nfrom the chosen framing of the problem and may be caused by factors l ike unknown variables that \\ninfluence the mapping of the input variables to the output variable.  \\n \\nBias:  Bias means that the model favo rs one result more than the others. Bias is the simplifying \\nassumptions made by a model to make the target function  easier to learn.  The m odel with high bias pays \\nvery little attention to the training data and oversimplifies the model. It always leads to a high error in \\ntraining and test data.  \\n \\nVariance:  Variance is the amount that the estimate of the target function will chan ge if different training \\ndata was used. The m odel with high variance pays a lot of attention to training data and does not \\ngeneralize on the data which it hasn’t seen before. As a result, such models perform very well on training \\ndata but ha ve high error rates on test data.  \\n  \\nSo, the end goal is to come up with a model that balances both Bias and Variance. This is called  Bias \\nVariance Trade -off. To build a good model, we need to find a good balance between bias and variance \\nsuch that it minimizes the total erro r. \\n \\nQ14: What is the difference  between Type 1 and T ype 2 error and  \\n          severity of the error?     \\n \\nType I Error  \\nA Type I error is often referred to as a “false positive\" and is the incorrect rejection of the true null \\nhypothesis in favor of the alternative.  \\nIn the example abo ve, the null hypothesis refers to the natural state of things or the absence of the tested \\neffect or phenomenon, i.e. , stating that the patient is HIV negative. The alternative hypothesis states that \\nthe patient is HIV positive. Many medical tests will have  the disease they are testing for as the alternative \\nhypothesis and the lack of that disease as the null hypothesis.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f597552c-d95a-45fd-9e2d-cd52df34deda', embedding=None, metadata={'page_label': '48', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  14 | 18 \\n \\nA Type I error would thus occur when the patient doesn’t have the virus , but the test shows that they do. \\nIn other words, the test incorrect ly rejects the true null hypothesis that the patient is HIV negative.  \\n \\nType II Error  \\nA Type II error is the inverse of a Type I error and is the false acceptance of a null hypothesis that is not \\ntrue, i.e. , a false negative. A Type II error would en tail the test telling the patient they are free of HIV \\nwhen they are not.  \\n \\nConsidering this HIV example, which error type do you think is more acceptable? In other words, would \\nyou rather have a test that was more prone to Type I or Type s II error? With HIV,  the momentary stress \\nof a false positive is likely  better than feeling relieved at a false negative and then failing to take steps to \\ntreat the disease. Pregnancy tests, blood tests , and any diagnostic tool that has serious consequences for \\nthe h ealth of a patient are usually overly sensitive for this reason – they should  err on the side of a false \\npositive.  \\n \\nBut in most fields of science, Type II errors are seen as less serious than Type I errors. With the Type II \\nerror, a chance  to reject the null hypothesis was lost, and no conclusion is inferred from a non -rejected \\nnull. But the Type I error is more serious  because you have wrongly rejected the null hypothesis and \\nultimately made a claim that is not true. In science, finding a  phenomenon where there is none is more \\negregious than failing to find a phenomenon where there is.  \\n \\nQ15: What is binomial distributio n and polynomial distribution ? \\n \\nBinomial Distribution:  A binomial distribution can be thought of as simply the probability of a \\nSUCCESS or FAILURE outcome in an experiment or survey that is repeated multiple times. The \\nbinomial is a type of distribution that has two possible outcomes (the  prefix “bi” means two, or twice). \\nFor example, a coin toss has only two possible outcomes: heads or tails , and taking a test could have two \\npossible outcomes: pass or fail.  \\nMultimonial/Polynomial Distribution:  Multi or Poly means many. In probability theor y, the \\nmultinomial distribution is a generalization of the binomial distribution. For example, it models the \\nprobability of counts of each side for rolling a k -sided die n times. For n independent trials each of which \\nleads to success for exactly one of k categories, with each category having a given fixed success \\nprobability, the multinomial distribution gives the probability of any particular combination of numbers \\nof successes for the various categories  \\n \\n \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='22d1094a-8001-4b2c-94e2-af4646a0471b', embedding=None, metadata={'page_label': '49', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  15 | 18 \\n \\nQ16: What is the  Mean M edian Mode stand ard deviation for the       \\n         sample and population ?  \\n \\nMean  It is an important technique in statistics. Arithmetic Mean can also be called an average. It is the \\nnumber o f the quantity obtained by summing two or more numbers/variables and then dividing the sum \\nby the number of numbers/variabl es. \\n \\nMode  The mode is also one of the types for finding the average. A mode is a number that occurs most \\nfrequently in a group of numbers. Some series might not have any mode; some might have two modes , \\nwhich is called a bimodal series.  \\nIn the study of statist ics, the three most common ‘averages’ in statistics are mean, median, and mode.  \\n \\nMedian  is also a way of finding the average of a group of data points. It’s the middle number of a set of \\nnumbers. There are two possibilities, the data points can be an  odd number group , or it can be an even \\nnumber group.  \\nIf the group is odd, arrange the numbers in the group from smallest to largest. The median will be the \\none which is exactly sitting in the middle, with an equal number on either side of it. If the group is even, \\narrange the numbers in order and pick the two middle numbers and add them then divide by 2. It will be \\nthe median number of that set.  \\n \\nStandard Deviation (Sigma)  Standard Deviation is a measure of how much your data is spread out in \\nstatistics.  \\n \\nQ17: What is Mean Absolute E rror ? \\n \\nWhat is Absolute Error?  Absolute Error is the amount of error in your measurements. It is the \\ndifference between the measured value and the “true” value. For example, if a scale states 90 pounds , \\nbut you know your true weight is 8 9 pounds, then the scale has an absolute error of 90 lbs – 89 lbs = 1 \\nlbs. \\nThis can be caused by your scale , not measuring the exact amount you are trying to measure. For \\nexample, your scale may be accurate to the nearest pound. If you weigh 89.6 lbs, the s cale may “round \\nup” and give you 90 lbs. In this case the absolute error is 90 lbs – 89.6 lbs = .4 lbs.  \\n \\nMean Absolute Error  The Mean Absolute Error(MAE) is the average of all absolute errors. The formula \\nis: mean absolute error  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e7fe7afd-ae1b-4458-82ba-d1bd38b9680e', embedding=None, metadata={'page_label': '50', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\nP a g e  16 | 18 \\n \\nWhere,  \\nn = the number of e rrors, Σ = summation symbol (which means “add them all up”), |xi – x| = the absolute \\nerrors. The formula may look a little daunting, but the steps are easy:  \\nFind all of your absolute errors, xi – x. Add them all up. Divide by the number of errors. For exam ple, if \\nyou had 10 measurements, divide by 10.  \\n \\nQ18: What is the difference  between long  data and wide data?  \\n \\nThere are many different ways that you can present the same dataset to the world. Let's take a look at \\none of the most important and fundamental distinc tions, whether a dataset is wide or long.  \\nThe difference between wide and long datasets boils down to whether we prefer to have more columns \\nin our dataset or more rows.  \\n \\nWide Data  A dataset that emphasizes putting additional data about a single subject in columns is called \\na wide dataset because, as we add more columns, the dataset becomes wider.  \\n \\nLong Data  Similarly, a dataset that emphasizes including additional data about a subject in rows is called \\na long dataset because, as we add more rows, the dataset  becomes longer. It's important to point out that \\nthere's nothing inherently good or bad about wide or long data.  \\nIn the world of data wrangling, we sometimes need to make a long dataset wider, and we sometimes need \\nto make a wide dataset longer. However, it is true that, as a general rule, data scientists who embrace the \\nconcept of tidy data usually prefer longer datasets over wider ones.  \\n \\nQ19: What are the data no rmalization method you have applied , and \\nwhy?  \\n \\nNormalization  is a technique often applied as part of data preparation for machine learning. The goal of \\nnormalization is to change the values of numeric columns in the dataset to a common scale, without \\ndistorting differences in the ranges of values. For machine learning, every dataset does not require \\nnorma lization. It is required only when features have different ranges.  \\nIn simple words, when multiple attributes are there , but attributes have values on different scales, this \\nmay lead to poor data models while performing data mining operations. So they are no rmalized to bring \\nall the attributes on the same scale , usually something between (0,1).  \\nIt is not always a good idea to normali ze the data since we might lose information about maximum and \\nminimum values. Sometimes it is a good idea to do so.  \\nFor example , ML algorithms such as Linear Regression or Support Vector Machines typically converge \\nfaster on normali zed data. But on algorithms like K -means or K Nearest Neighbours , normali zation could \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6d85ddff-c618-4d02-8bae-73296c33dff3', embedding=None, metadata={'page_label': '51', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  17 | 18 \\n \\nbe a good choice or a bad depending on the use case since the dista nce between the points plays a key \\nrole here.  \\n  \\n \\nTypes of Normalisation :  \\n1 Min -Max Normalization:  In most cases, standardization is used feature -wise \\n 2 Z-score normalization  In this technique, values are normalized based on a mean and standard \\ndeviation of the data  \\n \\n \\nv’, v is new and old of each entry in data respectively. σA, A is the standard deviation and mean of A \\nrespectively.  \\nstandardization (or Z -score normalization) is that the features will be rescaled so that they’ll have the \\npropert ies of a standard normal distribution with  \\nμ=0 and σ=1 where μ is the mean (average) and σ is the standard deviation from the mean; standard \\nscores (also called z scores) of the samples are calculated as follows:  \\nz=(x−μ)/σ \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f632b959-ac79-49ad-baa0-501bae98b831', embedding=None, metadata={'page_label': '52', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  18 | 18 \\n \\nQ20: What is the difference b etween normalization   and  \\n          Standardization  with example ? \\n \\nIn ML, every practitioner knows that feature scaling is an important issue. The two most discussed \\nscaling methods are  Normalization  and Standardization . Normalization typically means it rescales the \\nvalues into a range of [0 ,1]. \\nIt is an alternative approach to Z -score normalization (or standardization) is the so -called Min -Max \\nscaling (often also called “normalization” - a common cause for ambiguities). In this approach, the data \\nis scaled to a fixed range - usually 0  to 1. Scikit-Learn provides a transformer  called  MinMaxScaler  for \\nthis. A Min -Max scaling is typically done via the following equation:  \\nXnorm = X -Xmin/Xmax -Xmin  \\nExample with sample data:  Before Normalization:  Attribute Price in Dollars Storage Space Camera  \\n\\uf0b7 Attribute Price in Dollars Storage Space Camera  \\n\\uf0b7 Mobile 1 250 16 12  \\n\\uf0b7 Mobile 2 200 1 6 8 \\n\\uf0b7 Mobile 3 300 32 16  \\n\\uf0b7 Mobile 4 275 32 8  \\n\\uf0b7 Mobile 5 225 16 16  \\nAfter Normalization: (Values ranges from 0 -1 which is working as expected)  \\n\\uf0b7 Attribute Price in Dollars Storage Space Camera  \\n\\uf0b7 Mobile 1 0.5 0 0.5  \\n\\uf0b7 Mobile 2 0 0 0  \\n\\uf0b7 Mobile 3 1 1 1  \\n\\uf0b7 Mobile 4 0.75 1 0  \\n\\uf0b7 Mobile  5 0.25 0 1  \\nStandardization (or Z -score normalization) typically means rescales data to have a mean of 0 and a \\nstandard deviation of 1 (unit variance) Formula:  Z or X_new=(x− μ)/σ where μ is the mean (average) , \\nand σ is the standard deviation from the mean; standard scores (also called z scores) Scikit -Learn \\nprovides a transformer called StandardScaler for standardization  Example:  Let’s take an approximately \\nnormally distributed set of numbers: 1, 2, 2, 3, 3, 3, 4, 4, and 5. Its mean is 3 , and its  standard dev iation: \\n1.22. Now, let’s subtract the mean from all data points. we get a new data set of: -2, -1, -1, 0, 0, 0, 1, 1, \\nand 2. Now, let’s divide each data point by 1.22. As you can see in the picture below, we get: -1.6, -0.82, \\n-0.82, 0, 0, 0, 0.82, 0.82, an d 1.63  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fb7f6556-3749-4f92-b436-1c8ecc96072b', embedding=None, metadata={'page_label': '53', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW PREPARATION  \\n(30 Days of Interview  \\nPreparation)  \\n \\n# DAY 04  \\n  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d07a0813-7017-42d9-9cd7-07d90e62ab82', embedding=None, metadata={'page_label': '54', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\n \\nQ1. What is upsampling and downsampling with examples?  \\nThe classification data set with skewed class proportions is called an \\nimbalanced data set. Classes which make up a large proportion of the data \\nsets are cal led majority classes. Those make up smaller proportion s are \\nminority classes.  \\nDegree of imbalance Prop ortion of Minority Class  \\n1>> Mild 20 -40% of the data set  \\n2>> Moderate 1 -20% of the data set  \\n3>> Extreme <1% of the data set  \\nIf we  have an imbalanced data set, first try training on the true distribution. \\nIf the model works well and generali ses, you are  done! If not, try the \\nfollowing up sampling  and down sampling  technique.  \\n1. Up-sampling  \\nUpsampling  is the process of randomly duplicating observations from the \\nminority class to reinforce its signal.  \\nFirst, we will  import the resampling module from Scikit -Learn:  \\nModule for resampling Python  \\n1- From  sklearn.utils import resample  \\nNext, we will  create a new Data Frame  with an up -sampled minority class. \\nHere are the steps:  \\n1- First, we will  separate observations from each class into different Data \\nFrames . \\n2- Next, we will  resample the minority class with replacement, setting the \\nnumber of samples to match that of the majority class.  \\n3- Finally, we'll combine the up -sampled minority class Data Frame  with the \\noriginal majority class Data Frame . \\n2-Down -sampling  \\nDownsampling  involves randomly removing observations from the majority \\nclass to prevent its signal from dominating the learning algorithm.  \\nThe process is similar to that of sampling . Here are the steps:  \\n1-First, we will  separate observations from each class into different Data \\nFrames . \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2e7822e0-31bb-4936-913d-69b1fc519285', embedding=None, metadata={'page_label': '55', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n2-Next, we will  resample the majority class without replacement, setting the \\nnumber of samples to match that of the minority class.  \\n3-Finally, we will  combine the down -sampled majority class Data Frame  \\nwith the original minority  class Data Frame . \\nQ2. What is the statistical  test for data validation with an example,  \\n        Chi-square, ANOVA  test, Z statics, T statics, F statics,  \\n Hypothesis Testing ? \\nBefore discussing the different statistical test , we need to get a clear \\nunderstanding of what a null hypothesis is. A null hypothesis  proposes that \\nhas no si gnificant difference exists in the set of a given observation . \\nNull:   Two sample s mean are equal. Alternate: Two samples mean  are not \\nequal . \\nFor rejecting the  null hypothesis, a test is calculated. Then the test statistic \\nis compared with a critical value , and if  found to be greater than the critical \\nvalue , the hypothesis will be  rejected.  \\nCritical Value: - \\nCritical  value s are the  point beyond which we reject th e null  hypothesis. \\nCritical value tells us, what is the probability of N number of samples, \\nbelonging  to the same distribution. Higher, the critical value which means \\nlower the probability of N number of  samples belonging to the same \\ndistribution.  \\nCritical  values can be used to do hypothesis testing in the following way . \\n1. Calculate test statistic  \\n2. Calculate critical values based on the significance level alpha  \\n3. Compare test statistic s with critical values.  \\nIMP-If the test statistic is lower than the critical value, accept the hypothesis \\nor else reject the hypothesis.  \\nChi-Square Test: - \\nA chi -square test is used if there is a relationship between two categorical \\nvariables.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8ae83479-49fd-486b-9490-90213f9be0fe', embedding=None, metadata={'page_label': '56', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nChi-Square test is used to determine whether there is a significant \\ndifference between the  expected frequency and the observed frequency  in \\none or more categories. Chi -square is also called the non -parametric test \\nas it will not use any parameter  \\n \\n \\n \\n2-Anova test: - \\nANOVA, also  called a n analysis of variance, is used to compare multiple s \\n(three or more) samples with a single test.  \\nUseful when there are more than three  populations. Anova compares the \\nvariance within and between the groups of the population . If the variation is \\nmuch larger than the within variation, the means of different s amples will \\nnot be equal. If the between and within variations are approximately  the \\nsame size, then there will be no significant difference between sample \\nmeans. Assumptions of ANOVA: 1 -All populations involved follow a normal \\ndistribution. 2 -All populati ons have the same variance (or standard \\ndeviation). 3 -The samples are randomly selected and independent of one \\nanother.  \\nANOVA  uses the mean of the samples or the population to reject or \\nsupport the null hypothesis. Hence it is called parametric testing.  \\n3-Z Statics: - \\nIn a z -test, the sample s are assumed to be normal  distributed. A z score  is \\ncalculated  with population parameters  as “population mean” and \\n“population standard deviation” and it is used to validate a hypothesis that \\nthe sample drawn belongs to the same population.  \\nThe statistics used for this hypothesis testing is called z -statistic, the score \\nfor which is calculated as z = (x — μ) / (σ / √n), where x= sample mean μ = \\npopulation mean σ / √n = population standard deviation If the test statistic is \\nlower than the critical value, accept the hypothesis or else reject the \\nhypothesis  \\n4- T Statics: - \\nA t-test used to compare the mean of  the given samples. Like  z-test,  t -test \\nalso assumed a normal distribution of the sample s. A t-test is used when \\nthe population parameters (mean  and standard deviation) are  unknown.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1555bfb5-f64c-4436-87bd-7f58a0be7106', embedding=None, metadata={'page_label': '57', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nThere are  three versions of t -test \\n1. Independe nt samples t -test which compare  mean s for two groups  \\n2. Paired sa mple t -test which compares mean  from the same group at \\ndifferent times  \\n3. Sample t -test, which  tests  the mean of the single group against  the \\nknown mean. The statistic for  hypothesis testing is called t -statistic, \\nthe score for which is calculated as t = (x1 — x2) / (σ / √n1 + σ / √n2), \\nwhere  \\n              x1 =  It is mean of sample A, x2 = mean of sample B,   \\n              n1 = size of sample 1 n2 = size of    sample 2  \\n \\n \\n \\n5- F Statics: - \\nThe F -test is designed to test if the two population variances are equal. It \\ncompare s the ratio of  the two variances. Therefore , if the variances are \\nequal , then the ratio of the variances will be 1.  \\nThe F -distribution is the ratio of two independent chi -square variables \\ndivided by their respective degrees of freedom.  \\nF = s1^2 / s2^2 and where s1^2 > s2^2 . \\nIf the null hypothesis is true , then the F test -statistic given above can be \\nsimplified . This ratio of sample variances will be test ed statistic used. If the \\nnull hypothesis is false, then we will reject the null hypothesis that the ratio \\nwas equal to 1 and our assumption that they were equal.  \\n \\nQ3. What is the Central limit theorem?  \\nCentral Limit Theorem  \\nDefinition: The theorem states that as the size of the sample increases, the \\ndistribution of the mean across multiple samples will approximate a \\nGaussian distribution (Normal). Generally,  sample sizes equal to or greater \\nthan 30 are consider  sufficient for the CLT to hold. It means that the \\ndistribution of the sample means is normally  distributed. The average of the ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e2b02751-0638-4156-a701-9f3d80ccca96', embedding=None, metadata={'page_label': '58', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nsample means will be equal to the population mean. This is the key aspect \\nof the theorem.  \\nAssumptions:  \\n1. The data must follow the randomization condition. It must be sampled \\nrandomly  \\n2. Samples should be independent of each other. One sample should \\nnot influence the other samples  \\n3. Sample size should be no more than 10% of the populati on when \\nsampling is done without replacement  \\n4. The sample size should be sufficiently large. The mean of the sample \\nmeans is denoted as:  \\nµ X̄ = µ \\nWhere , \\nµ X̄ = Mean of the sample means µ= Population mean and, the standard \\ndeviation of the sample mean is denoted as:  \\nσ X̄ = σ/sqrt(n)  \\nWhere , \\nσ X̄ = Standard deviation of the sample mean σ = Population standard \\ndeviation n = sample size  \\nA sufficiently large sample size can predict the characteristics of a \\npopulation accurately. For Example,  we shall take a uniformly distributed \\ndata:  \\nRandomly distributed data: Even for a randomly ( Exponential) distributed \\ndata the plot of the means is normally distributed.  \\nThe a dvantage of CLT is that we need not worry about the actual data \\nsince the means o f it will always be normally distributed. With this , we can \\ncreate component intervals, perform T -tests and ANOVA  tests from the \\ngiven samples.  \\n \\nQ4. What is the correlation  and co efficient?  \\nWhat is the Correlation Coefficient?  \\nThe correlation coefficient is a statistical measure that calculates the \\nstrength of the relationship between the relative movements of two ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8c69b4b9-529b-4bd3-9b2f-7e03345dd5fc', embedding=None, metadata={'page_label': '59', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nvariables. We use it to measure both the strength and direction of a linear \\nrelationship between two variables the values range between -1.0 and 1.0. \\nA calculated number greater than 1.0 or less than -1.0 means that there \\nwas an error in the correlation measurement. A correlation of -1.0 shows a \\nperfect negative correlation, while a correlation of 1.0 shows a perfect \\npositive correlation.  \\n \\n \\n \\nCorrelation coefficient formulas are used to find how strong a relationship is \\nbetween data. The formulas return a value between -1 and 1, where:  \\n1 indicates a strong positive relationship. -1 indicates a strong negative \\nrelationship. A result of zero indicates no relationship at all.  \\n \\nMeaning  \\n1. A correlation coefficient of 1 means that for every positive increase in \\none variable, there is a positive increase in a fixed proportion in the \\nother. For example, shoe sizes go up in (almost) perfect correlation \\nwith foot length.  \\n2. A correlation coefficient of -1 means that for every positive increase in \\none variable, there is a negative decrease of a fixed proportion in the \\nother. For example, the amount of gas in a tank decreases in  (almost) \\nperfect correlation with speed.  \\n3. Zero means that for every increase, there isn’t a positive or negative \\nincrease. The two just aren’t related.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='68493c10-80bb-42b1-a7e9-14b4ccd63683', embedding=None, metadata={'page_label': '60', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nWhat is a Negative Correlation?  \\nNegative correlation is a relationship between two variables in which one \\nvariable increases as the other decreases, and vice versa. In statistics, a \\nperfect negative correlation is represented by the value -1. Negative \\ncorrelation or inverse correlation is a relationship between two variables \\nwhereby they move in opposite d irections. If variables X and Y have a \\nnegative correlation (or are negatively correlated), as X increases in value, \\nY will decrease; similarly, if X decreases in value, Y will increase.  \\nWhat Is Positive Correlation?  \\nPositive correlation is a relationship between two variables in which both \\nvariables move in tandem —that is, in the same direction. A positive \\ncorrelation exists when one variable decreases as the other variable \\ndecreases or one variable increases while the other increases.  \\n \\n \\n \\nWe use the corre lation coefficient to measure the strength and direction of \\nthe linear relationship between two numerical variables X and Y. The \\ncorrelation coefficient for a sample of data is denoted by r.  \\nPearson Correlation Coefficient  \\nPearson is the most widely used c orrelation coefficient. Pearson correlation \\nmeasures the linear association between continuous variables. In other \\nwords, this coefficient quantifies the degree to which a relationship between \\ntwo variables can be described by a line. Formula  developed by Karl \\nPearson over 120 years ago is still the most widely used today. The \\nformula for the correlation (r) is  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2f562413-a1b0-4958-8833-a45b2fcb577d', embedding=None, metadata={'page_label': '61', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n \\nWhere  n is the number of pairs of data;  \\nAre the sample means of all the x -values and all the y -values, respectively; \\nand sx and sy a re the sample standard deviations of all the x - and y -values, \\nrespectively.  \\n1. Find the mean of all the x -values and mean of all y -values.  \\n2. Find the standard deviation of all the x -values (call it sx) and the \\nstandard deviation of all the y -values (call it sy) . For example, to find \\nsx, you would use the following equation:  \\n3. For each of the n pairs (x, y) in the data set, take  \\n4. Add up the n results from Step 3.  \\n5. Divide the sum by sx ∗ sy. \\n6. Divide the result by n – 1, where n is the number of (x, y) pairs. (It’s \\nthe same as multiplying by 1 over n – 1.) This gives you the \\ncorrelation, r.  \\n \\nQ5: What is the d ifference between mac hine learning and deep  \\n       learning?  \\nMachine  Learning  | deep learning  \\nMachine  Learning  is a technique  to learn  from that data and then apply  wha\\nt has been  learnt  to make  an informed  decision  | The main  difference  betwe\\nen deep  and machine  learning  is, machine  learning  models  become  better  \\nprogressively  but the model  still needs some  guidance.  If a machine -\\nlearning  model  returns  an inaccurate  prediction  then the programmer  need\\ns to fix that problem  explicitly  but in the case  of deep  learning,  the model  do\\nes it by himself.   \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='77eab393-fa4f-43ee-bde2-b5c4f0117f29', embedding=None, metadata={'page_label': '62', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n>Machine  Learning  can perform  well with small  size data also | Deep  Learn\\ning does not  perform  as good  with smaller  datasets.  \\n>Machine  learning  can work  on some  low-\\nend machines  also  | Deep  Learning  involves  many  matrix  multiplication  op\\nerations  which  are better  suited  for GPUs  \\n>Features  need  to be identified  and extracted  as per the domain  before  pu\\nshing  them  to the algorithm  | Deep  learning  algorithms  try to learn  high-\\nlevel features  from data.  \\n>It is generally  recommended  to break  the problem  into smaller  chunks,  sol\\nve them  and then combine  the results  | It generally  focusses  on solving  the \\nproblem  end to end \\n>Training  time is comparatively  less | Training  time is comparatively  more  \\n>Results  are more  interpretable  | Results  Maybe  more  accurate  but less int\\nerpretable  \\n> No use of Neural  networks  | uses  neural  networks  \\n> Solves  comparatively  less complex  problems  | Solves  more  complex  prob\\nlems . \\nQ6: What is perceptron and how it is related to human neuron s? \\nIf we focus on the structure of a biological neuron, it has dendrites, which  \\nare used to receive inputs. These inputs are summed in the cell body and \\nusing the Axon it is passed on to the next biological neuron as shown \\nbelow.  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5272df9d-5feb-43ca-9ba8-69a4f1754017', embedding=None, metadata={'page_label': '63', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nDendrite:  Receives signals from other neurons  \\nCell Body:  Sums all the inputs  \\nAxon:  It is used to transm it signals to the other cells  \\nSimilarly, a perceptron receives multiple inputs, applies various \\ntransformations and functions and provides an output. A Perceptron is a \\nlinear model used for binary classification. It models a neuron, which  has a \\nset of inpu ts, each of which is given a specific weight. The neuron \\ncomputes some function on these weighted inputs and gives the output.  \\n \\n \\nQ7: Why deep learning is better than  machine learning ? \\nThough traditional ML algorithms solve a lot of our cases, they are not \\nuseful while working with high dimensional data that  is where we have a \\nlarge number of inputs and outputs. For example, in the case of \\nhandwriting recognition, we have a large amount of input where we will \\nhave different type s of inputs associated with different type s of handwriting.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5afbb188-7a95-406c-898e-c8e75d58a0e3', embedding=None, metadata={'page_label': '64', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n \\nThe second major challenge is to tell the computer what are the features it \\nshould look for that will play an important role in predicting the outcome a s \\nwell as to achieve better accuracy while doing so.  \\nQ8: What kind of problem can be solved by using deep learning?  \\nDeep Learning is a branch of Machine Learning, which  is used to solve \\nproblems in a way that mimics the human way of solving problems. \\nExamples:  \\n\\uf0b7 Image recognition  \\n\\uf0b7 Object Detection  \\n\\uf0b7 Natural Language processing - Translation, Sentence formations, text \\nto speech, speech to text  \\n\\uf0b7 understand the semantics of actions  \\nQ9: List down all the activation funct ion using mathematical    \\n       Expression and example. What is the activation function?  \\nActivation functions are very important  for an Artificial Neural Network to \\nlearn and make sense of something complicated and the Non-linear \\ncomplex functional mappings between the inputs and response variable. \\nThey  introduce non -linear properties to our Network. Their  main purpose s \\nare to convert a n input signal of a node in a n A-NN to an output signal.  \\nSo why do we need Non -Lineari ties?  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='84e298ec-20e3-4444-8505-1a04a6ff1ad1', embedding=None, metadata={'page_label': '65', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nNon-linear functions are those, which have a degree more than one,  and \\nthey have a curvature when we plot a Non -Linear function. Now we need a \\nNeural Network Model to learn and represent almost anything and any \\narbitrary complex function, which  maps inputs to outputs. Neural -Networks \\nare considered Universal Function Approximations . It means that they can \\ncompute and learn any function at all.  \\nMost popular types of Activation functions - \\n\\uf0b7 Sigmoid or Logistic  \\n\\uf0b7 Tanh — Hyperbolic tangent  \\n\\uf0b7 ReLu -Rectified li near units  \\nSigmoid Activation function:  It is a activation function of form f(x) = 1 / 1 \\n+ exp( -x) . Its Range is between 0 and 1. It is a n S-shaped curve. It is easy \\nto understand.  \\n \\nHyperbolic Tangent function - Tanh :  It’s mathematical  formula is f(x) = 1 \\n— exp(-2x) / 1 + exp( -2x). Now it’s the output is zero centred because its \\nrange in between -1 to 1 i.e. -1 < output < 1 . Hence optimi sation is easier \\nin this method ; Hence in practice , it is always preferred over Sigmoid \\nfunction.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4ad39560-1e56-4f3b-a088-b97270749dfe', embedding=None, metadata={'page_label': '66', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n \\nReLu - Rectified Linear units:  It has become more  popular in the past \\ncouple of years. It was rece ntly proved that it has  six times improvement in \\nconvergenc e from Tanh function. It’s R(x) = max ( 0,x) i.e. if x < 0 , R(x) = 0 \\nand if x >= 0  , R(x) = x. Hence as seen that  mathematical  form of this \\nfunction , we can see that it is very simple and efficient. Many  times in \\nMachine learning and computer science we notice that most simple and \\nconsistent techniques and methods are only preferred and  are the best. \\nHence,  it avoids and rectifies the vanishing gradient problem. Almost all the \\ndeep learning Models use ReLu nowadays.  \\n \\n \\nQ10: Detail explanation  about gradient decent using exam ple and  \\n       Mathematical expression?  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='28158181-05cf-44a0-91c4-e18c32185b26', embedding=None, metadata={'page_label': '67', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nGradient descent is an optimis ation algorithm used to minimize some \\nfunction by iteratively moving in the direction of steepest descent as \\ndefined by  negative of the gradient. In machine learning, we use d gradient \\ndescent to update the parameters of our model. Parameters refer t o \\ncoefficients in the Linear Regression and weights in neural networks.  \\n \\nThe s ize of these steps  called the learning rate. With the  high learning rate, \\nwe can cover more ground each step, but  we risk overshooting the lower  \\npoint since the slope of the hill is constantly changing. With a very low er \\nlearning rate, we can confidently move in the direction of the negativ e \\ngradient because we are r ecalculating it so frequently. The L ower learning \\nrate is more precise, but calculating the gradient is time -consuming, so it \\nwill take a very large  time to get to the bottom.  \\nMath  \\nNow let’ s run gradient descent using  new cost function . There are two \\nparameters in  cost function we can control: m (weight) and b (bias). Since \\nwe need to consider that the impact each one has on the final prediction, \\nwe need to use partial derivatives. We calculate  the partial derivative  of the \\ncost function concerning  each parameter and store the results in a \\ngradient.  \\nMath  \\nGiven the cost function:  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0c3cc131-4f79-4022-8d70-9baf7543fea9', embedding=None, metadata={'page_label': '68', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n \\nTo solve for the gr adient, we iterate  by our data points using our new m \\nand b values and compute the partial derivatives. This new gradient te lls us  \\nabout  the slope of  the cost function at our current position (current \\nparameter values) and the direction s we should move to update our \\nparameters. The learning rate controls the size of our upd ate. \\nQ11: What is backward  propagation?   \\nBack -propagation is the essen ce of the neural net training and this  \\nmethod of fine -tuning the weights of a neural net based on the error s rate \\nobtained in the previous epoch. Proper tuning of the weights allows us  to \\nreduce error rates and to make the model reliable by increasing its \\ngenerali sation.  \\nBackpropagation is a short form of \"backward propagation of errors.\" This \\nis the standard method of training artific ial neural networks. This  helps to \\ncalculate the gradient of a loss function with respects to all the weights in \\nthe network.  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f9dff176-d541-4be6-9d8e-a440f80dcf26', embedding=None, metadata={'page_label': '69', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nMost prominent advantages of Backpropagation are:  \\n\\uf0b7 Backpropagation is the fast, simple and easy to program . \\n\\uf0b7 It has no parameters to tune apart from the numbers of input . \\n\\uf0b7 It is the flexible method as it does not require prior knowledge about \\nthe network  \\n\\uf0b7 It is the  standard method that generally works well . \\n\\uf0b7 It does not need any special mention s of the features of the function \\nto be learned . \\n \\n \\n \\nQ12: How we a ssign weights in deep learning?  \\nWe already know that in a neural network, weights are usually initialised  \\nrandomly and that kind of initiali sation takes a fair/significant amount of \\nrepetitions to converge to the least loss and reach the ideal wei ght matrix. \\nThe problem is, that  kind of initiali sation is prone to vanishing or exploding \\ngradient problems.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c920fa0a-d169-4f44-ab02-dcfec716a2ca', embedding=None, metadata={'page_label': '70', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nGeneral ways to make it initiali se better  weights:  \\nReLu activa tion function in the deep nets.  \\n1. Generate a random sample of weights from a Gaussian \\ndistribution having mean 0 and a standard deviation of 1.  \\n2. Multiply the  sample with the square root of (2/ni). Where ni is the \\nnumber of input units for that layer.  \\nb) Likewise , if you’re using Tanh activation function  : \\n1. Generate a random sample of weights from a Gaussian distribution \\nhaving mean 0 and a standard deviation of 1.  \\n2. Multiply the sample with the square root of (1/ni)  where ni is several  \\ninput units for that layer.  \\n \\nQ13: What is optimi ser is deep learning, and which one is  the best?  \\nDeep learning is an iterative process. With so many  hyper parameters  to \\ntune or methods to try, it is important to be able to train models fast, to \\nquickly complete the iterative cycle. This is the key to increase  the speed \\nand efficiency of a machine learning team.  \\nHence the importance of optimi sation algorithms such as s tochastic \\ngradient descent, min -batch gradient descent, gradient descent with \\nmomentum and the Adam optimi ser. \\nAdam optimi ser is the best one.  \\nGiven an algorithm  f(x), it helps in either minimisation or maximisation  of \\nthe value of  f(x). In this context of deep learning, we use optimi sation \\nalgorithms to train the neural network by optimi sing the cost function  J.  \\nThe cost function is defined as:  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='59d9cdf6-6a58-4497-bbf0-d9626d612386', embedding=None, metadata={'page_label': '71', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n \\n \\nThe value of the cost function  J is the mean of the loss  L between the \\npredicted value  y’ and actu al value  y. The value  y” is obtained during the \\nforward propagation step and makes use of the Weights  W and biases  b of \\nthe network. With the help of optimi sation algorithms, we minimi se the \\nvalue of Cost Function  J  by updating the values of trainable \\nparameters  W and b. \\nQ14: What is gradient de scent, mini -batch gradient de scent, batch  \\n         gradient decent, stochastic gradient decent and adam?  \\nGradient Descent  \\nit is an  iterative machine learning optimi sation algorithm to  reduce the cost \\nfunction, and  help models to make accurate predictions.  \\nGradient indicates the direction of increase. As we want to find the \\nminimum point s in the valley , we need to go in the opposite direction of the \\ngradient.  We update  the parameters in  the negative gradient direction to \\nminimi se the loss.  \\n \\nWhere θ is the weight parameter, η is the learning rate , and ∇J(θ;x,y) is the \\ngradient of weight parameter θ \\nTypes of Gradient Descent  \\nDifferent types of Gradient descents are  \\n\\uf0b7 Batch Gradient Descent or Vanilla Gradient Descent  \\n\\uf0b7 Stochastic Gradient Descent  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='74b1fe48-d69a-4619-b360-dd4e1dad7a7e', embedding=None, metadata={'page_label': '72', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n\\uf0b7 Mini batch Gradient Descent  \\nBatch Gradient Descent  \\nIn the batch gradient , we use the entire dataset to compute the gradient of \\nthe c ost function for each iteration for gradient descent and then update  the \\nweights.  \\nStochastic Gradient descent  \\nStochastic gradient descent , we use a single data  point or example to \\ncalculate the gradient and update the weights with every iteration.  \\nWe first need to  shuffle the dataset s so that we get a completely \\nrandomi sed dataset. As the dataset s are random  and weights , are updated \\nfor every single example, an update of the weights and the cost functions \\nwill be noisy jumping all over the place   \\nMini Batch Gradient descent  \\nMini-batch gradient s is a variation of stochastic gradient descent where \\ninstead of a single training example, a mini-batch of samples are  used.  \\nMini -batch gradient descent is widely used and converges faster and is \\nmore stable . \\nThe batch size can vary depend ing upon  the dat aset.  \\nAs we take batches with different samples, it reduces the noise which is a \\nvariance of the weights updates , and that helps to have a more stable \\nconverge faster . \\nQ15: What are auto encoders ? \\nAn autoencoder , neural network s that ha ve three layers:  \\nAn input layer, a hidden layer  which is also known as encoding layer , and a \\ndecoding layer. This  network is trained to reconstruct its inputs, which \\nforces the hidden layer to try to learn good representations of the inputs.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='404175f4-3a89-45b7-a959-84a3da6ccd3b', embedding=None, metadata={'page_label': '73', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nAn autoencoder  neural network is a n unsupervised Machine -learning  \\nalgorithm that applies backpropagation, setting the target values to be \\nequal to the inputs. An autoencoder  is trained to attempt s to copy its input \\nto its output. Intern ally, it has a hidden layer which  describes a code used \\nto represent the input.  \\n \\nAutoencoder  Components:  \\nAutoencoders  consists of 4 main parts:  \\n1- Encoder: In this,  the model learns how to reduce the input dimensions \\nand compress the input data into an encoded representation.  \\n2- Bottlene ck: In this,  the layer that contains the compressed \\nrepresentation of the input data. This is the lowest possible dimension of \\nthe input data.  \\n3- Decoder: In this,  the model learns how to reconstruct the data from  the \\nencod represented  to be as close to the original input s as possible.  \\n4- Reconstruction  Loss: In this  method that measures measure how well \\nthe deco der is performing and how closed the output is related to  the \\noriginal input.  \\nTypes of Autoencoders  : \\n1. Denoising auto encode r \\n2. Sparse auto encoder  \\n3. Variational auto encoder  (VAE)  \\n4. Contractive auto encoder  (CAE)  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c5a5d484-9ad9-4d73-b6bf-055cd0931bb2', embedding=None, metadata={'page_label': '74', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n \\n \\n \\n \\nQ16: What  is  CNN?  \\nThis is the simple application of a filter to an input that results  in \\ninactivation. Repeated application of the same filter to input results  in a \\nmap of activations called a feature map, indicating the locations and \\nstrength of a detected feature in input, such as an image.  \\nConvolutional layers are the major building blocks which are used in \\nconvolutional neural networks.  \\nA covnets is the sequence of layers, and every layer transforms one \\nvolume to another through differentiable function s. \\nDifferent t ypes of layers  in CNN : \\nLet’s take an example by running a covnets on of image of dimension s 32 x \\n32 x 3.  \\n1. Input Layer : It holds the raw input of image with width 32, height 32 \\nand depth 3.  \\n2. Convolution Layer:  It computes the output volume by computing dot \\nproduct s between all filters and image patch es. Suppose we use a \\ntotal of 12 filters for this layer we’ll get output volume of dimension 32 \\nx 32 x 12.  \\n3. Activation Function Layer:  This layer will apply the element -wise \\nactivation function to the output of the convolution layer. Some  \\nactivation functions are RELU: max(0, x), Sigmoid: 1/(1+e ^-x), Tanh, \\nLeaky RELU, etc. So the  volume remains un changed . Hence output \\nvolume will have dimension s 32 x 32 x 12.  \\n4. Pool Layer:  This layer  is periodically inserted  within the covnets , and \\nits main function is to reduce the size of volume which makes the ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5ea409e5-8d8f-4466-89f7-37ec1930c25c', embedding=None, metadata={'page_label': '75', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\ncomputation fast reduces memory and also prevents overfitting. Two \\ncommon types of pooling layers are  max pooling  and average \\npooling. If we use a max pool with 2 x 2 filters and stride  2, the \\nresultant volume will be of dimension 16x16x12.  \\n \\n5. Fully -Connected Layer:  This layer is a regular neural network layer \\nthat takes input from the previous layer and computes the class \\nscores and outputs the 1 -D array of size equal to the number of \\nclasses.  \\n \\n \\nQ17: What is  pooling, padding, filtering operations on CNN?  \\nPooling Layer  \\nIt is commonly used to periodically insert a Pooling layer in -between \\nsuccessive Conv layers in a ConvNet architecture. Its function is to \\nprogressively reduce the spatial size of the representation to reduce the \\nnumber  of parameters and computation in the network, and hence to also \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0f69cc95-cc3d-48ff-9d9d-ef6ea3e7bedb', embedding=None, metadata={'page_label': '76', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\ncontrol overfitting. The Pooling Layer operates independently on every \\ndepth slice of the input and resizes it spatially, using the MAX operation.  \\n \\nThe most common form is a pooling layer with filters of size 2x2 applied \\nwith a stride of 2 downsamples  every depth slice in the input by two along \\nboth width and height, discarding 75% of the activations. Every MAX \\noperation would , in this case,  be taking a max over four numbers (little 2x2 \\nregion in some depth slice). The depth dimension remain s unchanged.  \\n \\n \\n \\nQ18: What is the Evolution technique of CNN ? \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='51abbf0b-c4a4-4aec-97ab-733313694869', embedding=None, metadata={'page_label': '77', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nIt all started with LeNet in 1998 and eventually, after nearly 15 years, lead \\nto groundbreaking models winning the  ImageNet Large Scale Visual \\nRecognition Challenge  which includes AlexNet  in 2012 to Google Net  in \\n2014 to ResNet in 2015 to an ensemble of previous models in 2016. In the \\nlast two years, no significant progress has been made , and the new models \\nare an ensemble of previous groundbreaking  models.  \\n \\nLeNet in 1998  \\nLeNet  is a 7 -level convolutional network by LeCun in 1998 that classifies \\ndigits and used by several banks to recogni se the hand -written numbers on \\ncheques digiti sed in 32x32 pixel greyscale input  images.  \\nAlexNet in 2012  \\nAlexNet : It is considered to be the fir st paper/ model , which rose the \\ninterest in CNNs when it won the Ima geNet challenge in the year 2012. It  is \\na deep CNN trained on ImageNet and outperformed all the entries that \\nyear.  \\nVGG in 2014  \\nVGG was submitted in  the year  2013 , and it became a runner up  in the \\nImageNet contest in 2014. It is widely used as a simple architecture \\ncompared to AlexNet.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b4b576d6-d1c4-4301-8a63-b0eff411018f', embedding=None, metadata={'page_label': '78', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\n \\nGoogleNet in 2014  \\nIn 2014, several great models w ere developed like VGG , but the winner of \\nthe ImageNet contest was GoogleNet.  \\nGoogLeNet  proposed a module called the inception modules that includes \\nskipping connections in the network , forming a mini -module , and this \\nmodule is repeated throughout the network.  \\nResNet in 2015  \\nThere are 152 layers in the Microsoft ResNet. The authors showed \\nempirically that if you keep on adding layers , the error rate should keep on \\ndecreasing in contrast to “plain nets” we 're adding a few layers resulted in \\nhigher training and test errors.  \\nQ19: How to initiali se biases in deep learning?  \\nIt is possible and common to initiali se the biases to be zero since the  \\nrandom numbers in the weights provide the asymmetry braking . For ReLU \\nnon-linearities, some people like to use small constant value such as 0.01 \\nfor all biases because this ensures tha t all ReLU units fire in the beginning, \\ntherefore obtain,  and propagate some gradient . However, it is un clear if this \\nprovides a consistent improvement (in fact some results seem to indicate s \\nthat this performs worst ) and it is more common ly used to use 0 bias \\ninitialisation . \\nQ20: What is learning Rate?  \\nLearning Rate  \\nThe l earning rate controls how much we should adjust the weights \\nconcerning  the loss gradient. Learning rates are randomly initiali sed. \\nLower the value s of the learning rate slower will be the convergence to \\nglobal minima.  \\nHigher value s for the learning rate will not allow the gradient descent to \\nconverge  \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f96c6762-4264-453a-a55b-2bd583fece6a', embedding=None, metadata={'page_label': '79', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nSince our goal is to minimi se the  function  cost to find the optimi sed value \\nfor weights, we run  multiples iteration with different weights and calculate \\nthe cost to arrive at a minimum cost   \\n \\n----------------------------------------------------------------------------------------------------\\n------------------------  \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='020ea1fa-a23b-4052-8c87-f7adbb076923', embedding=None, metadata={'page_label': '80', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nData Science Interview Questions  Page 1 \\n \\n \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW PREPARATION  \\n(30 Days  of Interview  \\nPreparation ) \\n# Day-5 \\n \\n \\n \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='80bb4f6d-0e7f-4fa0-b4c6-575085679683', embedding=None, metadata={'page_label': '81', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nData Science Interview Questions  Page 2 \\n \\n \\n \\nQ1: What are E pochs?  \\nOne Epoch is  an ENTIRE dataset is passed forward s and backward s through the neural network.  \\nSince one epoch is too large  to feed to the computer at once , we divide it in to several smaller batches.  \\nWe always use  more than one Epoch because one epoch leads to underfitting .  \\nAs the number of epochs increases, several  times the weight are changed in the neural network and the \\ncurve goes from  underfitting  up to optimal  to overfitting  curve.  \\nQ2. What is the batch size?  \\nBatch Size  \\nThe total number of training and examples present in a single batch.  \\nUnlike the learning rate hyper parameter where its value doesn’t affect computational time, the batch \\nsizes must be examined  in conjunction s with the execution time of training. T he batch size is limited by  \\nhardware’s memory, while the learning rate is not. Leslie recommends usin g a bat ch size that fits in  \\nhardware’s memory and enable s using larger learning rate . \\nIf our  server has multiple GPUs, the total batch size is the batch size on a GPU multiplied by the number s \\nof GPU . If the architecture s are small or your hardware permits very large batch sizes, then you might \\ncompare the performance of different batch sizes. Also , recall that small batch sizes add regulari zation  \\nwhile large batch sizes add less, so utilize this while balancing the proper amo unt of regulari zation . It is \\noften better to use large  batch sizes so a larger learning rate can be used.  \\nQ3: What is dropout in Neural network?  \\nDropout refers to ignoring units during the training phase of a certain s et of neurons which is chosen \\nrandomly . These u nits are not considered during the  particular forward or backward pass.  \\nMore technically, at each training stage, individual nodes are either dropped out of the net with \\nprobability  1-p or kept with probability  p, so that a reduced network  is left; incoming and outgoing edges \\nto a dropped -out node are also removed.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e1dffdc8-db7d-4aa3-a76b-869e46721429', embedding=None, metadata={'page_label': '82', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nData Science Interview Questions  Page 3 \\n \\nWe need Dropout to prevent over -fitting  \\nA dropout is an approach to regulari zation  in neural networks which helps to reduce  interdependent \\nlearning amongst the neurons.  \\nWhere to use  \\nDropout is implemented per -layer in a neural network.  \\nIt can be used with most types of layers, such as dense fully connected layers, convolutional layers, and \\nrecurrent layers such as the long sho rt-term memory network layer.  \\nDropout may be implemented on any or all hidden layers in the network as well as the visible or input \\nlayer. It is not used on the output layer.  \\n \\nBenefits: - \\n1. Dropout forces a neural network to learn more robust features that ar e very useful in conjunction \\nwith different random subsets of the other neurons.  \\n2. Dropout generally  doubles the number of iterations required to converge. However, the training \\ntime for each epoch is less.  \\nQ4: List down hyperparameter  tuning in deep learning.  \\nThe process of setting the hyper -parameters requires expertise and extensive trial and error. There are no \\nsimple and easy ways to set hyper -parameters — specifically, learning rate, batch size, momentum, and \\nweight decay.  \\nApproac hes to searching for the best configuration:  \\n• Grid Search  \\n• Random Search  \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='589a6f22-3d6b-4a0c-a424-1764ed5d91bf', embedding=None, metadata={'page_label': '83', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nData Science Interview Questions  Page 4 \\n \\nApproach  \\n1. Observe and understand the clues available during training by  monitoring validation/test \\nloss early in training, tune your architecture and hyper -parameters with short runs of a few \\nepochs.  \\n2. Signs of  underfitting  or overfitting  of the test or validation loss early in the training process are \\nuseful for tuning the hyper -parameters . \\n \\nTools for Optimizing Hyperparameters  \\n• Sage Maker  \\n• Comet.ml  \\n• Weights  &Biases  \\n• Deep Cognition  \\n• Azure ML  \\n \\nQ5: What do you understand by activation func tion and error \\nfunctions?  \\nError functions  \\nIn most learning networks, an error is calculated as t he difference between the predicted output and the \\nactual  output.  \\n \\nThe function that is used to compute this error is known as Loss Function J(.). Different loss functions \\nwill give different errors for the same prediction, and thus have a considerable effect on the performance \\nof the model. One of the most widely used lo ss function is mean square error, which calculates the square \\nof the difference between the actual values and predicted value. Different loss functions are used to deal s \\nwith a different type of tasks, i.e. regression and classification . \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d5ba0187-a017-4cf7-9038-c03746edc4c3', embedding=None, metadata={'page_label': '84', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nData Science Interview Questions  Page 5 \\n \\nRegressive loss f unctions:  \\nMean Square Error  \\nAbsolute error  \\nSmooth Absolute Error  \\nClassification loss functions:  \\n1. Binary Cross -Entropy  \\n2. Negative Log -Likelihood  \\n3. Margin Classifier  \\n4. Soft Margin Classifier  \\nActivation function s decide  whether a neuron should be activated or not by calculating a weighted sum \\nand adding bias with it. The purpose of the activation function is to  introduce non -linearity  into the output \\nof a neuron.  \\nIn a neural network, we would update the weights and biase s of the neurons based on  the error at the \\noutput s. This process is known as  back -propagation . Activation function  make s the back -propagation \\npossible since the gradients are supplied along with the error s to update the weights and biases.  \\nQ6: Why do we need No n-linear activation functions?  \\n \\nA neural network without activation function s is essentially  a linear regression model. The activation \\nfunction s do the non -linear transformation to the input , making it capable of learning and performing  \\nmore complex  tasks. \\n1. Identity  \\n2. Binary Step  \\n3. Sigmoid  \\n4. Tanh  \\n5. ReLU  \\n6. Leaky ReLU  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4e95a826-c4bf-47f8-b10f-2c6acfe626ac', embedding=None, metadata={'page_label': '85', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nData Science Interview Questions  Page 6 \\n \\n7. Softmax  \\nThe activation function s do the non -linear transformation to the input , making it capable of learning and \\nperforming  more complex tasks.  \\nQ7: W hat do you under by vanishing gradient pr oblem and how can  \\n       Do we solve that?  \\nThe problem:  \\nAs more layers usi ng certain activation function are added to neural networks, the gradients of the loss \\nfunction approach zero, making the network s tougher  to train.  \\nWhy:  \\nCertain activation functions, like the sigmoid function, squishes a large input space into a small input \\nspace between 0 and 1. Therefore, a large change in the input of the sigmoid function will cause a small \\nchange in the output. Hence, the derivative becomes sm all. \\n \\nFor shallow network s with only a few layers that use these activations, this isn’t a big problem. However, \\nwhen more layers are used, it can cause the gradient to be too small for training to work effectively.  \\nHowever, when  n hidden layers use an activation like the sigmoid function,  n small derivatives are \\nmultiplied together. Thus, the gradient decreases exponentially as we propagate down to the initial layers.  \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0cb79047-26e5-43ff-8198-bd69b0287e7b', embedding=None, metadata={'page_label': '86', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nData Science Interview Questions  Page 7 \\n \\nSolutions:  \\nThe simplest solution is to use other activation functions, such as ReLU, which doesn’t cause a small \\nderivative.  \\nResidual networks are another solution, as they provide residual connections straight to earlier layers.  \\nFinally, batch normali zation  layers can also resolve the issue.  \\nQ8: What is Transfer  learning in deep learning  ? \\nTransfer learning : It is a machine learning method where a model is developed for the task is again used \\nas the starting point for a model on a second task.  \\nIt is a popular approach in deep learning where pre -trained models are  used as the starting point on \\ncomputer vision and natural language processing tasks given the vast compute and time resources \\nrequired to develop neural network models on these problems  \\nTransfer learning is a machine learning technique where a model train ed on one task is re -purposed on a \\nsecond related task.  \\nTransfer learning is an optimi zation  that allows rapid progress or improved performance when mode lling \\nthe second task.  \\nTransfer learning only works in deep learning if the model features learned from the first task are general.  \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3d5b9f00-4d35-4fb9-be97-0704443cb33d', embedding=None, metadata={'page_label': '87', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nData Science Interview Questions  Page 8 \\n \\n \\nQ9: W hat is VGG16  and explain the architecture of VGG16 ? \\nVGG -16 is a simpler architecture model since it’s not using m any hyperparameters. It always uses 3 x 3 \\nfilters with the stride of 1 in convolution layer and uses SAME padding in pooling layers 2 x 2 with a \\nstride of 2.  \\n \\n \\nThis architecture is from the VGG group, Oxford. It improves  AlexNet by replacin g the large kernel -\\nsized filter  with multiple 3X3 kernel -sized filters one after another. With a given receptive field(the \\neffective area size of input image on which output depends), multiple stacked smaller size kernel is better \\nthan the one with a larger size kernel because multiple non-linear layers increases the depth of the \\nnetwork which enables it to learn more complex features, and that too at a lower cost.   \\nThree fully connected layers follow the VGG convolutional  layers. The width of the network s start s at \\nthe small value of 64  and increases by a factor of 2 after every sub -sampling/pooling layer. It achieves \\nthe top -5 accuracy of 92.3 % on ImageNet.  \\n \\nQ10: W hat is RESNET?  \\nThe winner of ILSRVC 2015, it also called as Residual Neural Network (ResNet) by Kaiming. This \\narchitecture introduced a concept called “skip connections”. Typically, the input matrix calculates in two \\nlinear transformation s with ReLU activation function. In Residual network, it directly cop ies the input \\nmatrix to the second transformation output an d sum s the output in final ReLU function.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='98518c92-cb0a-404d-91c5-d41247cf726b', embedding=None, metadata={'page_label': '88', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nData Science Interview Questions  Page 9 \\n \\n \\nSkip Connection  \\n \\nExperiments in paper four can judge the power of the residual network . The p lain 34 layer network had \\nhigh validation error than the 18 layers plain network. This is where we realize  the degrada tion problem s. \\nAnd the same 34 layers networ k when converted to  the residual network has much less training error \\nthan the 18 layer s residual network.   \\nQ11:  What is Image Net? \\nImageN et is a project aimed at (manually) labe lling and categori zing images into almost 22,000 separate \\nobject categories for computer vision research es. \\nWhen we hear the about “ImageNet” in the context of deep learning and Convolutiona l Neural Network, \\nwe are  referring to ImageNet Large Scale Visual Recognition Challenge . \\nThe main aim  of this image classi fication challenge is to train the model that can correctly classify an \\ninput image into the 1,000 separate object s category . \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='db0c6624-fece-4a1a-b2d1-add98eb99420', embedding=None, metadata={'page_label': '89', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nData Science Interview Questions  Page 10 \\n \\nModels are trained on  the ~1.2 million training images with another 50,000 images for validation and \\n100,000 images for testing.  \\nThese 1,000 image categories represent object classes that we encounter in our day -to-day lives, such as \\nspecies of dogs, cats, various household obj ects, vehicle types, and much more.  \\nWhen it comes to  the image classification, the ImageNet challenge is the  “de facto  “ benchmark for \\ncomputer vision classification algorithms — and the leaderboard for this challenge has \\nbeen  dominated  by Convolutional N eural Networks and Deep learning techniques since 2012.  \\n \\nQ12:  What is DarkNet? \\nDark Net is a framework  used to train neural networks ; it is open source and written in C/CUDA and \\nserves as the basis for YOLO. Darknet is also used as the framework for training YOLO, meaning it sets \\nthe architecture of the network.  \\nClone the repo locally , and you have it. To compile it, run a  make. But fir st, if you intend to use the GPU \\ncapability , you need to edit the  Makefile  in the first two lines, where you tell it to compile for GPU usage \\nwith CUDA drivers.  \\nQ13: What is YOLO  and explain the architecture of YOLO (you only  \\n Look Once). One use case?  \\nYOLO v1  \\nThe first YOLO You only look once (YOLO ) version  came about May 2016 and sets the core of the \\nalgorithm, the following versions are improvements that fix some drawbacks.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='43cf70e0-b969-4d52-a5a0-eadee1eb828e', embedding=None, metadata={'page_label': '90', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nData Science Interview Questions  Page 11 \\n \\nIn short, YOLO is a network “inspired by”  Google Net . It has 24 convolutional layers working as the \\nfeature extractors and two dense layers for making the predictions. The architecture works upon is called  \\nDarknet , a neural network framework created by the first author of the YOLO paper.  \\nCore Concept: - \\nThe al gorithm works off by dividing the  image  into the grid of the cells,  for each cell bounding boxes \\nand their scores are predicted, alongside class probab ilities. The c onfidence is given in terms of  IOU \\n(intersection over union ), metric, which is measuring how much the  detected object overlaps with the \\nground truth as a fraction of the total area spanned by the two together (the union).  \\nYOLO v2 - \\nThis impr oves on some of the shortcomings  of the first version, namely the fact that it is not very good \\nat detecting objects that are very near and tends to make  some of the mistakes o n locali zation . \\nIt introduces a few new er things: Which are  anchor boxes  (pre-determined sets of boxes such that the \\nnetwork moves from predicting the bounding boxes to predicting the offsets from these) and the use of \\nfeatures that are more fine -grained so smaller objects can be predicted better.  \\n \\nYOLO v3 - \\nYOLOv3 came a bout April 2018 , and it adds  small improvements, includ ing the fact that bounding boxes \\nget predicted at the different scales. The underlying meaty part of the  YOLO  network, Darknet, is \\nexpanded in this version to have 53 convolutional layers  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7b36945c-696d-4d25-8022-8a394d8cc674', embedding=None, metadata={'page_label': '91', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nData Science Interview Questions  Page 12 \\n \\n \\n \\n----------------------------------------------------------------------------------------------------------------------------  \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1a4b3b15-df15-4f6a-8ecc-9807b93b8194', embedding=None, metadata={'page_label': '92', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW \\nPREPARATION  \\n(30 Days of Interview  \\nPreparation)  \\n \\n# DAY 06  \\n \\n \\n \\n \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5ca3963f-8890-49ca-bb62-77b934f7ba75', embedding=None, metadata={'page_label': '93', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nQ1. What is NLP?  \\nNatural language processing (NLP) : It is the  branch of  artificial intelligence  that helps computers \\nunderstand, interpret and manipulate human language. NLP draws from many disciplines, including \\ncomputer science an d computational linguistics, in its pursuit to fill the gap between human \\ncommunication and computer understanding.  \\n \\n \\nQ2. What are the Libraries we  used for NLP?  \\n   We usually use these libraries in NLP , which are:  \\n   NLTK (Natural language Tool kit), TextBlob, CoreNLP,  Polyglot,  \\n  Gensim, SpaCy, Scikit -learn  \\n  And the new one is Megatron library launched recently.  \\nQ3. What do you understand by tokeni sation?  \\nTokeni sation is the act of breaking a sequence of strings into pieces such as words, keywords, phrases, \\nsymbols and other elements called tokens. Tokens can be individual words, phrases or even whole \\nsentences. In the process of tokeni sation, some characters like punctuation marks are discarded.  \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ee3499c9-6910-46a8-9683-7c3a574e855a', embedding=None, metadata={'page_label': '94', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nQ4. What do you understand by stemming?  \\nStemming : It is the process of reducing infle xions in words to their root forms such as mapping a \\ngroup of wo rds to the same stem even if  stem itself is not a valid word in the Language.  \\n \\n \\n \\nQ5. What is lemmati sation?  \\nLemmati sation : It is the process of the group  together the different inflected forms of the word so \\nthat they can be an alysed as a single item. It  is quite similar to stemming , but it brings context to the \\nwords. So it link s words with similar  kind meaning to one word.  \\n \\n \\n \\nQ6. What is Bag-of-words  model?  \\nWe need the  way to represent text data for the machine learning algorithm s, and the bag -of-words \\nmodel helps us to achieve the  task. T his model is very understand able and to implement. It is the  way \\nof extracting features from the text for  the use in machine learning algorithms.  \\nIn this approach, we use the tokeni sed words for each of observation and find out  the frequency of \\neach token.  \\nLet’s do an  example to understand this concept in depth.  \\n“It is going to rain today .” \\n“Today , I am not going outside .” \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='806ceb1d-84a0-43aa-a0b9-203c432c339d', embedding=None, metadata={'page_label': '95', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n“I am going to watch the season premiere .” \\nWe treat each sentence as the  separate document  and we make the  list of all words from all the three \\ndocuments excluding the punctuation. We get,  \\n‘It’, ’is’, ’going’, ‘to’, ‘rain’, ‘today’ ‘I’, ‘am’, ‘not’, ‘outside’, ‘watch’, ‘the’, ‘season’, ‘premiere .’ \\nThe next step is the create vectors. Vectors convert text that can be used by the machine learning \\nalgorithm.  \\nWe take the first document — “It is going to rain today ”, and we check the frequency of words from \\nthe ten unique words.  \\n“It” = 1  \\n“is” = 1  \\n“going” = 1  \\n“to” = 1  \\n“rain” = 1  \\n“today” = 1  \\n“I” = 0  \\n“am” = 0  \\n“not” = 0  \\n“outside” = 0  \\nRest of the documents will be:  \\n“It is going to rain today ” = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]  \\n“Today I am not going outside ” = [0, 0, 1, 0, 0, 1, 1, 1, 1, 1]  \\n“I am going to watch the season premiere ” = [0, 0,  1, 1, 0 , 0, 1, 1, 0, 0]  \\nIn this approach, each word (a token) is called a “gram”.  Creating the  vocabulary of two -word pairs \\nis called a bigram model.  \\nThe process of converting  the NLP text into numbers is called  vectori sation  in ML. There are d ifferent \\nways to convert text into the vectors  : \\n• Counting the number of times that each word appears in the document.  \\n• I am c alculating the frequency that each word appears in a document out of all the words in \\nthe document.  \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='364186f5-8d0f-4471-9795-8f26f41d843c', embedding=None, metadata={'page_label': '96', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nQ7.What do you understand by TF -IDF? \\nTF-IDF:  It stands for the term of frequency -inverse document frequency.  \\nTF-IDF weight :  It is a statistical measure used to evaluate how important a word is to a document in \\na collection or corpus. The importance increas es proportionally to the number of times a word appears \\nin the document but is offset by the frequency of the word in the corpus.  \\n• Term Frequency (TF) : is a scoring of the frequency of the word in the current document. \\nSince every document is different in length, it is possible that a term would appear much \\nmore times in long documents than shorter ones. The term frequency is often divided by the \\ndocument length to normali se. \\n \\n \\n• Inverse Document Frequency (IDF) : It is a scoring of how rare the word is across the \\ndocuments. It  is a measure of how rare a term is , Rarer the term,  and more is the IDF score.  \\n \\n \\nThus,  \\n \\n \\n \\n \\nQ8. What is Word2vec?  \\n \\nWord2Vec is a shallow, two -layer neural network which is trained to reconstruct linguistic \\ncontexts of words.  It takes as its input a large corpus of words and produces a vector space, \\ntypically of several  of hundred dimensions, with each of unique word in the corpus being \\nassigned to the  corresponding vector in space.  \\nWord vectors are positioned in a  vector space such that words which  share common \\ncontexts in the corpus are located close  to one another in the space.  \\nWord2Vec is a particularly computationally -efficient predictive model for learning word \\nembeddings from raw text. \\nWord2Vec is a group of models which helps derive relations between a word and its \\ncontextual words. Let’s look at two important models inside Word2Vec: Skip -grams and \\nCBOW.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='66ff41d8-57ac-4848-87ad-1fa4049bf093', embedding=None, metadata={'page_label': '97', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nSkip -grams  \\n \\n \\nIn Skip -gram model, we take a  centre  word and a window of context (neighbour)  words , and \\nwe try to predict the context of words out to some window size for each centre word. So, our \\nmodel is going to define a probability distribution , i.e. probability of a word appearing in the \\ncontext given a centre word and we are going to choose our vector representations to maximi se \\nthe probability.  \\n \\n Continuous Bag -of-Words (CBOW)  \\n CBOW predicts target words (e.g. ‘mat’) from the surrounding context words (‘the cat sits \\non the’).  \\nStatistically, it affects  that CBOW smoothes over a lot of  distributional information (by \\ntreating an entire context as one observation). For the most part, this turns out to be a useful \\nthing for smaller datasets.  \\n            \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8fa22f94-74a8-41cf-be58-eb528c61f7ee', embedding=None, metadata={'page_label': '98', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nThis was about converting words into vectors. But where does the “learning” happen? \\nEssentially, we begin with small random initiali sation of word vectors. Our predictive model \\nlearns the vectors by minimi sing the loss function. In Word2vec, this happens with feed -\\nforward neural network s and optimi sation techni ques such as Stochastic gradient descent. \\nThere are also count -based models which make the co-occurrence count matrix of  the words \\nin our corpus; we have a very large matrix with each row for the “words” and columns for the \\n“context”. The number of “contexts” is , of course  very large, since it is very essentially \\ncombinatoria l in size. To overcome this  issue, we apply  SVD  to a matrix. Th is reduces the \\ndimensions of  the matrix to retain  maximum pieces of information . \\n \\n \\nQ9. What is Doc2vec ? \\n \\nParagraph Vector (more popularly known as  Doc2Vec ) — Distributed Memory ( PV-DM) \\n \\nParagraph Vector (Doc2Vec) is supposed to be an extension to Word2Vec such \\nthat Word2Vec learns to project words into a latent d -dimensional space  whereas  Doc2Vec \\naims  at learning  how to project a document into a latent d -dimensional space . \\n \\nThe basic idea behind PV -DM is inspired by Word2Vec. In  CBOW model of Word2Vec, the \\nmodel learns to pr edict a cent re word based on the context s. For example - given a sentence \\n“The cat sat on the table ”, CBOW model would learn to predict the word s “sat” given the \\ncontext words — the cat, on and table . Similarly,in PV -DM the main idea is: randomly sample \\nconsecutive words from the paragraph and  predict a cent re word  from the randomly sampled \\nset of words by taking as  the input — the context words and the paragraph id . \\n \\nLet’s have a look at the model diagr am for some more clarity. In this given model, we see \\nParagraph m atrix, (Average/Concatenate ) and c lassifier sections.  \\nParagraph matrix : It is the matrix where each column represen ts the vector of a paragraph.  \\nAverage/Concatenate: It  means that whether the word vectors and paragraph vector are \\naveraged or concatenated.  \\nClassifier: In this, it  takes the hidden layer vector (the one that was concatenated/avera ged) as \\ninput and predicts the C entre word.  \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b5fcc850-e43b-4c47-bc9f-967312598c26', embedding=None, metadata={'page_label': '99', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n \\nIn the matrix D, It  has the embeddings for “seen” paragraphs (i.e. arbitrary length \\ndocuments), the same way Word2Vec models learns embeddings for words. For unseen \\nparagraphs, the model is again r un through gradient descent (5 or so iterations) to infer a \\ndocument vector.  \\n \\nQ9. What is Time -Series forecasting?  \\n \\nTime series forecasting is a technique for the prediction of events through a    sequence of \\ntime. The technique is used across many fields of study, from the geology to behaviour to \\neconomics. The techniques predict future events by  analysing  the trends of the past, on the \\nassumption that future trends will hold similar to historical trends.  \\n \\nQ10. What is the difference between in Time series and  \\n regression?  \\n  \\nTime -series:  \\n1. Whenever data is recorded at regular intervals of time.  \\n2. Time -series forecast is Extrapolation.  \\n3. Time -series refers to an ordered series of data.  \\n            Regression:  \\n1. Whereas in regression , whether data is recorded at regular or irregular intervals of time , \\nwe can apply.  \\n2. Regression is Interpolation.  \\n3. Regression refer both ordered and unordered series of data.  \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fa767967-b78b-42b6-8333-6cfc586969be', embedding=None, metadata={'page_label': '100', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nQ11. What is the difference between station ery and non - \\n stationary data?  \\nStationary: A series is said to be \"STRICTLY STATIONARY” if the Mean, Variance &  \\nCovariance is constant over some time or time -invariant.  \\n \\n  \\n \\n \\nNon-Stationary:  \\n \\nA series is said to be \"STRICTLY STATIONARY” if the Mean, Variance & Covariance is \\nnot constant over some  time or time -invariant.  \\n \\n \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='71729ae5-06fe-42f6-8b96-86a48f40e666', embedding=None, metadata={'page_label': '101', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nQ12. Why you cannot take non -stationary data to solve time series  \\n Problem?  \\n \\no  Most models assume stationary of data. In other words, standard techniques are \\ninvalid if data is \"NON -STATIONARY\".  \\no Autocorrelation may result due to \"NON -STATIONARY\".  \\no Non-stationary processes are a random walk with or without a drift (a slow , steady \\nchange).  \\no Deterministic trends (trends that are constant, positive or negative, independent of \\ntime for the whole life of the series).  \\n \\n-------------------------------------------------------------------------------------------------------------------  \\n \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='80aa160d-dc5e-4a55-aca6-0b04dfb56b2d', embedding=None, metadata={'page_label': '102', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW \\nPREPARATION  \\n(30 Days of Interview  \\nPreparation)  \\n \\n# DAY 07  \\n \\n \\n \\n \\n \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d454abef-bebb-48ec-b310-faf6482f54dc', embedding=None, metadata={'page_label': '103', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nQ1. What is the p rocess  to make data stationery from non - \\n       stationary  in time series?  \\nAns:  \\nThe two most common ways to make a non -stationary time series stationary are:  \\n\\uf0b7 Differencing  \\n\\uf0b7 Transforming  \\n \\nLet us look at some details for each of them:  \\n \\nDifferencing:  \\nTo make your series stationary, you take a difference between the data points. So let us say, your \\noriginal time series was:  \\n \\nX1, X2, X3,...........Xn  \\n \\nYour series with a difference of degree 1 becomes:  \\n \\n(X2 - X1, X3 - X2, X4 - X3,.......Xn - X(n-1) \\n \\nOnce, you make the difference, plot the series and see if there is any improvement in the ACF  curve. \\nIf not, you can try a second or even a third -order differencing. Remember, the more you difference, \\nthe more complicated your analysis is becoming.  \\n \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='080f23b9-5120-437a-b6df-91742a314376', embedding=None, metadata={'page_label': '104', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nTransforming:  \\nIf we  cannot make a time series stationary, you can try out transforming the variables. Log transform \\nis probably the most com monly used tran sformation if we  see the diverging time series.  \\nHowever, it is  suggested that you use transformation only in case differencing is not working.  \\n \\n \\nQ2. What is the pr ocess to check stationary  data ? \\n \\nAns:  \\n \\nStationary series : It is one in which the properties – mean, variance and covariance, do not vary \\nwith time.  \\n \\n \\n \\nLet us get an idea with these three plots:  \\n \\n\\uf0b7 In the first plot, we can see that the mean varies (increases) with time , which r esults in an \\nupward trend. This is the  non-stationary series.   \\nFor the  series classification as stationary, it should not exhibi t the trend.  \\n\\uf0b7 Moving on to the second plot, we  do not see a trend in the series, but the variance of the series \\nis a function of time. As mentioned previously, a stationary series must have a constan t \\nvariance.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c22244b8-f4f5-4116-9d7d-7de410f2913b', embedding=None, metadata={'page_label': '105', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n\\uf0b7 If we  look at the third plot, the spread becomes closer,  as the time in creases, which implies that \\ncovariance is a function of time.  \\n \\nThese three plots refer to the non-stationary time series. Now give your attention to fourth:  \\n \\n \\n \\nIn this case, M ean, Variance and C ovariance are constant with time. This is  how a stationary time \\nseries looks like.  \\n \\nMost of the statistical models require the series to be stationary to make an effective and precise \\nprediction.  \\n \\nThe v arious process you can use to find out your data is stationary or not by the following terms:  \\n1. Visual Test  \\n2. Statistical Test  \\n3. ADF(Augmented Dickey -Fuller) Test  \\n4. KPSS(Kwiatkowski -Phillips -Schmidt -Shin) Test  \\nQ3. What are ACF and PACF? . \\nAns:  \\nACF  is a (complete) auto -correlation function which gives us the values of the auto-correlation of \\nany series with  lagged values. We plot these values along w ith a confidence band. We have an ACF \\nplot. In simple terms, it describes how well the present value of the series is related to its past \\nvalues. A time series can have components like the trend, seasonality, cyclic and r esidual. ACF \\nconsiders all the  components while finding corr elations ; hence , it’s a ‘complete auto -correlation \\nplot’.  \\nPACF  is a partial autocorrelation function . Instead of finding correlations of present with lags like \\nACF, it finds  the correlation s of the residuals  with the next lag value thus ‘partial’ and not \\n‘complete’ as we remove already fou nd variations before we find  next correlation. So if there are \\nany hidden pieces of information  in the resid ual which can be modelled by  next lag, we might get a \\ngood correlation , and we’ll  keep that next lag as a feature while modelling. Remember , while \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fbc4e65f-7864-4b04-a711-a6dc8f72292a', embedding=None, metadata={'page_label': '106', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\nmodelling we don’t want to keep too many correlated features,  as that it can create multicollinearity \\nissues. Hence we need to retain only  relevant features.  \\n        \\nQ4. What do you understand by the trend of data?  \\nAns:  \\nA general systematic linear or (most often) nonlinear component that changes over time and does not \\nrepeat.  \\nThere are different approaches to understanding trend.  A positive trend means it is likely that \\ngrowth continues.  Let's illustrate this with a simple example:  \\n         \\nHmm, this looks like there is a trend.  To build up confidence, let's add a linear regression for this \\ngraph:  \\n           \\nGreat, now it’s clear theirs a trend in the graph by adding Linear Regression.  \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f1f64de3-9862-43f5-bb03-e4f10b0e4d41', embedding=None, metadata={'page_label': '107', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nQ5. What is the Augmented Dickey -Fuller Test?  \\nAns:  \\nThe Dickey -Fuller test : It is one of the most popul ar statistical tests. It is used to determine the \\npresence of unit root in a  series, and hence help us  to understand if the series is stationary or not. \\nThe n ull and alternate hypothesis for this test is: \\nNull Hypothesis : The series has a unit root (value of a =1)  \\nAlternate Hypothesis : The series has no unit root.  \\nIf we fail to reject the null hypothesis, we can say that the series is non -stationary. This means that \\nthe series can be linear or difference stationary.  \\n \\n \\n \\nQ6. What is AIC and BIC into time series?  \\nAns:  \\nAkaike’s  information criterion  (AIC) compares the quality of a set of statistical models to each \\nother. For example, you might be interested in what variables contribute to low socioeconomic \\nstatus and how the variables contribute to that status. Let’s say you cre ate several  regression  models \\nfor various factors like education, family size, or disability status; The AIC will take each model \\nand rank th em from best to worst. The “best” model will be the one that neither under -fits nor over -\\nfits. \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='955cea34-fd66-41dd-840c-1e73eddf7c67', embedding=None, metadata={'page_label': '108', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nThe Bayesian  Information Criterion (BIC) can be defined as :  \\nk log(n) - 2log(L(θ̂)). \\nHere n is the  sample  size. \\nK is the number of  parameters  which your model estimates.  \\nθ is the set of all parameter . \\nL (θ̂) represents the likelihood of the model tested,  when evaluated at  maximum  likelihood  values \\nof θ. \\nQ7. What are the components of the Time -Series?  \\nAns:  \\nTime series analysis : It provides a body of techniques to understand a dataset better . The most \\nuseful one is the decomposition of the  time series into four constituent parts - \\n1. Level - The baseline value for the series if it were a straight line.  \\n2. Trend - The optional and linear , increasing or decreasing behavio ur of series over time.  \\n3. Seasonality - Optional repeated  pattern s /cycles of  behavio ur over time.  \\n4. Noise  - The optional variability in the observations that cannot be explained by the model.  \\n \\nQ8. What is Time Series Analysis?  \\nAns:  \\nTime series analysis : It involves developing models that best capture or describe an observed time \\nseries to understand the underlying cause . This study seeks the “why” behind the  time-series \\ndataset s. This involves making assumptions abou t the form of  data and decomposing  time-series \\ninto the constitution component . \\n \\nQuality of descriptive model is determined by how well it describes all available data and the \\ninterpretation it provides to inform the problem domain better . \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d64b9371-63e2-4312-bbb1-8d2c6bfa78a3', embedding=None, metadata={'page_label': '109', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nQ9. Give some examples of the Time -Series f orecast ? \\nAns:  \\nThere is almost an endless supply of the time series forecasting problems. B elow are ten examples \\nfrom a range of industries to make the notions of time series analysis and forecasting more \\nconcrete.  \\n1. Forecasting the corn yield in tons by  the state each year.  \\n2. Forecasting whether an EEG trace in seconds  indicates a patient is having a seizure or not.  \\n3. Forecasting the closing price of stock s every day. \\n4. Forecasting the birth rate s at all hospitals in the city every  year.  \\n5. Forecasting product sales in  the units sold each day for the store.  \\n6. Forecasting the number of passengers through  the train station each day.  \\n7. Forecasting unemployment for a state each quarter.  \\n8. Forecasting the utilisation demand on the  server every  hour.  \\n9. Forecasting the size of the rabbit population s in the state each breeding season.  \\n10. Forecasting the average price of gasoline in a city each day.  \\n \\nQ10. What are the techniques of Forecasting?  \\nAns:  \\nThere are  so many statistical techniques available for time series  forecast however  we have found a \\nfew effective  ones which are listed below:  \\n\\uf0b7 Simple Moving Average (SMA)  \\n\\uf0b7 Exponential Smoothing (SES)  \\n\\uf0b7 Autoregressive Integration Moving Average (ARIMA)  \\n \\nQ11. What is the Moving Average?  \\nAns:  \\nThe moving average model is probably the most naive approach to time series modelling. This model \\nstates that the next observation is the mean of all past observations.  \\nAlthough simple, this model might be surprisingly good , and it represents a good starting point.  \\nOtherwise, the moving average can be used to identify interesting trends in the data. We can define \\na window  to apply the moving average model to  smooth  the time series and highlight different trends.  \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3810652e-873f-4ac2-b50e-0399e503e9be', embedding=None, metadata={'page_label': '110', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nExample of a moving average on a 24h window  \\nIn the plot above, we applied the moving average model to a 24h window. The green \\nline smoothed  the time series, and we can see that there are two peaks in the  24h period.  \\nThe longer the window, the  smoother  the trend will be.  \\nBelow is an example of movin g average on a smaller window.  \\n \\n \\nExample of a moving average on a 12h window  \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='561500b3-10ff-4f7d-9c48-5766695cc810', embedding=None, metadata={'page_label': '111', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nQ12. What is  Exponential smoothing ? \\nAns:  \\nExponential smoothing  uses similar logic to moving average, but this time, different  decreasing \\nweight  is assigned to each observation . We can also say , less importance  is given to the \\nobservations as we move further from the present.  \\nMathematically, exponential smoothing is expressed as:  \\n \\n \\nHere,  alpha  is the  smoothing factor which  takes values between 0 to  1. It determines how  fast the \\nweight will decrease  for the previous observations.  \\n \\n \\nFrom the above plot , the dark blue line represents the exponential smoothing of the time series \\nusing a smoothing factor of 0.3 , and the orange line uses a smoothing factor  of 0.05.  As we  can see , \\nthe smaller the smoothing factor, the smoother the time series will be.  Because as  smoothing factor \\napproaches  0, we approach to the moving average model  \\n----------------------------------------------------------------------------- -------------------------------------------  \\n \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7719bd74-9bec-457c-b22b-2071f81ef7ad', embedding=None, metadata={'page_label': '112', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  1 | 7 \\n \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW \\nPREPARATION  \\n(30 Days of Interview  \\nPreparation)  \\n \\n# DAY 08  \\n \\n \\n \\n \\n \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4995249c-ea61-4961-8ccf-804cbce40295', embedding=None, metadata={'page_label': '113', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  2 | 7 \\n \\nQ1. What  is Tensorflow?  \\nAns: \\nTensorFlow : TensorFlow is an open -source software library released in 2015 by Google to make it \\neasier for the developers to design, build, and train deep learning models. TensorFlow  is originated \\nas an internal library that the Google developers used to build the models in house, and we e xpect \\nadditio nal functionality to be added in  the open -source version as they are tested and vetted in internal \\nflavo ur. Although TensorFlow is the only one of several options available to the developers and  we \\nchoo se to use it here because of  thoughtful design and ease of use.  \\nAt a high level, TensorFlow is a Python library that allows users to express arbitrary computation as \\na graph of  data flows . Nodes in this graph represent mathematical operations, whereas edges \\nrepresent data that is com municated from one node to another. Data in TensorFlow are represented \\nas tensors, which are multidimensional arrays. Although this framework for thinking about \\ncomputation is valuable in many different fields, TensorFlow is primarily used for deep learnin g in \\npractice and research.  \\n \\n  \\n \\n \\nQ2. What  are Tensors?  \\nAns:  \\nTensor : In mathematics , it is an algebraic object that describes the linear  mapping  from one set of \\nalgebraic objects to the another. Objects that  the tensors may map between include, but are not limited \\nto the vectors , scalars  and recursively, even other ten sors (for example, a matrix is the  map between \\nvectors and  thus a tensor. Therefore the  linear map between matrices is also the tensor). Tensors are \\ninherently related to  the vector  spaces  and their  dual spaces  and ca n take several different forms.  For \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='92baf254-6468-462b-bab9-b02755b6f001', embedding=None, metadata={'page_label': '114', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  3 | 7 \\n \\nexample , a scalar , a vector , a dual vector  at a point, or a  multi -linear  map between vector \\nspaces.  Euclidean  vectors  and scalars are simple tensors.  While tensors are defined  as independent  of \\nany basis . The literature on physics , often referred  by their components on a basis related to a \\nparticular coordinate system.  \\n \\n \\nQ3. What  is TensorBoard?  \\nAns:  \\nTensorBoard,  a suit of visuali sing tools, is an easy solution to Tensorflow offered by the creators \\nthat lets you visuali se the graphs, plot quantitative metrics about the graph with additional data like \\nimages to pass through it.  \\n \\nThis one is some example  of how the TensorBoard is working.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5b6e2af7-79bc-4eef-9e19-431ced16ae39', embedding=None, metadata={'page_label': '115', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  4 | 7 \\n \\nQ4. What  are the features  of TensorFlow?  \\nAns:  \\n• One of  the main  features of TensorFlow is its ability to build neural network s. \\n• By using th ese neural network s, machines can perform logical thinking and learn similar to  \\nhumans.  \\n• There are  the other tensors for  processing , such as data loading, preprocess ing, calculation, \\nstate and output s. \\n• It considered not only as deep learning  but also as the  library for performing the tensor \\ncalculations , and it is the most excellent library when considered as the deep learning \\nframework that can also describ e basic calculation processing.  \\n• TensorFlow describes all calculation processes by calculation graph, no matter how simple \\nthe calculation is.  \\n \\nQ5. What  are the advantages  of TensorFlow?  \\nAns:  \\n• It allows Deep Learning.  \\n• It is open -source and free.  \\n• It is reliable (and without major bugs)  \\n• It is backed by Google and a good community.  \\n• It is a skill recogni sed by many employers.  \\n• It is easy to implement.  \\n \\nQ6. List a few limitations  of Tensorflow.  \\nAns : \\n• Has the GPU memory conflicts with Theano if imported in the same scope.  \\n• It has dependencies with other libraries.  \\n• Requires prior knowledge of  the advanced calculus and  linear algebra along with the  pretty \\ngood understanding of machine learning.  \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cdbe9bcc-0913-4480-8b87-08f7a03bd5db', embedding=None, metadata={'page_label': '116', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  5 | 7 \\n \\nQ7. What  are the use cases  of Tensor  flow? \\nAns:  \\nTensorflow  is an important  tool of deep  learning,  it has mainly  five use cases,  and they are: \\n• Time  Series  \\n• Image  recognition  \\n• Sound  Recognition  \\n• Video  detection  \\n• Text-based   Appl ications  \\n \\nQ8. What  are the very important  steps  of Tensorflow  architecture?  \\nAns:  \\nThere  are three  main  steps  in the Tensorflow  architecture  are: \\n• Pre-process  the Data  \\n• Build  a Model  \\n• Train  and estimate  the model  \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cf85bd9e-e59b-44d0-8cd5-d9ed0babcf6f', embedding=None, metadata={'page_label': '117', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  6 | 7 \\n \\nQ9. What  is Keras?  \\nAns:  \\nKeras :  It is an Open Source Neural Network library written in Python that runs on the top of Theano \\nor Tensorflow. It is designed to be  the modular, fast and easy to use. It was developed by François \\nChollet, a Google engineer.  \\n \\nQ10. What  is a pooling  layer?  \\nAns:  \\n Pooling layer:  It is generally used in reducing the spatial dimensions and not depth, on a \\nconvolutional neural network model.  \\n \\n \\nQ11. What is the difference betwee n CNN and RNN?  \\nAns:  \\nCNN (Convolutional Neural Network)  \\n• Best suited for spatial data like images  \\n• CNN is powerful compared to RNN  \\n• This network takes a fixed type of inputs and outputs  \\n• These are  the  ideal for video and image processing  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='77415b12-7e65-4864-952f-fa19a248de70', embedding=None, metadata={'page_label': '118', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  7 | 7 \\n \\nRNN  (Recurrent Neural Network)   \\n• Best suited for sequential data  \\n• RNN supports less feature set than CNN.  \\n• This network can manage  the arbitrary input and output lengths.  \\n• It is ideal for  text and speech analysis.  \\nQ12. What are the benefits of Tensorflow over other libraries?  \\nAns:  \\n       The following benefits are:  \\n• Scalability  \\n• Visuali sation of Data \\n• Debugging facility  \\n• Pipelin ing \\n \\n-------------------------------------------------------------------------------------------------------------  \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='481ce4f8-ebf0-4939-bc96-83266ae0060a', embedding=None, metadata={'page_label': '119', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  1 | 9 \\n \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW \\nPREPARATION  \\n(30 Days of Interview  \\nPreparation)  \\n \\n# DAY 09  \\n \\n \\n \\n \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7bf8e4b4-5260-4930-8939-a2ff04f03224', embedding=None, metadata={'page_label': '120', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  2 | 9 \\n \\nQ1: How would you define Machine Learning?  \\nAns:  \\nMachine learning : It is an application of artificial intelligence (AI) that provides systems the ability \\nto learn automatically  and to improve from  experience s without being programmed.  It focuses on the \\ndevelopment of computer applications  that can access the data and use d it to learn for themselves.  \\nThe process of learning starts  with the observations or data, such as examples, direct experience, or \\ninstruction, to look for the patterns in data and  to make bette r decisions in the future based on \\nexamples that we provide.  The primary aim is to allow the computers to learn automatically  without \\nhuman intervention or assistance and adjust actions accordingly.  \\n \\nQ2. What is a labeled training set?  \\nAns:  \\nMachine learning  is derived from  the availability of the labeled  data in the form of a  training \\nset and test set  that is used by the learning  algorithm. The separation of  data into the  training portion \\nand a test portion is t he way the algorithm learns.  We split up the data containing known response \\nvariable values into two pieces. The training set is used to train the algorithm, and then you use the \\ntrained model on the test set to predict the variable respons e values that are already known. The final \\nstep is to compare with the predicted responses against  actual (observed) responses to see how close \\nthey are. The difference is the test error metric. Depending on the test error, you can go back to refine \\nthe model and repeat the process until you’re satisfi ed with the accuracy.  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fb8f0d4a-ac87-4397-a3e2-d7db3b4d501b', embedding=None, metadata={'page_label': '121', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  3 | 9 \\n \\nQ3. What are the  two common supervised tasks?  \\nAns:  \\nThe two common supervised tasks are regression and classification.  \\nRegression - \\nThe r egression problem is when the output variable is the  real or continuous value, such as “salary” \\nor “weight .” Many different models can be used,  and the simplest is linear regression. It tries to fit \\nthe data with the best hyper -plane , which goes through the points.  \\n \\nClassification  \\nIt is the  type of supervised learning. It specifies the class to which the data elements belong to and \\nis best used when the output has finite and discrete values. It predicts a class for an input variable , \\nas well.  \\n \\n \\nQ4. Can you name four common unsupervised tasks?  \\nAns:  \\nThe c ommon unsupervised tasks include clustering, visualization, dimensionality reduction, and \\nassociation rule learning.  \\nClustering  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6bd2a19d-1a8d-4235-b53c-54ea1639fd17', embedding=None, metadata={'page_label': '122', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  4 | 9 \\n \\nIt is a Machine Learning technique that  involves  the grouping of the data poin ts. Given a set of data \\npoints , and we can use a clustering algorithm to  classify each data point into the  specific group. In \\ntheory , data points that lie in the same group should have  similar properties and / or features, and \\ndata points in the different groups should have high  dissimilar properties and/or features. Clustering \\nis the  method of unsupervised learning and is a common technique for statistical data analysis used \\nin many fields.  \\n \\nVisualization  \\nData visualization  is the technique that uses an array of static a nd interactive visuals within the \\nspecific context to help people to understand and make sense of the large amounts of data. The data \\nis often displayed in the story format that visualizes patterns, trends , and correlations that may  go \\notherwise  unnoticed. It  is regularly used as  an avenue to monetize data as the  product. An example \\nof using monetization and data visualization is Uber. The app combines visualization with real -time \\ndata so that  customers can request a ride.   \\n \\nQ5. What type of Machine Learning algorithm we use to  \\n       allow a robot to walk in various unknown terrains?  \\nAns:  \\nReinforcement Learning is likely to perform the best if we want a robot to learn  how to walk in the \\nvarious unknown terrains since this is typically the type of  problem that the reinforcement l earning \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8c99ade5-4e1d-4ce4-ad7a-da8b09875d42', embedding=None, metadata={'page_label': '123', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  5 | 9 \\n \\ntackles. It may be possible to express the problem as a supervised or semisupervised learning \\nproblem, but it would be less natural.  \\nReinforcement Learning - \\nIt’s about to take  suitabl e action s to maximize  reward s in a particular situation. It is employed by the \\nvarious software and machines to find out the best possible behavior/ path it should take in specific \\nsituation s. Reinforcement learning is different  from the supervised learning in a  way that in \\nsupervised learning,  training data has  answer key with it so that the model is trained with the correct \\nanswer itself , but in reinforcement learning, there is no answer , and the reinforcement agent decides \\nwhat to do to perform the given task. In the absence of the training dataset, it is bound to learn from \\nits experience.  \\n \\nQ6. What type of algorithm would we use to segment your  \\n       customers into multiple groups?  \\nAns:  \\nIf we don’t know how to define the groups, then we can use the  clustering algorithm (uns upervised \\nlearning) to  segment our  customers into clusters of similar customers. However, if we know what \\ngroups we would like to have, then we  can feed many examples o f each group to a classification \\nalgorithm (supervised learning), and it will classify all your customers into these groups.  \\nQ7: What is an online machine learning ? \\nAns:  \\nOnline machine learning : It is a method of  machine  learning  in which data becomes available in \\nsequential order and  to update our best predictor for  the future data at each step, as opposed to \\nbatch learning techniqu es that generate the best predictor by learning on entire training data set at \\nonce. Online learning is a common technique and used in  the areas of machine learning where it is \\ncomputational ly infeasible to train over the dataset s, requiring the need for Out- of- \\nCore  algorithms. It is also used in situations where the algorithm must  adapt to new patterns in the \\ndata dynamically  or when t he data itself is generated as the  function of time, for example , stock \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0e872561-1c3a-4829-8ed4-32181858e23c', embedding=None, metadata={'page_label': '124', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  6 | 9 \\n \\nprice prediction.  Online learning algorithms might  be prone to  catastrophic  interference  and \\nproblem that can be addressed by  the incremental  learning  approaches . \\n \\n \\nQ8: What is out -of-core learning?  \\nAns:  \\nOut-of-core: It refers to the processing data that is too large to fit into the computer’s main \\nmemory. Typically, when the dataset fits neatly into the  computer’s main memory, randomly \\naccessing sections of data has a (relatively) small performance penalty.  \\nWhen data must be stored in a medium like a large spinning hard drive or an external computer \\nnetwork, it becomes very expensive to seek an arbitrary section of data randomly  or to process  the \\nsame data multiple times. In such  a case, an out -of-core algorithm will try to access all the relevant \\ndata in a sequence.  \\nHowever, modern computers have deep memory hierarchy, and replacing random access with the \\nsequential access can increase  the performance even on datasets that fit within memory.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='87545c75-7dd3-4821-8bb0-d995f7b7eb13', embedding=None, metadata={'page_label': '125', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  7 | 9 \\n \\n \\nQ9. What is the Model Parameter?  \\nAns:  \\nModel parameter : It is a configuration variable that is internal to a model and whose value can be \\npredicted  from the data.  \\n\\uf0b7 While making predictions, the model parameter is needed.  \\n\\uf0b7 The values define the skill of a model on problem s. \\n\\uf0b7 It is estimated or learned from data.  \\n\\uf0b7 It is often not set manually by the practitioner.  \\n\\uf0b7 It is often saved as part of the learned model.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e9e710d4-b38d-4adc-94c2-59782269c5c3', embedding=None, metadata={'page_label': '126', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  8 | 9 \\n \\nParameters are key to machine learning algorithms. They are part of the model that is learned from \\nhistorical training data.  \\nQ11: What is Model Hyperparameter?  \\nAns: \\nModel hyperparameter : It is a conf iguration that is external to a  model and whose value s cannot \\nbe estimated from the data.  \\n\\uf0b7 It is often used in processes to help estimate model parameters.  \\n\\uf0b7 The practitioner often specifies them . \\n\\uf0b7 It can often be  the set using heuristics.  \\n\\uf0b7 It is tuned for the given predictive modeling problem s. \\nWe cannot know the best value for the model hyperparameter on the given problem. We may use \\nthe rules of thumb, copy values used on other problems, or search for the best value by trial and \\nerror.  \\n \\nQ12. What is cross -validation ? \\nAns:  \\nCross -validation : It is a technique for evaluating M achine Learning  models by training several \\nMachine Learning  models on subsets of available input data and evaluating them on  the \\ncomplementary subset of  data. Use cross -validation to detect overfitting, i .e., failing to generalize a \\npattern.  \\nThere are  three steps involved in cross -validation are as follows :  \\n\\uf0b7 Reserve some portion of the sample data set. \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='66c21e11-cb8f-477a-adac-3444fc4d7f61', embedding=None, metadata={'page_label': '127', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  9 | 9 \\n \\n\\uf0b7 Using the rest data set and train  model s. \\n\\uf0b7 Test the model using  a reserve portion of the data -set. \\n \\n \\n \\n--------------------------------------------------------------------------------------------------------------  \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='35bdc106-fbd4-4d4c-b43d-5d4198327bf2', embedding=None, metadata={'page_label': '128', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  1 | 11 \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW PREPARATION  \\n(30 Days of Interview  \\nPreparation)  \\n \\n# DAY 10  \\n \\n \\n \\n \\n \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='156e3ece-2be9-442a-9991-b3182b7569bf', embedding=None, metadata={'page_label': '129', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  2 | 11 \\n \\n \\nQ1. What is a Recommender System ? \\n \\nAnswer:  \\nA recommender system is today widely deployed in multiple fields like movie recommendations, music \\npreferences, social tags, research articles, search queries and so on. The recommender systems work as \\nper collaborative and content -based filtering or by de ploying a personality -based approach. This type of \\nsystem works based on a person’s past behavior in order to build a model for the future. This will predict \\nthe future product buying, movie viewing or book reading by people. It also creates a filtering ap proach \\nusing the discrete characteristics of items while recommending additional items . \\n \\n \\n \\nQ2. Compare SAS, R and Python programming ? \\n \\nAnswer:  \\nSAS: it is one of the most widely used analytics tools used by some of the biggest companies on earth. \\nIt has some of the best  statistical functions, graphical user interface, but can come with a price tag and \\nhence it cannot be readily adopted by smaller enterprise s \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5c8afd84-63f8-45e4-89d0-154f55febd9a', embedding=None, metadata={'page_label': '130', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  3 | 11 \\n \\nR: The best part about R is that it is an Open Source tool and hence used generously by academia and \\nthe research community. It is a robust tool for statistical computation, graphical representation and \\nreporting. Due to its open source nature it is always being updated with the latest features and then readily \\navailable to everybody . \\nPython : Python is a powerful open  source programming language that is easy to learn, works well with \\nmost other tools and technologies. The best part about Python is that it has innumerable libraries and \\ncommunity created modules making it very robust. It has functions for statistical ope ration, model \\nbuilding and more . \\n \\n \\n \\nQ3. Why is important in data analysis ? \\n \\nAnswer:  \\nWith data coming in from multiple sources it is important to ensure that data is good enough for analysis. \\nThis is where data cleansing becomes extremely vital. Data clea nsing extensively deals with the process \\nof detecting and correcting of data records, ensuring that data is complete and accurate and the \\ncomponents of data that are irrelevant are deleted or modified as per the needs. This process can be \\ndeployed in concu rrence with data wrangling or batch processing . \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='64070d57-f20f-46f5-8f3e-9c2a69824df3', embedding=None, metadata={'page_label': '131', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  4 | 11 \\n \\nOnce the data is cleaned it confirms with the rules of the data sets in the system. Data cleansing is an \\nessential part of the data science because the data can be prone to error due to human negligence, \\ncorruption during transmission or storage among other things. Data cleansing takes a huge chunk of time \\nand effort of a Data Scientist because of the multiple sources from which data emanates and the speed at \\nwhich it comes . \\n \\n \\n \\nQ4. What are the various aspects of  a Machine Learning process ? \\n \\nAnswer:  \\nHere we will discuss the components involved in solving a problem using machine learning . \\nDomain knowledg e \\nThis is the first step wherein we need to understand how to extract the various features from the data and \\nlearn more about the data that we are dealing with. It has got more to do with the type of domain that we \\nare dealing with and familiarizing the system to learn more about it . \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fb5ba02a-09fd-4f50-a843-e890fce26f11', embedding=None, metadata={'page_label': '132', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  5 | 11 \\n \\n \\nFeature Selectio n \\nThis step has got more to do with the feature that we are selecting from the set of features that we have. \\nSometimes it happens that there are a lot of features and we have to make an intelligent decision regarding \\nthe type of feature that we want to select to go ahead with our machine learning endeavor . \\nAlgorith m \\nThis is a vital step since the algorithms that we choose will have a very major impact on the entire process \\nof machine learning. You can choose between the linear and nonlinear algorithm. Some of the algorithms \\nused are Support Vector Machines, Decision Trees, Na ïve Bayes, K -Means Clustering, etc . \\nTrainin g \\nThis is the most important part of the machine learning technique and this is where it differs from the \\ntraditional programming. The training is done based on the data that we have and providing more real \\nworld experiences. With each consequent training step the machine gets better and smarter and able to \\ntake improved decisions . \\nEvaluatio n \\nIn this step we actually evaluate the decisions taken by the machine in order to decide whether it is up to \\nthe mark or not.  There are various metrics that are involved in this process and we have to closed deploy \\neach of these to decide on the efficacy of the whole machine learning endeavor . \\nOptimizatio n \\nThis process involves improving the performance of the machine learning p rocess using various \\noptimization techniques. Optimization of machine learning is one of the most vital components wherein \\nthe performance of the algorithm is vastly improved. The best part of optimization techniques is that \\nmachine learning is not just a consumer of optimization techniques but it also provides new ideas for \\noptimization too . \\nTestin g \\nHere various tests are carried out and some these are unseen set of test cases. The data is partitioned into \\ntest and training set. There are various testing t echniques like cross -validation in order to deal with \\nmultiple situations . \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8ca7cec4-8785-484e-8eaf-21d608e3f062', embedding=None, metadata={'page_label': '133', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  6 | 11 \\n \\n \\n \\nQ4. What is Interpolation and Extrapolation ? \\nAnswer:  \\nThe terms of interpolation and extrapolation are extremely important in any statistical analysis. \\nExtrapolation is the determination or estimation using a known set of values or facts by extending it and \\ntaking it to an area or region that is unknown. It i s the technique of inferring something using data that \\nis available . \\nInterpolation on the other hand is the method of determining a certain value which falls between a certain \\nset of values or the sequence of values. This is especially useful when you have  data at the two extremities \\nof a certain region but you don’t have enough data points at the specific point. This is when you deploy \\ninterpolation to determine the value that you need . \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bd6a65f1-79e1-4a62-a606-7160331bcfd5', embedding=None, metadata={'page_label': '134', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  7 | 11 \\n \\n \\nQ5. What does P -value signify about the statistical data ? \\nAnswer:  \\nP-value is u sed to determine the significance of results after a hypothesis test in statistics. P -value helps \\nthe readers to draw conclusions and is always between 0 and 1 . \\n• P- Value > 0.05 denotes weak evidence against the null hypothesis which means the null hypoth esis \\ncannot be rejected . \\n• P-value <= 0.05 denotes strong evidence against the null hypothesis which means the null hypothesis \\ncan be rejected . \\n• P-value=0.05is the marginal value indicating it is possible to go either way . \\n \\n \\n \\nQ6. During analysis, how do you treat missing values ? \\nAnswer:  \\nThe extent of the missing values is identified after identifying the variables with missing values. If any \\npatterns are identified the analyst has to concentrate on them as it could lead to interesting and meaningful \\nbusiness insights. If there are no patt erns identified, then the missing values can be substituted with mean \\nor median values (imputation) or they can simply be ignored . \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d59d48c8-5f41-4200-9395-fa0e8606cfec', embedding=None, metadata={'page_label': '135', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  8 | 11 \\n \\nThere are various factors to be considered when answering this question - \\nUnderstand the problem statement, understand the dat a and then give the answer.Assigning a default \\nvalue which can be mean, minimum or maximum value. Getting into the data is important . \\nIf it is a categorical variable, the default value is assigned. The missing value is assigned a default value . \\nIf you have  a distribution of data coming, for normal distribution give the mean value . \\nShould we even treat missing values is another important point to consider? If 80% of the values for a \\nvariable are missing then you can answer that you would be dropping the vari able instead of treating the \\nmissing values . \\n \\n \\nQ7. Explain the difference between a Test Set and a Validation Set ? \\n \\nAnswer:  \\nValidation set can be considered as a part of the training set as it is used for parameter selection and to \\navoid Overfitting of the mo del being built. On the other hand, test set is used for testing or evaluating \\nthe performance of a trained machine leaning model . \\nIn simple terms ,the differences can be summarized as - \\nTraining Set is to fit the parameters i.e. weights . \\nTest Set is to ass ess the performance of the model i.e. evaluating the predictive power and \\ngeneralization . \\nValidation set is to tune the parameters . \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='553ecfe7-8a6d-4755-8cf6-be58f721ddff', embedding=None, metadata={'page_label': '136', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  9 | 11 \\n \\nQ8. What is the curse of dimensionality? Can you list some ways to \\ndeal with it?  \\nAnswer : \\nThe curse of dimensionality is when the training data has a high feature count, but the dataset does not \\nhave  enough samples for a model to learn correctly from so many features. For example, a training dataset \\nof 100 samples with 100 features will be very hard to learn from because the model will find random \\nrelations between the features and the target. However , if we had a dataset of 100k samples with 100 \\nfeatures, the model could probably learn the correct relationships between the features and the target.  \\nThere are different options to fight the curse of dimensionality:  \\n\\uf0b7 Feature selection.  Instead of using all  the features, we can train on a smaller subset of features.  \\n\\uf0b7 Dimensionality reduction.  There are many techniques that allow to reduce the dimensionality \\nof the features. Principal component analysis (PCA) and using autoencoders are examples of \\ndimensionali ty reduction techniques.  \\n\\uf0b7 L1 regularization.  Because it produces sparse parameters, L1 helps to deal with high -\\ndimensionality input.  \\n\\uf0b7 Feature engineering.  It’s possible to create new features that sum up multiple existing \\nfeatures. For example, we can get st atistics such as the mean or median.  \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ec0865a4-7f8c-4ccc-9a9b-41f3ec19b2bb', embedding=None, metadata={'page_label': '137', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  10 | 11 \\n \\nQ9. What is data augmentation? Can you give some examples?  \\nAnswer : \\nData augmentation is a technique for synthesizing new data by modifying existing data in such a way \\nthat the target is not changed, or it is changed in a known way.  \\nComputer vision is one of fields where data augmen tation is very useful. There are many modifications \\nthat we can do to images:  \\n\\uf0b7 Resize  \\n\\uf0b7 Horizontal or vertical flip  \\n\\uf0b7 Rotate  \\n\\uf0b7 Add noise  \\n\\uf0b7 Deform  \\n\\uf0b7 Modify colors  \\nEach problem needs a customized data augmentation pipeline. For example, on OCR, doing flips will \\nchange the text and won’t be beneficial; however, resizes and small rotations may help.  \\n \\n \\nQ10. What  is stratified cross -validation and when should we use it?  \\nAnswer : \\nCross -validation is a technique for dividing data between training and validation sets. On typical cross -\\nvalidation this split is done randomly. But in  stratified  cross -validation, the split preserves the ratio of \\nthe categories on both the training and validation datasets.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='08c933d3-4e1e-449e-aa9a-7fd5c3c7c613', embedding=None, metadata={'page_label': '138', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  11 | 11 \\n \\nFor example, if we have a dataset with 10% of category A and 90% of category B, and we use stratified \\ncross -validation, we will have the same proportions in training and validation. In contrast, if we use \\nsimple cross -validation, in the worst case we may find that there are no samples of category A in the \\nvalidation set.  \\nStratified cross -validation may be applied in the following scenar ios: \\n\\uf0b7 On a dataset with multiple categories.  The smaller the dataset and the more imbalanced the \\ncategories, the more important it will be to use stratified cross -validation.  \\n\\uf0b7 On a dataset with data of different distributions.  For example, in a dataset for a utonomous \\ndriving, we may have images taken during the day and at night. If we do not ensure that both \\ntypes are present in training and validation, we will have generalization problems.  \\n \\n \\n \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b3474b8c-9c51-4df5-b80e-9f8060866015', embedding=None, metadata={'page_label': '139', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  1 | 12 \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW PREPARATION  \\n(30 Days of Interview  \\nPreparation)  \\n \\n# DAY 11  \\n \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='67dc3303-9c13-497c-a98b-c90fe8e3ffbe', embedding=None, metadata={'page_label': '140', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  2 | 12 \\n \\nQ1. What  are tensors?  \\nAnswer : \\nThe tensors are no more than a method of presenting the data in deep learning. If put in the simple term , \\ntensors are just multidimensional arrays that allow developers to represent the data in a layer , which \\nmeans deep learning you are using contains high -level data sets where each dimension represents a \\ndifferent feature . \\n \\nThe foremost benefit of using tensors is it provides the much -needed platform -flexibility and is easy to \\ntrainable on CPU. Apart from this, tensors have the auto differentiation capabili ties, advanced support \\nsystem for queues, threads, and asynchronous computation. All these features also make it customizable . \\n                        \\n \\nQ2. Define  the concept  of RNN ? \\n \\nAnswer:  \\nRNN is the artificial neutral which were created to analyze and recognize the patterns in the sequences \\nof the data. Due to their internal memory, RNN can certainly remember the things about the inputs they \\nreceive.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='56f386d7-5c78-4686-a80f-2251f06cd8fb', embedding=None, metadata={'page_label': '141', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  3 | 12 \\n \\n \\n \\n \\n Most common  issues  faced  with  RNN  \\n \\nAlthough  RNN is around for a while and use s backpropagation, there are some common issues faced \\nby developers who wor k it. Out of all , some of the most common issues are:  \\n\\uf0b7 Exploding gradients  \\n\\uf0b7 Vanishing gradients  \\n \\n \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3b72878f-4c9e-4b5a-b031-95e63f4d56ee', embedding=None, metadata={'page_label': '142', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  4 | 12 \\n \\nQ3. What is a ResNet , and where would you use it? Is it efficient?  \\nAnswer: \\nAmong the various neural networks that are used for computer vision, ResNet (Residual Neural \\nNetworks), is one of the most popular ones. It allows us to train extremely deep neural networks , which \\nis the prime reason for its huge usage and popularity. Before the invention of this network, training \\nextremely deep neural networks was almost impossible . \\nTo understand why  we must look at the vanishing gradient problem which is an issue that arises when \\nthe gradient is back propagated to all the layers. As a large number of multiplications are performed, the \\nsize of  the network keeps decreasing till it becomes extremely small , and thus, the network starts \\nperforming badly. ResNet helps to counter the vanishing gradient problem .  \\nThe efficiency of this network is highly dependent on the concept of skip connections. Skip connections \\nare a method of allowing a shortcut path through which the gradient can flow, which in effect helps \\ncounter the vanish ing gradient problem .  \\nAn example of a skip connection is shown below : \\n \\nIn general, a skip connection allows us to sk ip the training of a few layers. Skip connections are also \\ncalled identity shortcut connections as they allow us to directly compute an identity function by just \\nrelying on these connections and not having to look at the whole network . \\nThe skipping of thes e layers makes ResNet an extremely efficient network . \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9d4c085c-13a3-49ab-b95a-5a2638cc29dc', embedding=None, metadata={'page_label': '143', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  5 | 12 \\n \\nQ4. Transfer learning is one of the most useful concepts today. Where \\ncan it be used?  \\n \\nAnswer:  \\nPre-trained models are probably one of the most common use cases for transfer learning.  \\nFor anyone who does not have ac cess to huge computational power, training complex models is always \\na challenge. Transfer learning aims to help by both improving the performance and speeding up your \\nnetwork.  \\nIn layman terms, transfer learning is a technique in which a model that has alre ady been trained to do \\none task is used for another without much change. This type of learning is also called multi -task learning.  \\nMany models that are pre -trained are available online. Any of these models can be used as a starting \\npoint in the creation of  the new model required. After just using the weights, the model must be refined \\nand adapted on the required data by tuning the parameters of the model.  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a580f19a-1906-423f-8a44-97cabef5560d', embedding=None, metadata={'page_label': '144', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  6 | 12 \\n \\nThe general idea behind transfer learning is to transfer knowledge not data. For humans, this task is e asy \\n– we can generali ze models that we have mentally created a long time ago for a different purpose. One \\nor two samples is almost always enough. However, in the case of neural networks, a huge amount of data  \\nand computational power are required.  \\nTransfer l earning should generally be used when we don’t have a lot of labe led training data , or if there \\nalready exists a network for the task you are trying to achieve, probably trained on a much more massive \\ndataset. Note, however, that the input of the model mus t have the same size during training. Also, this \\nworks only if the tasks are fairly similar to each other , and the features learned can be generali zed. For \\nexample, something like learning how to recognize  vehicles can probably be extended to learn how to \\nrecogni ze airplanes and helicopters . \\n \\nQ5. What does tuning of hyperparameters signify? Explain with \\nexamples.  \\n \\nAnswer: \\nA hyperparameter is just a variable that defines the structure of the network. Let’s go through some \\nhyperparameters and see the effect of tuning them.  \\n1. A number of hidden layers – Most times, the presence or absence of a large number of hidden \\nlayers may de termine the output, accuracy and training time of the neural network. Having a \\nlarge number of these layers may sometimes cause an increase in accuracy.  \\n2. Learning rate – This is simply a measure of how fast the neural network will change its \\nparameters. A l arge learning rate may lead to the network not being able to converge, but might \\nalso speed up learning. On the other hand, a smaller value for the learning rate will probably slow \\ndown the network but might lead to the network being able to converge.  \\n3. Number o f epochs – This is the number of times the entire training data is run through the \\nnetwork. Increasing the number of epochs leads to better accuracy.  \\n4. Momentum – Momentum is a measure of how and where the network will go while taking into \\naccount all of its  past actions. A proper measure of momentum can lead to a better network.  \\n5. Batch Size – Batch size determines the number of subsamples that are inputs to the network \\nbefore every parameter update.  \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e34490ab-6947-4241-b7fa-1e3448e08ac1', embedding=None, metadata={'page_label': '145', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\nP a g e  7 | 12 \\n \\n \\nQ6. Why are deep learning models  referred  as black boxes?  \\nAnswer : \\nLately, the concept of deep learning being a black box has been floating around. A black box is a system \\nwhose functioning cannot be properly graspe d, but the output produced can be understood and utili zed. \\nNow, since most models are mathematically sound and are created based on legit equations, how is it \\npossible that we do not know how the system works?  \\n \\nFirst, it is almost impossible to visualize the  functions that are generated by a system. Most machine \\nlearning models end up with such complex output that a human can't  make sense of it.  \\nSecond, there are networks with millions of hyperparameters. As a human, we can grasp around 10  to 15 \\nparameters. But analysing a million of them seems out of the question.  \\nThird and most important, it becomes very hard, if not impossible, to trace back why the system made \\nthe decisions it did. This may not sound like a huge problem to worry about b ut consider the case of a \\nself driving car. If the car hits someone on the road, we need to understand why that happened and prevent \\nit. But this isn’t possible if we do not understand how the system works.  \\n \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='676bb7ec-2fc0-4cd7-b554-52668f429ec2', embedding=None, metadata={'page_label': '146', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  8 | 12 \\n \\nTo make a deep learning model not be a black box,  a new field called Explainable Artificial Intelligence \\nor simply, Explainable AI is emerging. This field aims to be able to create intermediate results and trace \\nback the decision -making process of a system.  \\nQ7. Why do we have gates in neural networks?  \\n \\nAnswer : \\nTo understand gates , we must first understand recurrent neural networks.  \\nRecurrent neural networks allow information to be stored as a memory using  loops. Thus, the output of \\na recurrent neural network is not only based on the current input but also the pas t inputs which are stored \\nin the memory of the network. Back propagation is done through time , but in general, the truncated \\nversion of this is used for longer sequences.  \\nGates are generally used in networks that are dependent on time. In effect, any network wh ich would \\nrequire memory, so to speak, would benefit from the use of gates. These gates are generally used to keep \\ntrack of any information that is required by the network without leading to a state of either vanishing or \\nexploding gradients. Such a networ k can also preserve the error through time. Since a sense of constant \\nerror is maintained, the network can learn better.  \\n \\nThese gated units can be considered as units with recurrent connections. They also contain additional \\nneurons , which are gates. If you  relate this process to a signal processing system, the gate is used to \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b8a40300-e42f-47e4-b7af-2af511afed2a', embedding=None, metadata={'page_label': '147', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  9 | 12 \\n \\nregulate which part of the signal passes through. A sigmoid activation function is used which means that \\nthe values taken are from 0 to 1.  \\nAn advantage of using gates is that it enables the network to either forget information that it has already \\nlearn ed or to selectively ignore information either based on the state of the network or the input the gate \\nreceives.  \\nGates are extensively used in r ecurrent neural networks , especially in Long Short -Term Memory (LSTM) \\nnetworks. A general LSTM network will have 3 to 5 gates , typically an input gate, output gate, hidden \\ngate, and activation gate.  \\n \\n \\nQ8. What is a Sobel filter ? \\nAnswer : \\nThe Sobel filter performs a two -dimensional spatial gradient measurement on a given image , which then \\nemphasizes regions that have a high spatial frequency. In effect, this means finding edges . \\nIn most cases, Sobel filters are used to find the approximate ab solute gradient magnitude for every point \\nin a grayscale image. The operator consists of a pair of 3×3 convolution kernels. One of these kernels is \\nrotated by 90 degrees . \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2df8dc9f-34f9-448b-9655-63fc193cd7b3', embedding=None, metadata={'page_label': '148', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  10 | 12 \\n \\n \\nThese kernels respond to edges that run horizontal or vertical with respect to the pi xel grid, one kernel \\nfor each orientation. A point to note is that these kernels can be applied either separately or can be \\ncombined to find the absolute magnitude of the gradient at every point . \\nThe Sobel operator has a large convolution kernel , which ends up smoothing the image to a greater \\nextent , and thus , the operator becomes less sensitive to noise. It also produces higher output values for \\nsimilar edges compared to other methods . \\nTo overcome the problem of output values from the operator overfl owing the maximum allowed pixel \\nvalue per image type, avoid using image types that support pixel values . \\n \\nQ9. What is the purpose of a Boltzmann Machine?  \\n \\nAnswer : \\nBoltzmann machines are algorithms that are based on physics, specifically thermal equilibrium. A special \\nand more well -known case of Boltzmann machines is the Restricted Boltzmann machine , which is a type \\nof Boltzmann machine where there are no connections between hidden layers of the network.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a681cd0c-40a7-48cf-a121-9fdff7a39216', embedding=None, metadata={'page_label': '149', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  11 | 12 \\n \\nThe concept was coined by Geoff Hinton , who most recently wo n the Turing award. In general, the \\nalgorithm uses the laws of thermodynamics and tries to optimi ze a global distribution of energy in the \\nsystem.  \\n \\nIn discrete mathematical terms, a restricted Boltzmann machine can be called a symmetric bipartite \\ngraph , i.e. two symmetric layers. These machines are a form of unsupervised learning , which means that \\nthere are no labels provided with data. It uses stochastic binary units to reach this state.  \\nBoltzmann machines are derived from Markov state machines. A Markov Stat e Machine is a model that \\ncan be used to represent almost any computable function. The restricted Boltzmann machine can be \\nregarded as an undirected graphical model. It is used in dimensionality reduction, collaborative filtering, \\nlearning features as well  as mode ling. It can also be used for classification and regression. In general, \\nrestricted Boltzmann machines are composed of a two -layer network , which can then be extended further.  \\nNote that these models are probabilistic since each of the nod es present in the system learns low -level \\nfeatures from items in the dataset. For example, if we take a grayscale image, each node that is \\nresponsible for the visible layer will take just one -pixel value from the image.  \\nA part of the process of creating such a  machine is a feature hierarchy where sequences of activations \\nare grouped in terms of features. In thermodynamics principles, simulated annealing is a process that the \\nmachine follows to separate signal and noise.  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='73d56738-710d-4e0e-ae00-272b942dd3bf', embedding=None, metadata={'page_label': '150', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  12 | 12 \\n \\nQ10. What are the types of weight initializati on? \\n \\nAnswe r: \\nThere are two major types of weight initiali zation: - zero initiali zation and random initiali zation.   \\nZero initiali zation : In this process, biases and weights are initialised to 0.   If the weights are set to \\n0, all derivatives with respect to the loss functions in the weight matrix become equal. Hence, none of \\nthe weights change during subsequent iterations. Setting the bias to 0 cancels out any effect it may \\nhave.  \\nAll hidden units become symmetric due to zero initiali zation. In general, zero initialization  is not very \\nuseful or accurate for classification and thus must be avoided when any classification task is required.  \\n \\nRandom initiali zation : As compared to 0 initiali zation, this involves setting random values for the \\nweights. The only disadvantage is that set  very high values will increase the learning time as the \\nsigmoid activation function maps close to 1. Likewise, if low values are set, the learning time increases \\nas the activation function is mapped close to 0.  \\nSetting too high or too low valu es thus  generally leads to the exploding or vanishing gradient problem.  \\nNew types of weight initiali zation like “He initiali zation”  and “Xavier initiali zation”  have also \\nemerged. These are based on specific equations and are not mentioned here due to thei r sheer \\ncomplexity.  \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a5ddca6f-dec7-4205-966f-dea3ec285cbc', embedding=None, metadata={'page_label': '151', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  1 | 11 \\n \\n \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW \\nPREPARATION  \\n(30 Days of Interview  \\nPreparation)  \\n \\n# DAY 12  \\n \\n \\n \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='162947a3-1c18-4ecd-8070-b37792af4925', embedding=None, metadata={'page_label': '152', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  2 | 11 \\n \\nQ1. Where is the confusion matrix used? Which module would you \\nuse to show it?  \\nAnswer : \\nIn machine learning, confusion matrix is one of the easiest ways to summarize  the performance of \\nyour algorithm . \\nAt times, it is difficult to judge the accuracy of a model by just looking at the accuracy because of \\nproblems like unequal distribution. So, a  better way to check how good your model is, is to use a \\nconfusion matrix . \\nFirst, let’s look at some key terms . \\nClassification accurac y – This is the ratio of the number of correct predictions to the number of \\npredictions mad e \\nTrue positive s – Correct pred ictions of true event s \\nFalse positive s – Incorrect predictions of true event s \\nTrue negative s – Correct predictions of false event s \\nFalse negative s – Incorrect predictions of false events . \\nThe confusion matrix is now simply a matrix containing true positive s, false positives, true \\nnegatives, false negatives . \\n        \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='969db4a3-d9f5-4265-aca8-b2c1c05c5580', embedding=None, metadata={'page_label': '153', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  3 | 11 \\n \\nQ2: What is Accuracy ? \\nAnswer: \\nIt is the most intuitive performance measure and it simply a ratio of correctly predicted to the total \\nobservations. We can say as, if we have high accuracy , then our model is best. Yes , we could say that \\naccuracy is a great measure but only when you have symmetric datasets where false positives and \\nfalse negative s are almost same.  \\n \\nAccuracy = True Positive + True Negative / (True Positive +False Positive + False                                         \\nNegative + True Negative)  \\n \\nQ3: What is Precision?  \\nAnswer: \\nIt is also called as the positive predictive value. Number  of correct positives in your model that \\npredicts compared to the total number of positives it predicts.  \\n \\nPrecision = True Positives / (True Positives + False Positives)  \\nPrecision = True Positives / Total predicted positive  \\n \\nIt is the number of positive elements predicted properly divided by the total number of positive \\nelements predicted.  \\nWe can say Precision is a measure of exactness, quality, or accuracy. High precision  \\nMeans that more or all of the positive results you pred icted are correct.   \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2f45da4d-8a8e-4a5a-ae8a-42415072612d', embedding=None, metadata={'page_label': '154', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  4 | 11 \\n \\nQ4: What is Recall ? \\nAnswer: \\nRecall we can also called as sensitivity or true positive rate.  \\nIt is several  positives that our model predicts compared to the actual number of positives in our data.  \\nRecall = True Positives / (True Positives + False Positives)  \\nRecall = True Positives / Total Actual Positive  \\n \\nRecall is a measure of completeness.  High recall which means that our model classified most or all \\nof the possible positive elements as positive.  \\n \\nQ5: What is F1 Score ? \\nAnswer: \\nWe use Precision and recall together because they complement each other in how they describe the \\neffectiveness of a model. The F1 score that combines these two as the weighted harmonic mean of \\nprecision and recall.  \\n \\nF1 Score = 2 * (Precision * Recall) / (Prec ision + Recall)  \\n \\n \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='99d93538-582c-42c2-bcb2-3280dfd065fc', embedding=None, metadata={'page_label': '155', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  5 | 11 \\n \\nQ6: What is Bias and Variance trade -off? \\nAnswer: \\nBias \\nBias means it’s how far are the predict values from the actual values. If the average predicted values \\nare far off from the actual values , then we called as this one have high bias.  \\nWhen our model  has a high bias , then it means that our  model is too simple and does not capture the \\ncomplexity of data , thus underfitting the data .   \\n \\nVariance  \\nIt occurs when our model performs good on the trained dataset but does not do well on a dataset that \\nit is not trained on, like a test dataset or validation dataset.  It tells us that actual value is how much \\nscattered from the predicted value.  \\n \\nBecause of High variance it cause overfitting that implies that the algorithm models random noise \\npresent in the training data . \\nWhen model have high variance , then model becomes very flexible and tune itself to the data points \\nof the training set.  \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f11a695a-0423-4289-97c5-e9ad967cdd91', embedding=None, metadata={'page_label': '156', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  6 | 11 \\n \\nBias-variance : It decomposition  essentially decomposes the learning error from any algorithm by \\nadding  bias, the variance and a bit of irreducible error due to noise in the underly ing dataset. \\nEssentially, if we  make the model more com plex and add more variables, We ’ll lose bias but gain \\nsome variance —to get the optimally reduced amount of error, you’ll h ave to  tradeoff  bias and \\nvariance. We  don’t want either high bias or high variance in your model.  \\n \\n                            Bias and variance using bulls -eye diagram  \\n \\n \\nQ7. What is data wrangling? Mention three points to consider in the \\nprocess.  \\nAnswer : \\nData wrangling is a process by which we convert and map data. This changes data from its raw \\nform to a format that is a lot more valuable . \\nData wrangling is the first step for machine learning and deep learning. The end goal is to provide \\ndata that is actionable and to provide it as fast as possible . \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='66d6bad0-224b-4e19-88dd-954d27b92eea', embedding=None, metadata={'page_label': '157', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  7 | 11 \\n \\nThere are three major things to focus on while talking about data wrangling – \\n1. Acquiring da ta \\nThe first and probably the most important step in data science is the acquiring, sorting and cleaning \\nof data.  This is an extremely tedious process and requires the most amount of time . \\nOne needs to : \\n\\uf0b7 Check if the data is valid and up -to-date. \\n\\uf0b7 Check if t he data acquired is relevant for the problem at hand . \\nSources for data collectio n Data is publicly available on various websites like \\nkaggle.com,  data.gov  ,World Bank , Five Thirty Eight Datasets , AWS Datasets, Google \\nDatasets . \\n2. Data cleaning  \\nData cleaning is an essential component of data wrangling and requires a lot of patience. To ma ke \\nthe job easier it is first essential to format the data make the data readable for humans at first . \\nThe essentials involved are : \\n\\uf0b7 Format the data to make it more readabl e \\n\\uf0b7 Find outliers (data points that do not match the rest of the dataset) in dat a \\n\\uf0b7 Find missing values and remove them from the data set (without this, any model being \\ntrained becomes incomplete and useless ) \\n3. Data Computation  \\nAt times, your machine not have enough resources to run your algorithm e.g. you might not have a \\nGPU. In these cases , you can use publicly available APIs to run your algorithm. These are standard \\nend points found on the web which allow you to use computing power over the web and process \\ndata without having to rely on your own system. An example would be the Google Colab  Platform . \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cd11dd2c-9729-48ef-bf50-26af12be0cf5', embedding=None, metadata={'page_label': '158', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  8 | 11 \\n \\nQ8. Why is normalization  required before applying any machine \\nlearning model? What module can you use to perform normalization ? \\nAnswer : \\nNormalization  is a process that is required when an algorithm uses something like distance \\nmeasures.  Examples would be clustering data, finding cosine similarities, creating recommender \\nsystems.  \\nNormalization  is not always required and is done to prevent variables that are on higher scale from \\naffecting outcomes that are on lower levels. For example, cons ider a dataset of employees’ income. \\nThis data won’t be on the same scale if you try to cluster it. Hence, we would have to normalize  the \\ndata to prevent incorrect clustering.  \\nA key point to note is that normalization  does not distort the differences in th e range of values.  \\nA problem we might face if we don’t normalize  data is that gradients would take a very long time \\nto descend and reach the global maxima/ minima.  \\nFor numerical data, normalization  is generally done between the range of 0 to 1.  \\nThe general  formula is:  \\nXnew = (x -xmin)/(xmax -xmin)  \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ce35ebe2-6425-45e9-9470-364b113c03ba', embedding=None, metadata={'page_label': '159', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  9 | 11 \\n \\nQ9. What is the difference bet ween feature selection and feature \\nextraction?  \\nFeature selection and feature extraction are two major ways of fixing the curse of dimensionalit y \\n1. Feature selection:  \\nFeature selection is used to filter a subset of input variables on which the attention should focus. \\nEvery other variable is ignored. This is something which we, as humans, tend to do subconsciously . \\nMany domains have tens of thousands of variables out of which most are irrelevant and redundant. \\nFeature selection limits the training data and reduces the amount of computational resources used. \\nIt can significantly improve a learning algorithms performance . \\nIn summary, we can say that the goal of feature sele ction is to find out an optimal feature subset. \\nThis might not be entirely accurate, however, methods of understanding the importance of features \\nalso exist. Some modules in python such as Xgboost help achieve the same . \\n2. Feature extraction  \\nFeature extrac tion involves transformation of features so that we can extract features to improve the \\nprocess of feature selection. For example, in an unsupervised learning problem, the extraction of \\nbigrams from a text, or the extraction of contours from an image are e xamples of feature extraction . \\nThe general workflow involves applying feature extraction on given data to extract features and \\nthen apply feature selection with respect to the target variable to select a subset of data. In effect, \\nthis helps improve the ac curacy of a model . \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7faf6423-5197-4dd2-8d25-94ae2afb9de5', embedding=None, metadata={'page_label': '160', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  10 | 11 \\n \\nQ10. Why is polarity and subjectivity an issue?  \\nPolarity and subjectivity are terms which are generally used in sentiment analysis.  \\nPolarity is the variation of emotions in a sentence. Since sentiment analysis is widely dependent on \\nemotions and their intensity, polarity turns out to  be an extremely important factor.  \\nIn most cases, opinions and sentiment analysis are evaluations. They fall under the categories of \\nemotional and rational evaluations.  \\nRational evaluations, as the name suggests, are based on facts and rationality while em otional \\nevaluations are based on non -tangible responses, which are not always easy to detect.  \\nSubjectivity in sentiment analysis, is a matter of personal feelings and beliefs which may or may \\nnot be based on any fact. When there is a lot of subjectivity in  a text, it must be explained and \\nanalysed in context. On the contrary, if there was a lot of polarity in the text, it could be expressed \\nas a positive, negative or neutral emotion.  \\nQ11. When would you use ARIMA?  \\nAnswer: \\nARIMA is a widely used statistical method which stands for Auto Regressive Integrated Moving \\nAvera ge. It is generally used for analyzing  time series data and time series forecasting. Let’s take a \\nquick look at the terms involved.  \\nAuto Regression  is a model that uses the relationship between the observation and some numbers of \\nlagging observations.  \\n                         \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='09a08d7c-cfb5-457a-b48b-5e12afdc82b5', embedding=None, metadata={'page_label': '161', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  11 | 11 \\n \\nIntegrated  means use of differences in raw observations which help make the time series stationary.  \\nMoving Averages  is a model that uses the relationship and dependency between the observation \\nand residual error from the models being applied to the lagging obs ervations.  \\nNote that each of these components are used as parameters. After the construction of the model, a \\nlinear regression model is constructed.  \\nData is prepared by:  \\n\\uf0b7 Finding out the differences  \\n\\uf0b7 Removing trends and structures that will negatively affect  the model  \\n\\uf0b7 Finally, making the model stationary.  \\n \\n \\n \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='37a118ca-06c1-4736-8fcf-f6bb7263c6ae', embedding=None, metadata={'page_label': '162', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  1 | 10 \\n \\n \\n \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW \\nPREPARATION  \\n(30 Days of  Interview  \\nPreparation ) \\n# Day 13  \\n \\n \\n \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='13054d98-710f-4c04-8a2a-9abbf8cda758', embedding=None, metadata={'page_label': '163', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  2 | 10 \\n \\nQ1. What is Autoregression ? \\nAnswer: \\nThe autoregressive (AR) model is commonly used to model time-varying processes and solve \\nproblems in the fields of natural science, eco nomics and finance, and others.  The models have always \\nbeen discussed in the context of random process and are often perceived as statistical tools for time \\nseries data.  \\nA regre ssion model, like linear regression, models an output value which are based on a linear \\ncombination of input values.  \\nExample: y^ = b0 + b1*X1  \\nWhere y^ is the prediction, b0 and b1 are coefficients found by optimi sing the model on training \\ndata, and X is an  input value.  \\nThis model  technique can be used on  the time series where input variables are taken as observations \\nat previous time steps, called lag variables.  \\nFor example, we can predict the value for the next time step (t+1) given the observations at the  last \\ntwo time steps (t -1 and t -2). As a regression model, this would look as follows:  \\nX(t+1) = b0 + b1*X(t -1) + b2*X(t -2)- \\nBecause the regression model uses the data from the same input variable at previous time steps, it is \\nreferred to as an autoregression.  \\nThe notation AR( p) refers to the autoregressive model of order  p. The AR( p) model is written  \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f7dc919d-55e5-45aa-89ee-590e43b5f324', embedding=None, metadata={'page_label': '164', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  3 | 10 \\n \\nQ2. What is Moving Average ? \\nAnswer:  \\nMoving average : From a dataset , we will get an overall idea of trends by this technique; it is \\nan average  of any subset of numbers. For  forecasting long -term trends , the moving average is \\nextremely useful for it . We can calculate it for any period . For example:  if we  have  sales data for \\ntwenty years , we can calculate the five-year moving average, a four -year moving average, a three -\\nyear moving average and so on.  Stock market  analysts will often use a 50 or 200 -day moving average \\nto help them see trends in the stock market and (hopefully) forecast where the stocks are headed.   \\n \\n \\nThe notation MA( q) refers to the moving average model of order  q: \\n \\nQ3. What is Autoregressive Moving Average (ARMA) ? \\nAnswer:  \\nARMA : It is a model of  forecasting  in which the methods of  autoregression  (AR) analysis and \\nmoving average (MA) are both applied to time -series data that is well behaved. In ARMA it is \\nassumed that the time series is stationary and when it fluctuates, it does so uniformly around a \\nparticular time.  \\nAR (Autoregression model) - \\nAutoregression (AR) model is commonly used in current spectrum estimation.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a05e0d3d-46a9-4b4e-a8c5-e0057e639c7e', embedding=None, metadata={'page_label': '165', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  4 | 10 \\n \\nThe following is the procedure for using ARMA . \\n\\uf0b7 Selecting the AR model and t hen equalizing the  output to equal the signal being studied if \\nthe input is an impulse function or  the white noise. It should at least be good approximation \\nof signal.  \\n\\uf0b7 Finding a  model’s parameters number using the known autocorrelation function or the data . \\n\\uf0b7 Using the derived model parameters to estimate the power spectrum of the signal.  \\nMoving Average (MA) model - \\nIt is a commonly used model in  the modern spectrum estimation and is also one of the methods of  \\nthe model parametric spectrum analysis. The procedure for estimating  MA model’s signal spectrum \\nis as follows.  \\n\\uf0b7 Selecting the MA model and then equali sing the output to equal the signal understudy in the \\ncase where the input is an impulse function or white noise. It should be at least a good \\napproximation of the signal.  \\n\\uf0b7 Finding the model’s parameters using the known autocorrelation function.  \\n\\uf0b7 Estimating the signal’s power spectrum using the derived model parameters.  \\nIn the estimation of the ARMA parameter spectrum, the AR parameters are first estimated, and then \\nthe MA parameters are estimated based on these AR parameters. The spectral estimates of the ARMA \\nmodel are then obtained. The parameter estimation of the MA model is , therefore often calculated as \\na process of ARMA parameter spectrum association.  \\nThe notation ARMA( p, q) refers to the model with  p autoregressive terms and  q moving -average \\nterms. This mo del contains the AR( p) and MA( q) models,  \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='46b4bc3e-a3e4-4a38-ae6d-5c6534b6dd9f', embedding=None, metadata={'page_label': '166', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  5 | 10 \\n \\nQ4. What is Autoregressive Integrated Moving Average (ARIMA) ? \\nAnswer:  \\nARIMA : It is a statistical analysis model that uses  time-series  data to either better understand the data \\nset or to predict future trends.   \\nAn ARIMA model can be understood by the outlining each of its components as follows - \\n\\uf0b7 Autoregression  (AR) : It refers to a model that shows a changing variable that regresses on \\nits own lagged, or  prior, values.  \\n\\uf0b7 Integrated (I) : It represents  the differencing of raw observations to allow  for the time series \\nto become stationary, i.e., data values are replaced by the difference between the data values \\nand the previous values.  \\n\\uf0b7 Moving average (MA) : It incorporates the dependency between an observation and the \\nresidual error from the  moving average mod el applied to the lagged observations.  \\nEach component functions as the  parameter with a standa rd notation. For ARIMA models, the  \\nstandard notation would be  the ARIMA  with p, d, and q, where integer values substitute for the \\nparameters to indicate the type of the ARIMA model used. T he parameters can be defined as - \\n\\uf0b7 p: It the number of lag observations in the model; also known as the lag order.  \\n\\uf0b7 d: It the number of times that the raw observations are differenced; also known as the degree \\nof differencing.  \\n\\uf0b7 q: It the size of the moving average window; also known as  the order of the moving average.  \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='69c0576a-c307-4266-bd42-b91c3b5e198b', embedding=None, metadata={'page_label': '167', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  6 | 10 \\n \\nQ5.What is SARIMA (Seasonal Autoregressive Integrated Moving -\\nAverage)?  \\nAnswer:  \\nSeasonal ARIMA: It  is an extension of ARIMA that explicitly supports the univariate time series \\ndata with the  seasonal component.  \\nIt adds three new hyper -parameters to specify the autoregression (AR), differencing (I) and the \\nmoving average (MA) for the seasonal component of the series, as well as an additional parameter \\nfor the period of the seasonality.  \\n \\nConfiguring the  SARIMA requires selecting hyperparameters for both the trend and seasonal \\nelements of the series.  \\n \\nTrend El ements  \\nThree trend elements  require s the configuration.  \\nThey a re same as the ARIMA model,  specifically - \\n \\np: It is Trend autoregression order.  \\nd: It is Trend difference order.  \\nq: It is Trend moving average order.  \\n \\nSeasonal Elements - \\n \\nFour seasonal elements  are not the part of the ARIMA that must be configured,  they are - \\nP: It is Seasonal autoregressive order.  \\nD: It is Seasonal difference order.  \\nQ: It is Seasonal moving average order.  \\nm: It is the number of time steps for the  single seasonal period.  \\n \\nTogether, the notation for the SARIMA model is specified as - \\n \\nSARIMA(p,d,q)(P,D,Q)m - \\n \\nThe elements can be chosen through careful analysis of the ACF and PACF plots looking at the \\ncorrelations of recent time steps.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b47c0605-3035-4381-b0a8-165d5d0c8909', embedding=None, metadata={'page_label': '168', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  7 | 10 \\n \\n \\n \\n \\nQ6. What is Seasonal Autoregressive Integrated Moving -Average  \\n with Exogenous Regressors (SARIMAX) ?  \\nAnswer:  \\nSARIMAX : It is an extension of the SARIMA model that also includes the modelling of the \\nexogenous variables.  \\nExogenous variables are also called  the covariates and can be thought of as parallel input sequences \\nthat have observations at the same time steps as the original series. The p rimary series may be \\nreferred  as endogen ous data to con trast it from  exogenous sequence(s). The observations for \\nexogenous variables are included in the model directly at each time step and are not modeled in the \\nsame way as the primary endogenous sequence (e.g. as an AR, MA, etc. process).  \\n \\nThe SARIMAX method  can also be used to model the subsumed models with exogenous variables, \\nsuch as ARX, MAX, ARMAX, and ARIMAX.  \\nThe method is suitable for univariate time series with trend and/or seasonal components and \\nexogenous variables.  \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='387f3b23-3c3c-497e-a44d-9a29aed9c260', embedding=None, metadata={'page_label': '169', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  8 | 10 \\n \\nQ7. What is Vector autoregression  (VAR )? \\nAnswer:  \\nVAR : It is a stochastic process  model used to capture the linear  interdependencies  among \\nmultiple  time series . VAR models generali se the univariate  autoregressive model  (AR model ) by \\nallowing for more than one evolving variable. All variables in the VAR e nter the model in the same \\nway: each variable has an equation explaining its evolution based on its own  lagged values , the lagged \\nvalues of the other model variables, and an  error term . VAR mode lling does not require s as much \\nknowledg e about the forces influencing the  variable as do  structural models  with simultaneous \\nequations : The only prior knowle dge required is a list of variables which can be hypothesi sed to \\naffect each other intertemporally.  \\nA VAR model des cribes the evolution of the  set of  k variables over the same  sample  period ( t = 1, \\n..., T) as the  linear  function of only their past values. The variables are coll ected in the k-\\nvector  ((k × 1)-matrix ) yt, , which has as the  (i th )element,  yi,t, the observation at time  t of \\nthe (i th )variable.  Example:  if the  (i th )variable is  the GDP , then  yi,t is the value of GDP at time  “t”. \\n- \\nwhere the observation  yt−i is called the (i-th) lag of y, c is the  k-vector of constants ( intercepts ), Ai is \\na time-invariant  (k × k)-matrix , and et is a k-vector of  error  terms satisfying.  \\n \\nQ8. What is Vector Autoregression Moving -Average (VARMA) ? \\nAnswer:  \\nVARMA: It is  method models the next step in each time series using an ARMA model. It is the \\ngenerali sation of ARMA to multiple parallel time series, Example - multivariate time series.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='18b74f67-cb11-4aad-90a2-3c445eb802ff', embedding=None, metadata={'page_label': '170', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  9 | 10 \\n \\nThe notation for a  model involves specifying the order for the AR(p) and the MA(q) m odels as \\nparameters to the VARMA  function, e.g. VARMA (p, q).  The VARMA model can also be used to \\ndevelop VAR or VMA models.  \\nThis method is suitable for multivariate time series without trend and seasonal components.  \\n \\n \\nQ9. What is Vector Autoregression  Moving -Average with Exogenous  \\n Regressors (VARMAX)?  \\nAnswer:  \\nVARMAX: It is an extension of the VARMA model that also includes the mode lling of the \\nexogenous variables. It is the  multivariate version of the ARMAX method.  \\nExogenous variables are also called the covariates and can be thought of as parallel input sequences \\nthat have observations at the same time steps as the original series. The pri mary series(es) are referred \\nas the endogenous data to contrast it from the exogenous sequence(s). The observations for  the \\nexogenous variables are included in the model directly at each time step and are not modeled in the \\nsame way as the primary endogenous sequence ( Example - as an AR, MA, etc.).  \\nThis meth od can also be used to model  subsumed models with e xogenous variables, such as VARX \\nand the  VMAX.  \\nThis method is suitable for multivariate time series without trend and seasonal components and \\nexogenous variables.  \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='51dd6040-51f7-4a27-9081-802f32bb3daa', embedding=None, metadata={'page_label': '171', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  10 | 10 \\n \\nQ10. What is Simple Exponential Smoothing (SES) ? \\nAnswer:  \\nSES: It  method models the next time step as an exponentially weighted linear function of observations \\nat prior time steps.  \\nThis method is suitable for univariate time series without trend and seasonal components.  \\nExponential smoothing  is the rule of thumb  technique for smoothing  time series  data using the \\nexponential  window function . Whereas in the  simple moving average , the past observations are \\nweighted equally, exponential functions are used to assign exponentially decreas ing weights over \\ntime. It is easily learned and easily applied procedure for making some determination based on prior \\nassumptions by the user, such as seasonality. Exponential smoothing is often  used for analysis of \\ntime-series data.  \\nExponential smoothing is one of many  window functions  commonly applied to smooth data in  signal \\nprocessing , acting as  low-pass filters  to remove high -frequency  noise .  \\nThe raw data sequence is often represented by {xt} beginning at time  t = 0, and the output of the \\nexponential smoothing algorithm is commonly written as {st} which may be regarded as a best \\nestimate of what the next value of x  will be. When the sequence of observations begins at time   t= \\n0, the simplest form of exponential smoothing is given by the formulas:  \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b2175a84-d18e-4790-bd55-bf782ff5cef9', embedding=None, metadata={'page_label': '172', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  1 | 11 \\n \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW \\nPREPARATION  \\n(30 Days of Interview  \\nPreparation)  \\n \\n# DAY 1 4 \\n  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='aded9584-5335-40e0-8f78-92d7efbe1128', embedding=None, metadata={'page_label': '173', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  2 | 11 \\n \\nQ1. What is Alexnet ? \\nAnswer:   \\nThe Alex Krizhevsky, Geoffrey Hin ton and Ilya Sutskever created the  neural network architecture \\ncalled ‘AlexNet’ and won Image Classification Challenge (ILSVRC) in 2012. They trained their \\nnetwork on 1.2 million high -resolution images into 1000 different classes with 60 million parameters \\nand 650,000 neurons. The training was done on two GPUs with split layer concept because GPUs \\nwere a little bit slow at that time.  \\nAlexNe t is the name of  convolutional neural network which has had a large impact on the field \\nof machine learning, specifically in the application of  deep learning  to machine vision.  The network \\nhad very similar architecture as  the LeNet  by Yann LeCun et al . but was deeper with more filters per \\nlayer, and with  the stacked co nvolutional layers. It consist  of  ( 11×11, 5×5,3×3, convolutions ), max \\npooling, dropout, data augmentation, ReLU activ ations and SGD with the momentum. It attached  \\nwith ReLU activations afte r every convolutional and fully connected layer. AlexNet was trained for \\nsix days simultaneously on two Nvidia Geforce GTX 580 GPUs , which is the reason for why their \\nnetwork is split into the two pipeli nes. \\nArchitecture  \\n \\n \\nAlexNet con tains eight layers with weights , first five are convolutional , and the remaining three are \\nfully connected. The output of  last fully -connected layer is fed to a 1000 -way softmax which \\nproduces a distribution over  the 1000 class labels. The network maximi ses the multinomial logistic \\nregression objective, which is equivalent to maximi sing the average across training cases of the log -\\nprobability of the correct label under the predictio n distribution. The kernels of  second, fourth, and \\nthe fifth convolutional layers are connected only with those kernel maps in the previous layer which \\nreside o n the same GPU. The kernels of  third convolutional layer are connected to all the kernel maps \\nin second layer. The neurons in fully connected layers are connected to all the neurons in the previous \\nlayer s. \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='54d2f980-8326-4530-868c-ef47513edcd9', embedding=None, metadata={'page_label': '174', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  3 | 11 \\n \\nIn short, AlexNet contains five convolutional layers and three  fully connected layers. Relu is applied \\nafter the very convolutional and  the fully connected layer. Dropout is applied before the  first and \\nsecond fully connected year.  The network has the 62.3 million parameters and needs 1.1 billion \\ncomputation units in a forward pass. We can also see convolution layers, which accounts for 6% of \\nall the parameters, consumes 95% of the computation.  \\n \\nQ2. What is VGGNet?  \\nAnswer:   \\nVGGNet  consists of 16 convolutional layers and is very appealing because of its very uniform \\narchitecture.  Similar to AlexNet, only 3x3 convolutions, but lots of filters. Trained on 4 GPUs for 2 –\\n3 weeks. It is currently the most preferred choice in the community for extracting features from \\nimages. The weight configuration of the VGGNet is publicl y available and has been used in many \\nother applications and challenges as a baseline feature extractor. However, VGGNet consists of 138 \\nmillion parameters, which can be a bit challenging to handle.  \\nThere are multiple variants of  the VGGNet (VGG16, VGG19 etc.)  which differ only in  total number \\nof layers in the network s. The structural details of  the VGG16 network has been shown:  \\n \\n \\n \\nThe idea behind having the fixed size kernels is that all the variable size convolutional kernels used \\nin the Alexnet (11x11, 5x5, 3x3) can be replicated by making use of multiple 3x3 kernels as the \\nbuildi ng blocks. The replication is in term of the receptive field covered by kernels  . \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='90af6861-df7c-4ec5-beb8-f827fb9acc3c', embedding=None, metadata={'page_label': '175', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  4 | 11 \\n \\nLet’s consider the  example. Say we have an input layer of  the size 5x5x1. Implementing the  conv \\nlayer with kernel size of 5x5 and stride one will the result s and output feature map of (1x1). The same \\noutput feature map can  obtained by implementing  the two (3x3) Conv layers with stride of 1 as  \\nbelow : \\n \\n \\nNow , let’s look at the number of the variables needed to be  trained. For a 5x5 Conv layer filter , the \\nnumber of variables is 25. On the other hand, two conv layers of kernel size 3x3 have a total of \\n3x3x2=18 variables (a reduction of 28%).  \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c8b4e802-f164-480a-8afe-ab315ee1262f', embedding=None, metadata={'page_label': '176', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  5 | 11 \\n \\nQ3. What is VGG16?  \\nAnswer:   \\nVGG16 : It is a convolutional neural network model proposed by the K. Simonyan and A. Zisserman \\nfrom the University of Oxford in the paper “Very Deep Convolutional Networks for  the Large -Scale \\nImage Recognition ”. The model achieves 92.7% top 5 test accuracy in ImageNet,  which is the dataset \\nof over 14 million images belongin g to the 1000 classes. It was one of  famous model submitted \\nto ILSVRC -2014 . It improves  AlexNet by replacing the large kernel -sized filters (11 and 5 in the fi rst \\nand second convolutional layer, respectively) with multiple 3×3 kernel -sized filters one after another. \\nVGG16 was trained for weeks and was using NVIDIA Titan Black GPU’s.  \\n \\nThe Architecture  \\nThe architecture depicted below is VGG16.  \\n \\nThe input to  the Cov1 layer is of fixed size  of 224 x 224 RGB imag e. The image is passed through \\nthe stack of convolutional (conv.) layers, where the filters were used with a very small receptive field: \\n3×3 (which is the smallest size to capture the notion of left/right, up/down, c entre). In one of the \\nconfigurations, it also utili ses the 1×1 convolution  filters, which can be seen as the  linear \\ntransformation of the input channels . The convolution stride is fixed to the 1 pixel,  the spatial padding \\nof the Conv. layer i nput is such that , the spatial resolution is preserved after the convolution, i.e. the \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='844569f8-9298-48aa-9da5-09d77916399c', embedding=None, metadata={'page_label': '177', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  6 | 11 \\n \\npadding is 1 -pixel for 3×3 Conv. layers. Spatial pooling is carried out by the five max -pooling layers, \\nwhich follow s some of the Conv. Layers . Max-pooling is performed over the  2×2 pixel window, with \\nstride 2.  \\nThree Full y-Connected (FC) layers follow the  stack of convolutional layers (which has the different \\ndepth in different architectures): the first two have 4096 channels each, the third performs 1000 -way \\nILSVRC classification and thus contains 1000 channels . The final layer is  softmax layer. The \\nconfiguration s of the fully connected layers is same in all  the networks.  \\nAll hidden layers are equipped with  rectification (ReLU) non -linearity. It is also noted that none of \\nthe networks (except for one) contain  the Local Response Normalisation (LRN), such normali sation \\ndoes not improve the performance on the ILSVRC dataset, but leads to increased memory \\nconsump tion and computation time.  \\nQ4. What is ResNet?  \\nAnswer:   \\nAt the ILSVRC 2015,  so-called Residual Neural Network (ResNet) by the Kaiming He et al \\nintroduced the anovel architecture with “skip connections” and features heavy batch normali sation. \\nSuch skip connections are a lso known as  the gated units or gated recurrent units and have the  strong \\nsimilarity to recent successful elements applied in RNNs. Thanks to this technique  as they were able \\nto train the  NN with 152 layers while still having lower complexity than  the VGGNet. It achieves  the \\ntop-5 error rate of 3.57% , which beats human -level performance on this dataset.  \\n \\nQ5. What is HAAR CASCADE ? \\nAnswer:   \\nHaar  Cascade : It is the  machine learning object detection s algorithm used to identify the objects in \\nan image or  the video and based on the concept of   features proposed by Paul Viola and Michael \\nJones in their paper \"Rapid Object Detection using a Boosted Cascade of Simple Features\" in 2001.  \\nIt is a machine learning -based approach where the  cascade function is trained from the  lot of positive \\nand negative i mages.  It is then used to detect the objects in other images.  \\nThe algorithm has four stages:  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4a4f5bbe-31cb-418e-aeec-8bd71f99599b', embedding=None, metadata={'page_label': '178', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  7 | 11 \\n \\n\\uf0b7 Haar  Feature Selection  \\n\\uf0b7 Creating   Integral Images  \\n\\uf0b7 Adaboost  Training  \\n\\uf0b7 Cascading Classifiers  \\nIt is well known for being able to detect faces and body parts in an image  but can be trained to \\nidentify almost any object.  \\n \\n \\nQ6. What is Transfer Learning?  \\nAnswer:   \\nTransfer learning : It is the  machine learning method where the  model developed for a task is \\nreused as the starting point for the model on the  second task  . \\nTransfer Learning differs from  the traditional Machine Learning in that it is the use of pre -trained \\nmodels that have been used for another task to jump -start the development process on a new task \\nor problem.  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='da826894-c166-4b3d-9b63-de769a3d9387', embedding=None, metadata={'page_label': '179', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  8 | 11 \\n \\nThe benefits of the Transfer Learning are that it can speed up the time as it takes to  develop and \\ntrain the  model by reusing these pieces or modules of already developed models. This helps to \\nspeed up the model training process and accelerate results.  \\n \\nQ7. What is Faster , R-CNN?  \\nAnswer:   \\nFaster R -CNN : It has two networks: region proposal network (RPN) for generating region \\nproposals and a network using these proposals to detect objects. The main differen ce here with \\nthe Fast R -CNN is that the later uses selective search to generate the region proposals. The time \\ncost of generating the region proposals is much smaller in  the RPN than selective search, when \\nRPN shar es the most computation with  object detection network.  In brief,  RPN ranks region \\nboxes (called anchors) and proposes the ones most likely containing objects.  \\nAnchors  \\nAnchors play an  very important role in Faster R -CNN. An anchor is the  box. In  default \\nconfiguration of Faster R -CNN, there are nine anchors at the position of an image. The graph s \\nshown  9 anchors at the position (320, 320) of an image with size (600, 800).  \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c7999539-4574-4738-99d6-7d8be19284eb', embedding=None, metadata={'page_label': '180', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  9 | 11 \\n \\nRegion Proposal Network : \\nThe outpu t of the  region proposal network is the  bunch of boxes/proposals that will be examined \\nby a classifier and regressor to check the occurrence of objects eventually . To be more \\nprecise,  RPN predicts the possibility of an anchor being background or foregrou nd, and refine \\nthe anchor.  \\n \\n \\nQ8. What is RCNN?  \\nAnswer:   \\nTo bypass the problem of selecting the huge number of regions,  Ross Girshick et al . proposed a \\nmethod where we use  the selective search to extract just 2000 regions from the image , and he \\ncalled them  as region proposals. Therefore, instead of trying to classify the  huge number of \\nregions, you can work with 2000 regions.   \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8ab66042-f069-426d-ab9f-aab0adafb083', embedding=None, metadata={'page_label': '181', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  10 | 11 \\n \\n \\nProblems with R -CNN : \\n\\uf0b7 It still takes the  huge amount of time to train the network as we  would have to classify \\n2000 region proposals per image.  \\n\\uf0b7 It cannot be implemented real -time as it takes around 47 seconds for each test image.  \\n\\uf0b7 The selective search algorithm is the fixed algorithm. Therefore, no learning is happening \\nat tha t stage. This  leads to the generation of the bad candidate region proposals.  \\n \\nQ9.What is GoogLeNet/Inception ? \\nAnswer:   \\nThe winner of the ILSVRC 2014 competition was GoogLeNet  from Google. It achieved a top -5 error \\nrate of 6.67%! This was very close to human -level performance which the organisers of the challenge \\nwere now forced to evaluate. As it turns out, this was rather hard to do and required some human \\ntraining to beat GoogLeNets accuracy. After the few days of training, the human expert (Andre j \\nKarpathy) was able to achieve the top-5 error rate of 5.1%(single model) and 3. 6%(ensemble). The \\nnetwork used the  CNN inspired by LeNet but implemented a novel element which is dubbed an \\ninception module. It used batch normali sation, image distortions and RM Sprop. This module is based \\non the several very small convolutions to reduce the number of parameters drastically . Their \\narchitecture consisted of the  22 layer deep CNN but reduced the number of parameters from 60 \\nmillion (AlexNet) to 4 million.  \\n It con tains  1×1 Convolution  at the middle of network , and global average pooling  is used at the end \\nof the network instead of using the fully connected layers. These two techniques are from another \\npaper “Network In -Networ k” (NIN) . Another technique, called  inception module, is to have different \\nsizes/types of convolutions for the same input and to stack  all the outputs.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='58f6ad4b-f8f7-4f0a-a369-424c2c00a22b', embedding=None, metadata={'page_label': '182', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  11 | 11 \\n \\n               \\nQ10. What is LeNet -5? \\nAnswer:   \\nLeNet -5, a pioneering 7 -level convolutional network by  the LeCun et al in 1998, that classifies digits, \\nwas applied by sever al banks to recognise hand -written numbers on checks (cheques) digiti sed in \\n32x32 pixel greyscale input  images. The ability to process higher -resolution images requires larger \\nand more convolutional layers, so th e availability of computing resources constrains this technique . \\n \\n LeNet -5 is very simple network. It only has seven  layers, among which there are three  convolutional \\nlayers (C1, C3 and C5), two sub-sampling (pooling) layers (S2 and S4), and one fully connected lay er \\n(F6), that are followed by  output layer s. Convolutional layers use 5 by 5 convolutions with stride 1. \\nSub-sampling layers are 2 by 2 average pooling layers. Tanh sigmoid activations are used to \\nthroughout the network. Several interesting architectural choices  were made in LeN et-5 that are not \\nvery common in the modern era of deep learning.  \\n \\n------------------- -----------------------------------------------------------------------------------------------------  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7506bc30-1095-4d42-93b6-63befa08f4f8', embedding=None, metadata={'page_label': '183', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nPage 1 of 12 \\n \\n \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW \\nPREPARATION  \\n(30 Days of Interview  \\nPreparation)  \\n \\n# DAY 15  \\n \\n \\n \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ead71837-49f6-4f00-a94c-95ece6448840', embedding=None, metadata={'page_label': '184', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nPage 2 of 12 \\n \\nQ1. What is Autoencoder ? \\nAnswer:  \\nAutoencoder  neural network : It is an unsupervised Machine learning algorithm that applies \\nbackpropagation, setting the target values to be equal to the inputs. It is trained to attempt to copy its \\ninput to its output. Internally, it has the  hidden layer that describes a code used to re present the input.  \\n \\n It is trying to learn the  approximation to the identity function, to output x̂ x^ that is similar to the xx. \\nAutoencoders  belong s to the neural network family, but they are also closely related to PCA \\n(principal components analysis).  \\nAuto encoders , although it is quite similar to PCA , but its Autoencoders  are much more flexible than \\nPCA. Autoencoders  can represent both liners and non -linear  transformation in encoding , but PCA \\ncan perform linear transformation. Autoencoders  can be layer ed to form deep learning network due \\nto its Network representation.  \\nTypes of Autoencoders:  \\n1.  Denoising autoencoder  \\nAutoencoders  are Neural Networks which are used for feature selection and extraction. \\nHowever, when there are more nodes in hidden layer than there are inputs, th e Network is \\nrisking to learn  so-called “Identity Function”, also called “Null Function”, meaning that \\noutput equals the input, marking the Autoencoder useless.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e6dbabac-e7fd-4665-92a3-a4445f5b5bcf', embedding=None, metadata={'page_label': '185', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nPage 3 of 12 \\n \\n Denoising Autoencoders solve this problem by corrupting the data on purpose by randomly \\nturning some of the input values to zero. In general, the percentage of input nodes which \\nare being set to zero is about 50%. Other sources suggest a lower count, such as 30%. It \\ndepends on the amount of data and input nodes you have.  \\n                      \\n2.  Sparse autoencoder  \\nAn autoencoder  takes the input image or vector and learns code dictionary that changes the  \\nraw input from one representation to another. Where in sparse autoencoders  with a sparsity \\nenforcer that directs a single -layer network to learn code dictionary which in turn minimizes \\nthe error in reproducing the input while restricting number of code wo rds for reconstruction.  \\nThe sparse autoencoder consists a single hidden layer, which is connected to the input vector \\nby a weight matrix forming the encoding step. The hidden layer then outputs to a \\nreconstruction vector, using a tied weight matrix to form  the decoder.  \\n \\nQ2. What Is Text Similarity ? \\nAnswer:  \\nWhen talking about text similarity, different people have a slightly different notion on what  text \\nsimilarity  means. In essence, the goal is to compute how ‘close’ two pieces of text are in (1) meaning \\nor (2) surface closeness. The first is referred to as  semantic similarity , and the latter is  referred  to \\nas lexical similarity . Although the methods for  lexical similarity  are often used to achieve  semantic \\nsimilarity  (to a certain extent), achieving true semantic similarity  is often much more involved.  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='be5f86f5-d935-4419-b0bc-61c1aad36e94', embedding=None, metadata={'page_label': '186', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nPage 4 of 12 \\n \\nLexical or Word Level Similarity  \\nWhen referring to text similarity, people refer to how similar  the two pieces of text a re at the surface \\nlevel. Example - how similar are the phrases  “the cat ate the mo use” with “the mouse ate the cat \\nfood”  by just looking at the words?  On the surface, if you consider only word -level similarity, these \\ntwo phrases (with determiners disregarded) appear very similar as 3 of the 4 unique words are an \\nexact overlap.  \\n \\n \\n \\nSemantic Similarity : \\nAnother notion of s imilarity mostly explored by  NLP research community is  how similar in meaning \\nare any two phrases? If we look at the phrases,  “ the cat ate the mouse  ” and “ the mouse ate the cat \\nfood”. As we know that while the words significantly overlap s, these two phrases have different \\nmeaning. Meaning out of the phrases is often the  more difficult task as it requires deeper level of  \\nanalysis.E xample, we can actually look at the simple aspects like order of \\nwords:  “cat==>ate= =>mouse”  and “mouse==>ate==>cat food” . Words overlap in this case, the \\norder of the occurrence is different , and we can tell that , these two phrases have different meaning. \\nThis is just the one example. Most people use  the syntactic parsing to help with the semantic \\nsimilarity. Let’s have a look at the parse trees for these two phrases. What can you get from it?  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='761a6dba-ac33-4a2c-b060-300ac56f7a5e', embedding=None, metadata={'page_label': '187', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nPage 5 of 12 \\n \\n \\n \\nQ3. What is dropout in neural networks?  \\nAnswer:  \\nWhen we training our neural network (or model) by updating each of its weights, it might become \\ntoo dependent on the dataset we are using. Therefore, when this model has to make a prediction or \\nclassification, it will not give satisfactory results. This is known as  over-fitting. We might understand \\nthis problem through a real -world example: If  a student of science learns  only one chapter of a book \\nand then takes a test on the  whole  syllabus, he will probably fail.  \\nTo overcome this problem, we use a technique that was introduced by Geoffrey Hinton in 2012. This \\ntechnique is known as  dropout . \\nDropout refers to ignoring units (i.e. , neurons) during the training phase of certain set of neurons , \\nwhich is chosen at random. By “ignoring”, I mean these units are not considered during a particular \\nforward or backward pass.  \\nAt each training stage, individ ual nodes are either dropped out of the net with probability  1-p or kept \\nwith probability  p, so that a reduced network is left; incoming and outgoing edges to a dropped -out \\nnode are also removed.  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='af2ceb97-e9e3-45b9-8e25-ad87f76085a9', embedding=None, metadata={'page_label': '188', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\nPage 6 of 12 \\n \\nQ4. What is Forward Prop agation?  \\nAnswer:  \\nInput X provides the information that  then propagates to hidden units at each layer and then finally \\nproduce the output y . The architecture of  network entails determining its depth, width, and the \\nactivation functions used on each layer.  Depth  is the number of  the hidden layers.  Width  is the \\nnumber of units (nodes) on each hidden layer since we don’t control neither input layer nor output \\nlayer dimensions. There are quite a few set of activation functions such  Rectified Linear Unit, \\nSigmoid, Hyperbolic tangent, et c. Research has proven that deeper networks outperform networks \\nwith more hidden units. Therefore, it’s always better and won’t hurt to train a deeper network.  \\n \\nQ5. What is Text Mining?  \\nAnswer:  \\nText mining : It is also referred  as text data mining , roughly  equivalent to  text analytics , is the process \\nof deriving high -quality  information  from  text. High -quality i nformation is typically derived through \\nthe devising of patterns and trends through means such as  statistical pattern learning . Text mining \\nusually involves the proc ess of structuring the input text (usually parsing, along with the addition of \\nsome derived linguistic features and the removal of others, and subsequent insertion into a  database ), \\nderiv ing patterns within the  structured data , and finally evaluation and interpretation of the output. \\n'High quality' in text mining usually refers to some combination of  relevance , novelty , and interest. \\nTypical text mining tasks  include  text categorization , text clustering , concept/entity extraction, \\nproducti on of granular taxonomies,  sentiment analysis , document summarization , and entity relation \\nmodeling ( i.e., learning relations between  named entities ). \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4d320e55-2fbe-4b44-b2ec-242b41898586', embedding=None, metadata={'page_label': '189', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nPage 7 of 12 \\n \\n \\nQ6. What is Information Extraction?  \\nAnswer:  \\nInformation extraction (IE) : It is the task of automatically extracting structured information from  the \\nunstructured and/or semi -structured machine -readable documents. In most of the cases , this activity \\nconcerns processing human language texts using  natural language processing (NLP ). \\nInformation extraction depends on named entity recognition ( NER ), a sub -tool used to find targeted \\ninformation to extract. NER recognizes entities first as one of several categories , such as location \\n(LOC), persons (PER) , or organizations (ORG). Once the information category is recognized, an \\ninformation extraction utility extracts the named entity’s related information and constructs a \\nmachine -readable document fro m it, which algorithms can further process to extract meaning. IE \\nfinds meaning by way of other subtasks , including co -reference resolution, relationship extraction, \\nlanguage , and vocabulary analysis , and sometimes audio extraction.  \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='262ef01d-a6a4-42cc-8dd3-98e530d02b2b', embedding=None, metadata={'page_label': '190', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nPage 8 of 12 \\n \\nQ7. What is Text Gen eration?  \\nAnswer:  \\nText Generation : It is a type of the Language Modelling problem.  Language Modelling  is the core \\nproblem for several  of natural language processing tasks such as speech to text, conversational \\nsystem, and the text summarization. The  trained language model learns t he likelihood of occurrence \\nof the  word based on the previous sequence of words used in the text.  Language models can be \\noperated at  the character level, n -gram level, sentence level or even paragraph level.  \\nA language model  is at the core of many NLP tasks, and is simply a probability distribution over a \\nsequence of words:  \\n                               \\n \\n \\nIt can also be used to estimate the conditional probability of the next word in a sequence:  \\n                                      \\n \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2288f856-bd0c-419a-94e5-4d8a6703efd7', embedding=None, metadata={'page_label': '191', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nPage 9 of 12 \\n \\nQ8. What is Text Summarization?  \\nAnswer:  \\nWe all interact with the applications which uses the text summarization. Many of the applications are \\nfor the platform which publishes articles on the daily news, entertainment, spor ts. With our busy \\nschedule, we like  to read the summary of those article s before we decide to jump in for reading entire \\narticle. Reading a summary help s us to identify the interest area, gives a brief context of the story.  \\nText summariz ation is a subdomain of  Natural Language Processing  (NLP) that deals with extracting \\nsummaries from huge chunks of texts. There are two main types of techniques used for text \\nsummarization: NLP -based techniques and deep learning -based techniques.  \\nText summarization : It refers to the technique of shortening long pieces of te xt. The intention is to \\ncreate the  coherent and fluent summary having only the main points outlin ed in the document.  \\nHow text summarization works : \\nThe two types of summarization,  abstractive  and the extractive  summarization.  \\n1. Abstractive Summarization:  It select words based on the semantic understanding ; even those \\nwords did not appear in the source documents. It aims at p roducing important material in the  \\nnew way. They interpret s and examine s the text using advanced natural language techniques \\nto generate the  new shorter text that conveys the most critic al information from the original \\ntext. \\nIt can be correlated in  the way human reads the text article or blog post and then summarizes in \\ntheir word.  \\n \\n2.  Extractive Summarization:  It attempt to s ummarize articles by selecting the  subset of words \\nthat retain the most important points.  \\n \\nThis approach weights the  most  important part of sentences and uses the same to form the \\nsummary. Different algorithm and the techniques are used to define  the weights for the \\nsentences and further r ank them based on importance and similarity among each other.  \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='722dbb85-6533-4c28-8c54-8d32a96ec494', embedding=None, metadata={'page_label': '192', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nPage 10 of 12 \\n \\n \\nQ9. What is Topic Modelling?  \\nAnswer:  \\nTopic Modelling is the task of using unsupervised learning to extract the main topics (represented as \\na set of words) that occur in a collection of documents.  \\nTopic modeling, in the context of Natural Language Processing, is described as a method of \\nuncovering hidden structure in a collection of texts.   \\n \\nDimensionality Reduction : \\nTopic modeling is the  form of dimensionality reduc tion. Rather than repr esenting the  text T in its \\nfeature space as  {Word_i: count(Word_i, T) for Word_i in V}, we can represent the text in its topic \\nspace as  ( Topic_i: weight(Topic_i, T) for Topic_i in Topics  ).  \\nUnsupervised learning : \\nTopic modeling can be compared to the clustering. As in the case of clustering, the number of topics, \\nlike the number of clusters, is the  hyperparameter. By doing  the topic modeling , we build clusters of \\nwords rather than clusters of texts. A text is thus a mixture of all the topics, each having a certain \\nweight.  \\n \\nA Form of Tagging  \\nIf document classification is assigning a single category to a text, topic modeling is assigning multipl e \\ntags to a text. A human expert can label the resulting topics with human -readable labels and use \\ndifferent heuristics to convert the weighted topics to a set of tags.  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='db84b2c6-50a3-427c-83f2-217156f53e9a', embedding=None, metadata={'page_label': '193', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nPage 11 of 12 \\n \\nQ10.What is Hidden Markov Models ? \\nAnswer:  \\nHidden Markov Models (HMMs) are the  class of probabilistic graphical model that allow us to \\npredict the  sequence of u nknown (hidden) variables from the  set of observed variables. The  simple \\nexample of an HMM is predicting the weather (hidden variable) based on the type of clothes that \\nsomeone wears (obse rved). An HMM can be viewed as the  Bayes Net unrolled through time with \\nobservations made at the  sequence of time steps being used to  predict the best sequence of the hidden \\nstates.  \\n \\nThe below  diagram from Wikipedia shows that  HMM and it s transitions. The scenario is the  room \\nthat contains urns  X1, X2 , and X3, each of which contains a known mix of balls, each ball labeled  y1, \\ny2, y3 , and y4. The  sequence of four balls is randomly drawn. In this part icular case,  the user observes \\nthe sequence of balls y1,y2,y3 , and y4 and is attempting to discern the hidden state , which is the right \\nsequence of three urns that these four balls were pul led from.  \\n \\n \\n  \\nWhy Hidden, Markov Model?  \\nThe reason it is called the  Hidden Markov Model is because we are constructing an inference model \\nbased on the assumptions of a Markov process. The Markov process assumption is simply that the \\n“future is independent of the past given the present”.  \\n \\nTo make this point clear, let us consider the scenario below where the weather, the hidden variable, \\ncan be hot, mild or cold , and the observed variables are the type of clothing worn. The arrows \\nrepresent transitions from a hidden state to another hidden state or from a hidden state to a n observed \\nvariable.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d4e1083f-dd10-4c1c-8a0a-e3bb4b1ea5be', embedding=None, metadata={'page_label': '194', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nPage 12 of 12 \\n \\n \\n \\n \\n-----------------------------------------------------------------------------------------------------------------  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b67a21f1-3568-43a7-8fca-bd64b2764b3f', embedding=None, metadata={'page_label': '195', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n  \\n \\n \\nDATA SCIENCE  \\nINTERVIEW \\nPREPARATION  \\n(30 Days of  Interview  \\nPreparation ) \\n# Day -16  \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a083c1fd-0a32-4a1f-a788-4465fbc9acd0', embedding=None, metadata={'page_label': '196', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nQ1.What is Statistics Learning?   \\nAnswer:  \\nStatistical learning : It is the  framework for understanding data based on  the statistics, which can be \\nclassified as  the supervised  or unsupervised . Supervised statistical learning  involves building the \\nstatistical model for predicting, or estimating, an output based on one or more inputs, while \\nin unsupervised statistical learning , there are inputs but no supervising output , but we can learn \\nrelationships and structure from such data.  \\nY = f(X) + ɛ ,X = (X1,X2, . . .,Xp) , \\nf : It is an unknown function  & ɛ is random error (reducible &  irreducible) . \\nPrediction & Inference : \\nIn the situations , where the  set of inputs X are readily available, but the output Y is not known, we \\noften treat f as the black box (not concerned with the exact form of  “f”), as long as it yields  the \\naccurate predictions for Y. This is  the prediction . \\nThere are the situations where we are interested in understanding the way that Y is affected as X \\nchange. In this  type of  situation , we wish to estimate f, but our goal is not necessarily to make the \\npredictions for Y. Here we are more interested in understanding the relationship between the X and  \\nY. Now f cannot be treated as the  black box, because we need to know it ’s exact form. This \\nis inference . \\n \\nParametric & Non -parametric methods  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fe01b0fb-836c-4637-a459-6b7db7de8785', embedding=None, metadata={'page_label': '197', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nParametric statistics : This  statistical tests based on underlying the assumptions about data’s \\ndistribution. In other words, It is based on the parameters of the normal curve. Because parametric \\nstatistics are based on the normal curve, data must meet certain assumptions, or parametric statistics \\ncannot be calculated. Before  running any parametric statistics, you should always be sur e to test the \\nassumptions for the tests that you are planning to run.  \\nf(X) = β0 + β1X1 + β2X2 + . . . + βpXp \\nAs by the name, nonparametric  statistics are not based on  parameters of the normal curve. Therefore, \\nif our data violate the assumptions of a usual  parametric and nonparametric statistics might better \\ndefine the data, try running the nonparametric equiva lent of the parametric test. We  should also \\nconsider using nonpara metric equivalent tests when we  have limited sample sizes (e.g., n < 30). \\nThough  the nonparametric statistical tests have more flexibility than do parametric statistical tests, \\nnonparametric tests are not as robust; therefore, most statisticians recommend that when appropriate, \\nparametric statistics are preferred.  \\n. \\nPrediction Accuracy and Model Interpretability : \\nOut o f many methods that we use for  the statistical learning, some are less flexible and more \\nrestrictive  . When inference is the goa l, then there are clear advantages of  using the simple and \\nrelatively inflexible statistical learning methods. When we are only interested in the prediction, we \\nuse flexible models available.  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f7b81b7e-1ebe-4598-9119-ee81d6c969e9', embedding=None, metadata={'page_label': '198', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nQ2. What  is ANOVA ?  \\nAnswer:  \\nANOVA : it stands for “ Analysis of Variance  ” is an extremely important tool for analysis of data \\n(both One Way and Two Way ANOVA is used). It is a statistical method to comp are the population \\nmeans of two or more groups by analyzing variance. The variance would differ only when the means \\nare significantly different.  \\nANOVA test is the  way to find out if survey or experiment results are  significant . In other words, It  \\nhelps us to figure out if we  need to  reject the null hypothesis  or accept the  alternate hypothesis . We \\nare testing groups to see if there’s a difference between  them. Examples of wh en we  might want to \\ntest different groups:  \\n\\uf0b7 The group of psychiatric patients are trying three different therapies: counseling, medication , \\nand biofeedback. We  want to see if one therapy is better than the others.  \\n\\uf0b7 The manufacturer has two different proc esses to make light bulbs if they want to know which \\none is better . \\n\\uf0b7 Students from  the different colleges take the same exam. We  want to see if one college \\noutperforms the other.  \\n \\n \\nTypes of ANOVA : \\n\\uf0b7 One-way ANOVA  \\n\\uf0b7 Two-way ANOVA  \\n \\nOne-way ANOVA is the hypothesis test in which only one categorical variable or the single \\nfactor is taken into consideration. With the help of  F-distribution , it e nables us to compare  \\nmeans of three or more samples. The Null hypothesis (H0) is the equity in all population \\nmeans while an Alternative hypothesis is the difference in at least one mean.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='35b7f732-6f31-4fe5-9d90-ea0b0c903a3d', embedding=None, metadata={'page_label': '199', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n \\nThere are t wo-ways ANOVA examines the effect of two independent factors on a dependent \\nvariable. It also studies the inter -relationship between independent variables influencing the \\nvalues of the dependent variable, if any.  \\n \\n \\n \\nQ3. What is ANCOVA ? \\nAnswer:  \\nAnalysis of Covariance (ANCOVA) : It is the inclusion of the  continuous variable in addition to the \\nvariables of interest (  the dependent and independent variable) as means for the control.  Because the \\nANCOVA is the  extension of the ANO VA, the researcher can still  assess main effects and the \\ninteractions to answer their research hypothes es.  The difference between  ANCOVA and an ANOVA \\nis that an ANCOVA model includes the  “covari ate” that is correlated with  dependent variable and \\nmeans on  dependent variable are adju sted due to ef fects the covariate has on it.  Covariates can also \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='551f2020-3f20-4717-9b4e-e3b01c05efcd', embedding=None, metadata={'page_label': '200', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nbe use d in many ANOVA based designs: such as between -subjects, within -subjects (repeated \\nmeasures), mixed (between – and within – designs) , etc.  Thus, this technique answers the question  \\nIn simple terms, The difference between ANOVA and the ANCOVA is the letter \"C\", which stands \\nfor \\'covariance\\'. Like ANOVA, \"Analysis of Covar iance\" (ANCOVA) has the  single continuous \\nresponse variable.  Unlike ANOVA, ANCOVA compares the  response variable by bo th the  factor and \\na conti nuous independent variable (example  comparing test score by both \\'level of education\\' and the \\n\\'number of hours spent in studying\\'). The term s for the continuous independent variable (IV) used in  \\nthe  ANCOVA is \"covariate\".  \\nExample of ANCOVA  \\n \\n \\n \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e5b38349-a073-44d7-9743-abdbaa2e2520', embedding=None, metadata={'page_label': '201', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nQ4. What is MANOVA?  \\nAnswer:  \\nMANOVA (multivariate analysis of variance) : It is a type of multivariate analysis used to analyze \\ndata that involves more than one dependent variable at a time. MANOVA allows us to test hypotheses \\nregarding the effect of one or more independent variables on two or more dependent variables.  \\nThe obviou s difference between ANOVA and the  \"Multivariate Analysis of Variance\" (MANOVA) \\nis the “M”, which stands for multivariate. In basic terms, MANOVA is an ANOVA with two or more \\ncontinuous response variable s. Like ANOVA, MANOVA has both the  one-way flavor and a two -\\nway flavor. The number of factor variables involved distinguish the  one-way MANOVA from a two -\\nway MANOVA.   \\n \\n \\nWhen comparing the two or more co ntinuous response variables by the  single factor, a one -way \\nMANOVA is appropriate (e.g. comparing ‘test score’ and ‘annual income’ toge ther by ‘level of \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d61d1974-5b9e-4518-98e8-cc1f68adc948', embedding=None, metadata={'page_label': '202', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\neducation’). The  two-way MANOVA also entails two or more continuous response variables, but \\ncompares them by at least two factors (e.g. c omparing ‘test score’ and ‘annual income’ together by \\nboth ‘level of education’ and ‘zodiac sign’).   \\n \\nQ5. What is MANCOVA?  \\nAnswer:  \\nMultivariate analysis of covariance  (MANCOVA) : It is a statistical technique that is the extension of \\nanalys is of covariance (ANCOVA).  It is the  multivariate analysis of variance (MANOVA) with a \\ncovariate(s).).   In MANCOVA, we assess for statistical differences on multiple continuous dependent \\nvariables by an independent grouping variable, while controlling for a third variable called the \\ncovariate; multiple covariates can be used, depending on the sample size.   Covariates are added so \\nthat it can reduce error terms and so that the analysis eliminates the covariates’ effect on the \\nrelationship between the independent grouping var iable and the continuous dependent variables.  \\nANOVA and ANCOVA, the main difference between  the MANOVA and MANCOVA , is the “C,” \\nwhich again stands for  the “covari ance.” Both the  MANOVA and MANCOVA feature two or more \\nresponse variables, but the key differe nce between the two i s the nature of the IVs. While the  \\nMANOVA can include only factors, an analysis evolves from MANOVA to MANCOVA when one \\nor more covariates are added to the mix.   \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='05c503e2-aa74-45f0-a1bf-18373379db22', embedding=None, metadata={'page_label': '203', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nQ6. Explain the differences between KNN classifier  \\n and KNN regression methods.  \\nAnswer:  \\nThey are quite similar. Given a value for  KK and a prediction point  x0x0, KNN regression first \\nidentifies the  KK training observa tions that are closes to  x0x0, represented by  N0. It then \\nestimates  f(x0)  using the average of all the training responses in  N0. In other words,  \\n                                                   \\nSo the main difference is the fact that for the classifier approach, the algorithm assumes the outcome \\nas the class of more presence, and on the regression approach , the response is the average value of \\nthe nearest neighbors.  \\n \\nQ7. What is t -test?  \\nAnswer:  \\nTo understand T -Test Distribution, Consider the situation, you want to compare the performance of \\ntwo workers of your company by checking the average sales done by each of them, or to compare \\nthe performance of a worker by comparing the average sales done by him with the standard value. In \\nsuch situations of daily life, t distribution is applicable.  \\nA t-test is the  type of inferential  statistic  used to determine if there is a significant difference between \\nthe means of two groups, which may be related in certain features. It is mostly used when the data \\nsets, like the dat a set recorded as the outcome from flipping a coin 100 times, would follow a normal \\ndistribution and may have unknown variances. A t -test is used as a hypothesis testing tool, which \\nallows testing of an  assumption  applicable to a population.   \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b06f37a2-c9cd-429e-b748-fbf3a8ba5a20', embedding=None, metadata={'page_label': '204', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nUnderstand t -test with Example:  Let’s say you have a cold , and you try a naturopathic remedy. Your \\ncold lasts a couple of days. The next time when you ha ve a cold, you buy an over -the-counter \\npharmaceutical , and the cold lasts a week. You survey your friends , and they all tell you that their \\ncolds were of a shorter duration (an average of 3 days) when they took the homeopathic remedy. \\nWhat you  want to know  is, are these results repeatable? A t -test can tell you by comparing the means \\nof the two groups and letting you know the probability of those results happening by chance.  \\n \\n \\n \\nQ8. What is Z -test?  \\nAnswer:  \\nz-test: It is a statistical test used to determine whether the two population means are different when \\nthe variances are known , and the sample size is large. The test statistic is assumed to have the normal \\ndistribution , and nuisance parameters such as  standard deviation  should be known for an accurate z -\\ntest to be performed.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='85c8fa07-4e2c-4113-b009-20895c253a3a', embedding=None, metadata={'page_label': '205', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nAnother definition of Z -test: A Z -test is a type of hypothesis te st. Hypothesis testing is just the  way \\nfor you to figure out if results from a te st are valid or repeatable.  Example, if someone said they had \\nfound the  new drug that cures cancer, you would want to  be sure it was probably true. H ypothesis \\ntest will tell you if it’s probably true or probably not true. A Z test is used when your data is \\napproximately  normally distribut ed. \\n Z-Tests Work ing : \\nTests that can b e conducted as  the z-tests include  one-sample location test, a two -sample location \\ntest, a paired difference test, and a maximum likel ihood estimate. Z -tests are  related to  t-tests, but t -\\ntests are best pe rformed when an experiment has the  small sample size. Also,  T-tests assume s the \\nstandard deviation is unknown, while z -tests assume s that it is known. If the standard deviation of \\nthe population is unknown, then the assumption of the sample  variance  equaling the population \\nvariance is made.  \\nWhen we can run the  Z-test : \\nDifferent types of tests are used in  the statistics (i.e. , f test , chi-square test , t-test). You would use a \\nZ test if:  \\n\\uf0b7 Your  sample size  is greater than 30. Otherwise, use a  t-test. \\n\\uf0b7 Data points should be independent  from each other. S ome other words, one data p oint is not  \\nrelated or doesn’t affect another data point.  \\n\\uf0b7 Your data should be normally distributed. However, for large sample sizes (over 30) , this \\ndoesn’t always matter.  \\n\\uf0b7 Your data should be randomly selected from a population, where each item has an equal \\nchance of being selected.  \\n\\uf0b7 Sample sizes  should be equal , if at all possible.  \\n                    \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='077f9661-fad3-4d3a-bc96-bdb227c23d8e', embedding=None, metadata={'page_label': '206', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nQ9. What is Chi -Square test?  \\nAnswer:  \\nChi-square (χ2) statistic : It is a test that measures how expectations compare to actual observed data \\n(or model results). The data used in calculating a chi -square  statistic  must be random, raw,  mutually \\nexclusive , drawn from independent variables, and drawn from a large enough sample. For example, \\nthe results of tossing a coin 100 times meet these criteria.  \\nChi-square tes t is intended  to test how  it is that an observed distribution is du e to chance. It is also \\ncalled the  \"goodness of fit\"  statistic because it measures how well the observed distribution of  the \\ndata fits with the distribution that is expected if the variables are indepen dent.  \\nChi-square test is designed to analyze  the categorical  data. That means that the data has been counted \\nand divided into categories. It will not work with parametric or continuous data (such as height in \\ninches). For example, if you want to test wheth er attending class influences how students perform on \\nan exam, using test scores (from 0 -100) as data would not be appropriate for a Chi -square test. \\nHowever, arranging students into the categories \"Pass\" and \"Fail\" would. Additionally, the data in a \\nChi-square grid should not be in the form of percentages, or anything other than frequency (count) \\ndata.   \\n                   \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c159197b-3fe3-4a3b-830e-529508437b8f', embedding=None, metadata={'page_label': '207', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\nQ10. What is correlation and the covariance in the statistics ? \\nAnswer:  \\nThe Covariance and Correlation are two mathematical concepts ; these two approaches are widely \\nused in the statistics. Both Correlation and  the Covariance establish the relationship and also \\nmeasure s the dependency between the two random variables,  the work is similar between these two , \\nin the mathematical terms, they are different from each other.  \\nCorrelation:  It is the  statistical technique that can show whether and how strongly pairs of variables \\nare related. For example, height and weight are related; taller people tend to be heavier than s horter \\npeople. The relationship isn't perfect. People of the same height vary in weight, and you can easily \\nthink of two people you know where the shorter one is heavier than the taller one. Nonetheless, the \\naverage weight of people 5'5'' is less than the average weight of people 5'6'', and their average weight \\nis less than that of people 5'7'', etc. Correlation can tell you just how much of the variation in peoples' \\nweights is related to their heights.  \\n \\nCovariance:  It measures the directional relationship  between the  returns on two  assets . The  positive \\ncovariance means that asset returns move together  while a negative covariance means they move \\ninversely. Covariance is calculated by analyzing a t-return surprises (standard  deviations  from the \\nexpected return)  or by multiplying the correlation between the two variables by the  standard \\ndeviation  of each variable.  \\n \\n \\n------------------------------------------------------------------------------------------------------------------------  \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6804c661-3ec8-4254-9556-1df20026ed66', embedding=None, metadata={'page_label': '208', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  1 | 11 \\n \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW \\nPREPARATION  \\n(30 Days of Interview  \\nPreparation)  \\n \\n# DAY 1 7 \\n  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a82d5f9d-00b6-4198-81fd-9a08565ba29e', embedding=None, metadata={'page_label': '209', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  2 | 11 \\n \\nQ1. What is ERM  (Empirical Risk Minimization)?  \\nAnswer:  \\nEmpirical risk minimi zation  (ERM): It is a principle in statistical learning theory which defines a \\nfamily of learning algorithms and is used to give theoretical bounds on their performance. The idea \\nis that  we don’t know exactly how well an algorithm will work in practice (the true \"risk\") because \\nwe don\\'t know the true distribution of data that the algorithm will work on, but as an alternative we \\ncan measure its performance on a known set of training data.  \\nWe assume d that our samples come from this distribution and use our dataset as an approximation. \\nIf we compute the loss using the data points in our dataset, it’s called empirical risk. It is “empirical” \\nand not “true” because we are using a dataset that’s a subset of the whole population.  \\nWhen our learning mod el is buil t, we have to pick  a function that m inimizes the empirical risk that \\nis the delta between predicted output and a ctual output for data points in  the dataset. This process of \\nfinding  this function is called empirical risk minimization (ERM) . We w ant to minimize the true risk. \\nWe don’t hav e information that allows us to achieve that, so we hope  that this empirical risk will \\nalmost be the same as the true empirical risk.  \\nLet’s get a better understanding by Example  \\nWe would want to build  a model that can differentiate between a male and a female based on specific  \\nfeatures. If we select 150  random people where women are really short , and men are really tall, then \\nthe model might incorrectly assume that height is  the differentiating feature. For  build ing a truly \\naccurate model, we have  to gather all the women and men in the world to extract  differentiatin g \\nfeatures. Unfor tunately, that i s not possible! So we select a  small number of people and hope that this \\nsample is representative of the whole population.  \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='77347516-8e19-45de-9095-82fdd778ce71', embedding=None, metadata={'page_label': '210', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  3 | 11 \\n \\nQ2. What is PAC  (Probably Approximately Correct)?  \\nAnswer:  \\nPAC: In computational learning theory , probably approximately correct  (PAC ) learning  is a \\nframework for mathematical analysis of  machine learning . \\nThe learner  receives samples and must  have to  pick a generalization function (called the  hypothesis ) \\nfrom a specific  class of possible functions. Our goal is that, with high p robability , the selected \\nfunction will have low  generalization error . The learner must be able to learn the concept given any \\narbitrary approximation ratio, probability of success, or  distribution of the samples . \\nHypothesis class  is PAC (Probably Approximately Correct ) learnable  if there exists a function  m_H  and \\nalgorithm that for any labeling function  f, distribution  D over the domain of inputs  X, \\ndelta  and epsilon  that with  m ≥ m_H  produces a hypothesis  h like that with probability 1 -delta  it \\nreturns  a true error  lower than  epsilon.  Labeling function is nothing other  than saying that we have a \\nspecific  function  f that labels the data in the domain.  \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9453a384-3785-4b94-9112-95fd0913fa51', embedding=None, metadata={'page_label': '211', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  4 | 11 \\n \\nQ3. What is ELMo ? \\nAnswer:  \\nELMo is a novel way to represent words in vectors or embeddings. These word embeddings help \\nachieve  state-of-the-art (SOTA) results in several NLP tasks:  \\n \\nIt is a deep contextualized word repr esentation that mod els both complex charact eristics of word use \\n(e.g.,  syntax and semantics), and  how these uses vary across linguistic contexts. These word ve ctors \\nare learned functions of  internal states of a deep biLM (bidirectional language model ), which is pre -\\ntrained on large text corpus. They could  be easily added to existing models and significantly improve \\nstate of the art across a broad range of challenging NLP problems, including question answering, \\ntextual entailment and sentiment analysis.  \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='098286f3-4a97-49af-99d5-6324a100c8f7', embedding=None, metadata={'page_label': '212', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  5 | 11 \\n \\nQ4. What is Pragmatic Analysis  in NLP ? \\nAnswer:  \\nPragmatic Analysis (PA) : It deals with outside word knowledge, which means understanding  i.e \\nexternal to documents and  queries. P A that focuses on what was described is reinterpreted by what it \\nactually  meant, deriving the various aspects of language that require real -world knowledge.  \\nIt deals with  overall communicative and  social content and its effect on interpretation. It means \\nabstracting the meaningful use of language  in situations. In this analysis, the main focus always on \\nwhat was said in reinterpreted on what is intended . \\nIt helps users to discover this intended effect by applying a set of rules that characterize cooperative \\ndialogues.  \\nE.g., \"close the  window?\" should be interpreted as a request instead of an order.  \\n \\n \\nQ5. What is Syntactic Parsing?  \\nAnswer:  \\nSyntactic Parsing  or Dependency Parsing : It is a task of recogniz ing a sentence and assigning a \\nsyntactic structure to it.  Most Widely we used syntactic structure is the parse tree which can be \\ngenerated using  some parsing algorithms. Th ese parse trees are useful in various applic ations like \\ngrammar checking or more importantly , it plays a critical role in the semantic analysis stage. For \\nexample to answer the question “ Who is the point guard for the LA Laker in the next game ? ” we \\nneed to figure out its subject, objects, attributes to help us figure out that the user wants the point \\nguard of the LA Lakers specifically for the next game.  \\nExample:  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6161d729-f743-4c01-a584-a0bcae643414', embedding=None, metadata={'page_label': '213', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  6 | 11 \\n \\n \\n \\nQ6. What is ULMFit?  \\nAnswer:  \\nTransfer Learning  in NLP(Natural language  Processing)   is an area that had not been explored with \\ngreat success. But, in May 2018 , Jeremy Howard  and Sebastian Ruder  came up with the paper \\n– Universal Language Model Fine -tuning for Text Classification (ULMFit)  which explores  the \\nbenefits of us ing a pre trained model on text classification. It proposes  ULMFiT (Universal Language \\nModel Fine -tuning for Text Classification ), a transfer learning method that could  be applied to any \\ntask in NLP.   In this method outperforms the state -of-the-art on six text classification tasks.  \\nULMFiT  uses a  regular  LSTM which  is the state -of-the-art language model architecture  (AWD -\\nLSTM) . The LSTM network has three  layers.  Single architecture is used throughout – for pre -training \\nas well as for fine -tuning.  \\nULMFiT achieves the state-of-the-art result using novel techniques like:  \\n\\uf0b7 Discriminative fine -tuning  \\n\\uf0b7 Slanted triangular learning rates  \\n\\uf0b7 Gradual unfreezing  \\nDiscri minative Fine -Tuning  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='384cec29-0609-4700-acf1-37969f54d2e9', embedding=None, metadata={'page_label': '214', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  7 | 11 \\n \\nDifferent layers of a neural network capture different types of information so they should be fine -\\ntuned to varying  extents. Instead of using the same learning rates for all layers of the model, \\ndiscriminative fine -tuning allows us  to tune each layer with different learning rates.  \\nSlanted triangular learn ing \\n       \\nThe model should quickly converge to a suitable region of the parameter space in the beginning of \\ntraining and then later refine its parameters. Using a constant learnin g rate throughout training is not \\nthe best way to achieve  this behaviour. Instead Slanted Triangular Learning Rates (STLR) linearly \\nincreases the learning rate at first and then linearly decays it.  \\nGradual Unfreezing  \\nGradual unfreezing is the concept of unfreezing the layers gradually , which avoids the catastrophic \\nloss of knowledge possessed by the model. It first unfreezes the top layer and fine -tunes all the \\nunfrozen layers for 1 epoch. It then unfreezes the next lower frozen layer and repeats until all the  \\nlayers have been fine -tuned until convergence at the last iteration.  \\nQ7. What is BERT?  \\nAnswer:  \\nBERT (Bidirectional Encoder Representations from Transformers)  is an open -sourced NLP pre -\\ntraining model developed by researchers at Google in 2018. A direct descendant  to GPT (Generalized \\nLanguage Models), BERT has outperformed several models in NLP and provided top  results in \\nQuestion Answering , Natural Language Inference (MNLI), and other frameworks.  \\nWhat makes it’s unique from the  rest of the model is that it is the first deeply bidirectional, \\nunsupervised language representation, pre -trained using only a plain text corpus. Since it’s open -\\nsourced, anyone with machine learning knowledge can easily build an NLP model without the need \\nfor sourcing massive datasets for t raining the model , thus saving time, energy, knowledge and \\nresources.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6b4d2eec-dd51-4fd8-8298-76f7329bb91b', embedding=None, metadata={'page_label': '215', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  8 | 11 \\n \\nHow does it work?  \\nTraditional context -free models (like word2vec or GloVe) generate a single word embedding \\nrepresentation for each word in the vocabulary which means the word “right” would have the same \\ncontext -free representation in “ I’m sure I’m right ” and “ Take a right turn. ” However, BERT would \\nrepresent based on both previous and next context , making it bidirectional. While the concept of \\nbidirectional was around for a long time, B ERT was first on its kind to successfully pre -train \\nbidirectional in a deep neural network.  \\n \\nQ8.What is XLNet?  \\nAnswer:  \\nXLNet is a BERT -like model instead of a totally different one. But it is a n auspicious  and potential \\none. In one word, XLNet  is a generalized autoregressive pretraining method.  \\nAutoregressive ( AR) language model : It is a kind of model that using the context word to predict the \\nnext word. But here the context word is constrained to two directions, either forward or backward s. \\n \\n \\nThe advantages  of AR language model are good at  generative N atural language Process(NLP)  tasks. \\nBecause when generating context, usually is the forward direction. AR language model naturally \\nworks well on such NLP tasks.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='409bd7df-7ba8-4ce1-9a6f-c513c461fa74', embedding=None, metadata={'page_label': '216', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\nP a g e  9 | 11 \\n \\nBut Autoregressive  language model has some disadvantages , and it only can use forward context or \\nbackward context, which means it can't use forward and backward context at the same time . \\n \\nQ9. What is the transformer ? \\nAnswer:  \\nTransformer : It is a deep  machine learning  model introduced in 2017, used primarily in the field \\nof natural language processing  (NLP).  Like recurrent neural netw orks(RNN) , It is design ed to handle \\nordered  sequences  of data, such as natural language, for various tasks like machine \\ntrans lation  and text summarization . However , Unlike recurrent neural netw orks(RNN) , Transformers \\ndo not require that the sequence be processed in the order. So, if the data in question is a natural \\nlanguage, the Transformer does not need to process the beginning of a sentence before it processes \\nthe end. Due to this feature, the Transformer allows for much more  parallelization  than RNNs during \\ntraining.  \\nTransformers are  developed to solve the problem of  sequence transduction  current neural networks . It \\nmeans any task that transforms an input  sequence to an output sequence. This includes speech \\nrecognition, text -to-speech transformation, etc.  \\nFor models to perform  a sequence transduction , it is necessary to have some sort of memory.  \\nexample , let us  say that we are translating the following sentence to another language (French):  \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7edfe1f0-1eca-4e21-a150-cc11b2404927', embedding=None, metadata={'page_label': '217', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  10 | 11 \\n \\n“The Transformers” is a Japanese band.  That band was formed in 1968, d uring  the height of the \\nJapanese music history .” \\nIn the a bove  example, the word “the band” in the second sentence refers to the band “The \\nTransformers” intro duced in the first sentence. When you read about the band in the second sentence, \\nyou know that it is referencing to the “The Transformers” band. That may be important for translation.  \\nFor translating other sentences like that, a model needs to figure out these  sort of dependencies and \\nconnections. Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) have \\nbeen used to deal with this problem because of their properties.  \\n \\n \\nQ10. What is Text summarization ? \\nAnswer:  \\nText summarization : It is the process of shortening a text document, to create a  summary  of the \\nsignificant  points of the original document.  \\nTypes of Text Summarization  Methods  : \\nText summarization methods can be classified into different types.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eda26f7c-85ad-44b9-b32c-070b21b6e720', embedding=None, metadata={'page_label': '218', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  11 | 11 \\n \\n     \\nExample:  \\n \\n \\n \\n-------------------------------------------------------------------------------------------------------------  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d0fe3ce6-921d-4553-ac7b-045de91b9a83', embedding=None, metadata={'page_label': '219', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW \\nPREPARATION  \\n(30 Days  of Interview  \\nPreparation ) \\n# Day-18  \\n \\n \\n \\n \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6fcd3395-5ef9-4d75-b51b-62198f3eee0e', embedding=None, metadata={'page_label': '220', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nQ1. What is   Levenshtein Algorithm?  \\nAnswer:  \\nLevenshtein distance is a string metric for measuring  the difference be tween two sequences.  The \\nLevenshtein  distance between two words is the minimum number of single -character edits (i.e. \\ninsertions, deletions or substitutions) required to change one word into the other.   \\nBy Mathematically, the Levenshtein distance between  the two strings a, b (of length |a| and |b| \\nrespectively) is given by  the leva, b( |a| , |b| ) where  :  \\n \\n \\nWhere, 1 (ai≠bi): This is the indicator function equal to zero  when ai≠bi and equal to 1 otherwise, \\nand leva, b(i,j) is the distance between the first i characters of a and the first j characters of b.  \\nExample:  \\nThe Levenshtein distance between \" HONDA \" and \" HYUNDAI \" is 3, since the following three edits \\nchange one into the other, and there is no way to do it with fewer than three edits:  \\n                                                               \\n                            \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d089acda-49ba-4dcb-9d33-85bd72667fa4', embedding=None, metadata={'page_label': '221', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nQ2. What is Soundex?  \\nAnswer:  \\nSoundex attempts to find similar names or homophones using phonetic notation. The program retains \\nletters according to detailed equations, to match individual titles for purposes of ampl e volume \\nresearch.  \\nSoundex phonetic algorithm : Its indexes strings depend  on their English pronunciation . The \\nalgorithm is use d to describe  homophon es, words that are pronounced the same, but spel t differently.  \\nSuppo se we have the following sourceDF.  \\n \\nLet’s run below code and see how the soundex  algorithm encodes the above words . \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='89542c44-b30b-4c4c-9a00-6b656fcd272d', embedding=None, metadata={'page_label': '222', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nLet’s summarize the  above  results:  \\n\\uf0b7 \"two\"  and \"to\" both are encoded as  T000  \\n\\uf0b7 \"break\"  and \"brake \" both are  encoded as  B620  \\n\\uf0b7 \"hear\"  and \"here \" both are  encoded as  H600  \\n\\uf0b7 \"free\"  is encoded as  F600  and \"tree\"  is encoded as  T600 :  Encodings are similar , but word is \\ndifferent  \\nThe Soundex algorithm was often used to compare first names that were spelt differently.  \\nQ3. What is C onstituency parse ? \\nAnswer:  \\nA constituency parse tree breaks a text into sub -phrases. Non -terminals in the tree are types of \\nphrases, the terminals are the words in the sentence, and the edges are unlabeled. For a simple \\nsentence , \"John sees Bill\", a constituency parse would be:  \\n \\nAbove  approaches  convert the parse tree into a sequence following a depth -first traversal to be able \\nto apply sequence -to-sequence models to it. The linearized version  of the above parse tree looks as \\nfollows: (S (N) (VP V N)).  \\nQ4. What is LDA (Latent Dirichlet Allo cation) ? \\nAnswer:  \\nLDA : It is used to classify text in the  document to a specific  topic. LDA builds a topic per document \\nmodel and words per topic model, mode lled as Dirichlet distributions.  \\n\\uf0b7 Each docume nt is modeled as a  distribution of topics , and each topic is mode lled as \\nmultinomial distribution of words.  \\n\\uf0b7 LDA assumes that every chunk of text we feed into it will contain words that are somehow \\nrelated. Therefore choosing the right corpu s of data is crucial.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4d16a20f-368b-4e4e-a76c-d1784661acb2', embedding=None, metadata={'page_label': '223', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n\\uf0b7 It also assumes documents are produced from a mixture of topics. Those topics then generate \\nwords based on their probability distribution.  \\n The Bayesian version of P LSA  is LDA . It uses Dirichlet priors for the word -topic and document -\\ntopic distributions, lending itself to better generalization.  \\nWhat  LDA give  us? \\nIt is a probabilistic method. For every  document , the results give us a mix ture of topics that make up  \\nthe document . To be precise, we can get probability distribution over the  k topics for every  document . \\nEvery word in the document  is attributed to the  particular topi c with probability given by  distribution.  \\nThese  topics themselves were  defined as pr obability distributions over  vocabulary . Our results are \\ntwo sets of probability distributions:  \\n\\uf0b7 The collection  of distributions of topics f or each  document  \\n\\uf0b7 The c ollection  of distributions of words for each topic.  \\n \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d7efcf76-e1cb-42c3-8e52-b42ed7350060', embedding=None, metadata={'page_label': '224', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\n \\nQ5.What is LSA?  \\nAnswer:  \\nLatent Semantic Analysis  (LSA) : It is a theory and  the method for extract and represent s the \\ncontextual usage meaning of w ords by stat istical computa tion applied to large corpus of texts.   \\nIt is an information retrieval technique which analyzes and identifies the pattern in an unstr uctured \\ncollection of text and  relationship between them.  \\nLatent Semantic Analysis itself is an unsupervised way of uncovering synonyms in a collection of \\ndocuments.  \\nWhy LSA (Latent Semantic Analysis) ? \\nLSA is a technique for creating vector represe ntation of  the document. Having a vector representation \\nof the document gives us  a way to compare documents for thei r similarity by calculating the distance \\nbetween vectors. In turn,  means we can do handy things such as classify  documents to find out which  \\nof a set knows topics they most likely reside  to. \\nClassification implies we have some known topics that we  want to group documents into, and that \\nyou have some labelled training data. If you 're going  to identify natural groupings of the documents \\nwithout any labelled da ta, you can use clustering  \\n \\n \\n \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='654a2ad1-408f-4785-8b8a-edfde54be36a', embedding=None, metadata={'page_label': '225', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nQ6. What is P LSA?  \\nAnswer:  \\nPLSA stands f or Probabilistic Latent Semantic Analysis, uses a probabilistic met hod instead  of SVD \\nto tackle  problem. The main idea is to find the  probabilistic model with latent topics that we \\ncan generate  data we observe in our document term matrix. Specifically, we want a  model P(D,  W) \\nsuch that for any document  d and word  w, P(d,w) corresponds to that entry in  document -term matrix.  \\nEach document is found in  the mixture of topics, and each topic cons ists of  the collection of w ords. \\nPLSA adds the  probabilistic spin to these assumptions:  \\n\\uf0b7 Given  document d, topic z is  available in that document with the probability P(z|d)  \\n\\uf0b7 Given the  topic z, word w is drawn from z with probability P(w|z)  \\n \\n \\nThe joint probability of seeing the  given document and word together is:  \\n \\n \\n \\nIn the above  case, P(D), P(Z|D), and P(W|Z) are  the parameters of our model s. P(D) can be \\ndetermined directly from  corpus. P(Z|D) and the P(W|Z) are mode lled as multinomial \\ndistributions  and can be trained using the  expectation -maximisation  algorit hm (EM).  \\n \\nQ7. What is LDA2Vec?  \\nAnswer:  \\nIt is i nspired by LDA,  word2vec model is expanded to simulta neously learn word, document, topic \\nand paragraph  topic vectors.  \\nLda2vec is obtained by modifying the skip -gram word2vec variant. In the original skip -gram method, \\nthe model is trained to predict context words based on a pivot word. In lda2vec, the pivot word vector \\nand a document vector are added to obtain a context vector. This context vector is then used to predict \\ncontext words.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ebb746a7-e233-4118-9953-951c1cf72b84', embedding=None, metadata={'page_label': '226', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nAt the document level, w e know how to represent the text as mixtures of topics. At the word -level, \\nwe typically use d something like word2vec to obtain vector representations.  It is an extension of \\nword2vec and LDA that jointly learns word, document, and topic vectors . \\nHow does it work ? \\nIt correct ly builds on top of the skip -gram model of word2vec to generate word vectors.  Neural net \\nthat learns  word embedding by trying to use  input word to predict enclos ing context words.  \\nWith L da2vec, other than  using  the w ord vector direc tly to predict context words,  you leverage \\na context vector  to make the predictio ns. Context vector is created as the sum  of two other vectors: \\nthe word vector  and the  document vector . \\nThe same skip -gram word2vec model generates the word vector . The document vector is most  \\nimpressive. It is a really weighted combination of two other components:  \\n\\uf0b7 the document weight vector , representing the “weights” of each topic in  a document  \\n\\uf0b7 Topic matrix  represents  each topic and its corresponding vector embedding . \\nTogether, a document vector and  word vector generate “conte xt” ve ctors for each word in  a \\ndocument.  lda2vec power  lies i n the fact that it  not only learns  word embeddings for words ; it \\nsimultaneously learns topic representations and document representations as well.  \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6f7c6aa0-a582-4d34-88a7-ca0162a2cdcf', embedding=None, metadata={'page_label': '227', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nQ8. What is Expectation -Maximization Algorithm (EM) ? \\nAnswer:  \\nThe Expect ation -Maximization Algorithm, in  short , EM algorithm , is an approach for maximum \\nlikelihood estimation in the presence of latent variables.  \\nThis algorithm is an iterative approach that cycles between two modes. The first mode attempts to \\npredict  the missing or latent variables  called the estimation -step or E -step. The second mode attempts \\nto optimi se the parameters of the model to explain the data best  called the maximi zation -step or M -\\nstep. \\n\\uf0b7 E-Step. Estimate the missing variables in the datase t. \\n\\uf0b7 M-Step. Maximize the parameters of the model in the presence of the data.  \\nThe EM algorithm can be applied quite widely, although it is perhaps most well known in machine \\nlearning for use in unsupervised learning problems, such as density estimation and clu stering.  \\nFor detail expla nation  of EM is, let us first consider this examp le. Say that we are in a school , and \\ninterested to learn the height distribution of female and male students in the  school . The m ost sensible \\nthing to do, as we  probably would agree with m e, is to randomly take a sample of N students of both \\ngenders, collect their height information and estimate the mean and standard deviation for  male and \\nfemale separately by way of maximum likelihoo d method.  \\nNow say that you  are not able to know the gen der of  student while we collect their height information , \\nand so there are two things you  have to guess/estimate: (1) whether the individual sample of hei ght \\ninformat ion belongs to a male or a female  and (2) the parameters ( μ, θ) for each gender which is \\nnow unobservable. This is tricky because only with the knowledge of who belongs to which group, \\ncan we make reasonable estimates of the group parameters separately. Similarly, only if we know the \\nparameters that define  the groups, can we assign a subject p roperly. How do you  break out of this \\ninfinite loop? Well,  EM algorithm just says to start with initial random guesses.  \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='23c4476f-9d99-44c5-a29c-ec4956ce3953', embedding=None, metadata={'page_label': '228', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nQ9.What is Text classification  in NLP ? \\nAnswer:  \\nText classification  is also known as  text tagging  or text categorization  is a process of categorizing \\ntext into organized groups. By using  NLP , text clas sification  can automatically analyze text and then \\nassign a set of pre -defined  tags or categories based on  content.  \\nUnstructured text is everywhere  on the internet , such a s emails, chat conversations, websites, and the \\nsocial media but it’ s hard to extract value from given  data unless it’s organized in a certain way. \\nDoing so used to be a difficult and expensive process since it required spending time and resources \\nto manually s ort the data or creating handcrafted rules that are difficult to maintain. Text classifiers \\nwith NLP have proven to be a great alternative to structure textual data in a fast, cost -effective, and \\nscalable way.  \\nText classification is becoming an increasingl y important part of businesses as it allows us to get \\ninsights from data and automate business processes quickly . Some of the most  common \\nexamples  and the use cases for automatic text  classification include the following:  \\n\\uf0b7 Sentiment Analysis:  It is the process of understanding if a given text is talking positively or \\nnegatively about a given subject (e.g. for brand monitoring purposes).  \\n\\uf0b7 Topic Detection:  In this,  the task of identifying the theme or to pic of a piece of text (e.g. \\nknow if a product review is about  Ease of Use , Customer Support , or Pricing  when analy sing \\ncustomer feedback).  \\n\\uf0b7 Language Detection:  the procedure of detecting the language of a given text (e.g. know if an \\nincoming support ticket  is written in English or Spanish for automatically routing tickets to \\nthe appropriate team).  \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dc90610d-399c-4765-a9f9-fad56212bd54', embedding=None, metadata={'page_label': '229', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nQ10. What is Word Sense Disambiguation (WSD)?  \\nAnswer:  \\nWSD  (Word Sense Disambiguation ) is a solution to the ambiguity which arises due to different \\nmeaning of words in a different context.  \\nIn natural language processing , word sense disambiguation  (WSD) is the problem of determining \\nwhich \"sense\" (meaning) of a word is activated by the use of the word in a particular context, a \\nprocess which appears to be most ly unconscious in people. WSD is the  natural classification problem: \\nGiven a word and its possible senses, as defined by the dictionary, classify an occurrence of the word \\nin the context into one or more of its sense classes. The features of the context (such as the \\nneighbo uring words) provide the evidence for classification.  \\nFor example, consider the se two below sentences.  \\n“ The bank  will not be acce pting  the cash on  Saturdays.   ” \\n“ The river overflowed the  bank  .” \\nThe word  “ bank  “ in the given  sentence refers to  commercial (finance) banks, while in the second \\nsentence, it refers to  a riverbank. The uncerta inty that arises , due to this  is tough for the  machine to  \\ndetect and resolve. Detection of change  is the first issue and fixing it and displaying the correct output \\nis the second issue.  \\n \\n \\n------------------------------------------------------------------------------------------------------------------  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ce94d2e3-a652-4a64-a462-b9f43c1eb336', embedding=None, metadata={'page_label': '230', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  1 | 12 \\n \\nDATA SCIENCE  \\nINTERVIEW \\nPREPARATION  \\n(30 Days of Interview  \\nPreparation)  \\n \\n# DAY 1 9 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eafeb668-c463-4364-bdd4-3555290751a8', embedding=None, metadata={'page_label': '231', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\n \\nP a g e  2 | 12 \\n \\nQ1. What is LSI (Latent Semantic Indexing) ? \\nAnswer : \\nLatent Semantic Indexing (LSI) : It is an indexing and retrieval method that uses a mathematical \\ntechnique called  SVD (Singular value decomposition ) to find patte rns in relationships between terms \\nand concepts contained in an unst ructured collection of text. It  is based on the principle that words \\nthat are used in the same contexts tend to have similar meanings.  \\nFor example, Tiger and Woods  are assoc iated with men instead of an animal , and a Wood, Parris , \\nand Hilton are associated with the singer . \\nExample:  \\nIf you use LSI to index a collection of  articles and the words “fan” and “regulator ” appear together \\nfrequently enough, the search algo rithm would  notice that  the two terms are semantica lly close. A \\nsearch for “fan ” will , therefore,  return a set of items  containing that phrase, but also items that contain \\njust the word “regulator ”. It does n't understand  word distance, but by examining a sufficient numb er \\nof documents, it only knows the two terms are interrelated. It then use s that information to provide  \\nan expanded set of results with better recall than a n understandable  keyword search.  \\nThe diagram below describe s the effect between LSI and keyword search es. W stands for a do cument . \\n \\n \\n \\n \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c93f3b75-8c4e-4658-ac74-ca1047eb686f', embedding=None, metadata={'page_label': '232', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  3 | 12 \\n \\n \\nQ2. What is Named Entity Recognition?  And tell some use cases of \\nNER?  \\nAnswer : \\nNamed -entity recognition  (NER) : It is also known as entity extraction , and entity i dentification  is a \\nsubtask of  information extraction  that explore  to locate and classify atomic elements in \\ntext into predefined categories  like the names of  persons,  organizations,  places,  expressions of \\ntimes,  quantities,  monetary values,  percentages  and more.  \\nIn each text document, particular terms  represent specific entities that  are mo re informative and have \\na different  context. These enti ties are called  named entities, which more accurate ly refer to condition s \\nthat represent real -world objects like people, places, orga nizations  or institutions , and so on, which \\nare often expressed  by proper names.  The n aive approach could be to find these by having a look  at \\nthe noun phrases in text docu ments.  It also is known as entity chunking/extraction , which is a popular \\ntechnique used in in formation extraction to analyze  and segment the named enti ties and categorize  \\nor classify them under various predefined classes.  \\n \\nNamed Entity Recognition  use-case \\n\\uf0b7 Classifying content for news providers - \\nNER  can automatically scan entire articles and reveal which are the significant  people, \\norganiz ations, and places discussed in them. Knowing the relevant tags for each item helps \\nin automatically categorizing the articles in defined hierarchies and enable smooth content \\ndiscovery.  \\n\\uf0b7 Customer Support:  \\nLet’s say we  are handling  the customer support departm ent of an electronic s store with \\nmultiple branches worldwide ; we go through a number mentio ns in  our customers’ feedback. \\nSuch as  this for instance . \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bd68c02e-8b6f-4332-baad-f11713e26553', embedding=None, metadata={'page_label': '233', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  4 | 12 \\n \\nNow, if we  pass it through the Named Entity Recognition API, it pulls ou t the entities Bangalore  \\n(location) and  Fitbit (Product). This can be then used to categorize the complaint and assign it to the \\nrelevant department within the organization that should be handling this.  \\n \\n \\nQ3. What is perplexity?  \\nAnswer : \\nPerplexity : It is a measurement of how well a probability model predicts a sample . In the context of \\nNLP, perplexity (Confusion)  is one way to evaluate language models . \\n The term perplexity has three clos ely related meanings. It  is a measure of how easy a probability \\ndistribution is to predict. It  is a measure of how variable a pre diction model is. And It  is a measure \\nof prediction error. The third meaning of perplexity is calculated slightly differently , but all three \\nhave the same fundamental idea.  \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3f4377ec-8baa-4695-8ea2-2e99bbc2c272', embedding=None, metadata={'page_label': '234', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  5 | 12 \\n \\nQ4. What is the language model?  \\nAnswer : \\nLanguage Modelling (LM) : It is one of the  essential  parts of mod ern NLP . There are many sorts of \\napplications for Language Modelling, like  Machine Translation, Spell Correction Speech \\nRecognition, Summarization, Question Ans wering, Sentiment analysis , etc. Each of those tasks \\nrequire s the use of  the language model . The l anguage model is need ed to represent the text to a form \\nunderstandable from the machine point of view.  \\nThe s tatistical  language model  is a probability distribution  over a series  of words. Given such a se ries, \\nsay of length  m, it assigns a probability  to the whole se ries. \\nIt provides context to distinguish between phrases  and words  that sound s are  similar. For example, \\nin American English , the phrases \"  wreck a nice bea ch \" and \" recognize speech \" sound alike  but mean \\ndifferent things.  \\nData sparsity is a significant  problem in building language models. Most possible  word sequences \\nare not noticed  in training. One so lution  is to make the inference  that the probability of a word only \\ndepends on the  previous  n words. This is called  as an  n-gram  model or unigram model when  n = 1. \\nThe unigram model is also known as the  bag of words model . \\nHow does this Language Model help  in NLP Tasks?  \\nThe probabilities restoration  by a language model is most  useful to compare  the likelihood that \\ndifferent sentences are \"good sentences .\" This was  useful in many practical tas ks, for example:  \\nSpell checking : You  observe a word that is not  identified  as a k nown word as part of a senten ce. \\nUsing the edit distance algorithm, we find the closest known words to the unknown words. These are \\nthe candidate corrections. For example, we observe the word \"wu rd\" in the context of the s entence , \\n\"I like to write this wu rd.\" The candidate corrections are [\"word\", \"weird\", \"wind\"]. How can we \\nselect among these candidates the most likel y correction for the suspected error \"w eird\"?  \\nAutomatic Speech Recognition : we receive as input a string of phonemes; a first model predicts for \\nsub-sequences of the stream of phonemes candidate words; the language model helps in ranking the \\nmost likely seq uence of words compatible with the candidate words produced by the acoustic model.  \\nMachine Translation : each word from the source language is mapped to multiple candidate words \\nin the target language; the language model in the target language can rank the most likely sequence \\nof candidate target words.  \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='030edbcf-dde2-46c6-8926-d004f4426d2a', embedding=None, metadata={'page_label': '235', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  6 | 12 \\n \\nQ5. What is Word Embedding?  \\nAnswer : \\nA word embedding is a learned representation for text where words that have the same meaning have \\na similar observ ation.  \\nIt is basically a form of word representat ion that bridges the human understanding of language to that \\nof a machine.  Word embeddings  divide  representations of text in an n -dimensional space. These are \\nessential for solving most NLP problems.  \\nAnd the oth er point worth considering is how we obtai n word embeddings as no two sets of word \\nembeddings are similar . Word embeddings aren\\'t random; they\\'re developed  by training the neural \\nnetwork. A recent powerf ul word embedding usage  comes from Google named Word2Vec , which is \\ntrained by predicting  several  words that appear next to other words in a language . For example,  the \\nword \"cat \", the neura l network would  predict the words like \"kitten\" and \"feline .\" This intuition of \\nwords come s out \"near\" each other allows us to place them in vector space.  \\n \\n \\nQ6. Do you have an idea about fastText?  \\nAnswer : \\nfastText : It is another word  embedding  method  that is an extension  of the word2vec \\nmodel.  Alternatively , learning vectors for words directly . It represents each word as an n -gram of \\ncharacters.  So, for example,  take th e word, “ artificial ” with n=3, the  fastText  representation of this \\nword is < ar, art,  rti, tif, ifi, fic,  ici, ial, al >, where the angular  brackets indicate the beginning and end \\nof the word.     \\nThis helps to capture the meaning of shorter words and grant  the embeddings to understand prefixes  \\nand suffixes . Once the word has be en showed using character skip -grams,  a n-gram model is trained \\nto learn  the embeddings. This model is ackno wledged to be a bag of words model with a sliding \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2a171e3d-2d6d-4b88-bb7a-4ee49edf91b6', embedding=None, metadata={'page_label': '236', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  7 | 12 \\n \\nwindow over a word because  no internal structure of the word is taken into account.  As long as  the \\ncharacters  are within this  window, the order of the n -grams  doesn’t  matter.    \\nfastText  works well with rare words. So even if a word  wasn’t  seen during training, it can be broken \\ndown  into n -grams to get its embeddings.  \\nWord2vec and  GloVe  both fail to provide any vector representation for words  that are not in the \\nmodel dictionary. This is  a huge advantage of  this method.    \\n \\n \\n \\nQ7. What is GloVe?  \\nAnswer : \\nGloVe( global vectors ) is for word representation. GloV e is an unsupervised learning algorithm \\ndeveloped by Stanford for achiev ing word embeddings by aggregating a global word -word co -\\noccurrence matrix from a corpus. The resulting embeddings show interesting linear substructures of \\nthe word in vector space.  \\nThe GloVe  model  produces a vector space with meaningful substructure, as evidenced by its \\nperformance of 75% on a new word analogy task. It also outperforms related models on similarity \\ntasks and named entity recognition.  \\nHow GloVe  find meaning in statistics?  \\n Produces a vector space with meaningful substructure, as evidenced by its performance of 75% on \\na new word analogy task. It also outperforms related models on similarity tasks and named entity \\nrecognition.  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='49a77096-64a9-40a3-b979-d9541d9a0ac3', embedding=None, metadata={'page_label': '237', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  8 | 12 \\n \\nGloVe aims to ach ieve two goals:  \\n\\uf0b7 (1) Create word vectors that  capture meaning in vector space  \\n\\uf0b7 (2) Takes advantage of  global count statistics  instead of only local information  \\n Unlike word2vec – which learns by streaming sentences – GloVe determine s based on a  co-\\noccurrence matrix  and trains word vectors , so their differences predict  co-occurrence ratios  \\nGloVe weights the loss based on word frequency . \\nSomewhat surprisingly, word2vec and GloVe turn out to be remarkab ly similar, despite starting off \\nfrom entirely different starti ng points . \\n \\n \\n \\nQ8. Explain  Gensim? \\nAnswer : \\nGensim : It is billed as a Natural Language Processing package that does ‘Topic Modeling for \\nHumans’. But its practically much more than that . \\nIf you are unfamiliar with  topic modeling , it is a technique to extract the underlying topics from \\nlarge volumes of text. Gensim provides algorithms  like LDA and LSI (which we already  seen in \\npreviou s interview questions ) and the necessa ry sophistication to built  high-quality topic models.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f4483737-b157-43c1-96f8-a84060bab468', embedding=None, metadata={'page_label': '238', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  9 | 12 \\n \\nIt is a n excellen t library package for processing texts, working with word vector models (such as \\nFastText , Word2Vec , etc) and for building the topic models.  Another  significan t advan tage with \\ngensim is: it lets us handle large text files without having to load the entire file in memory.  \\nWe can also tell as It is an open -source  library for unsupervised  topic modeling  and natural \\nlanguage processing , using modern statistical  machine learning . \\n \\nGensim is implemented in  Python  and Cython . Gensim is designed to handle extensiv e text \\ncollections using data streaming and incremental online algorithms, which differentiates it from \\nmost other machine learning  software packages that target only in -memory processing.  \\n \\n \\n \\n \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8b30ad74-ebc1-4489-a355-c1e7bae867d2', embedding=None, metadata={'page_label': '239', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  10 | 12 \\n \\nQ9. What is Encoder -Decoder Architecture?  \\nAnswer : \\n \\n \\nThe e ncoder -decoder architecture consists of two main parts :  \\n\\uf0b7 Encoder : \\nEncoder simply takes the input data, and train s on it, then it passes  the final state of its \\nrecurrent layer as an initial state to t he first recurrent layer of the decoder part.  \\n \\n\\uf0b7  Decoder :  \\nThe d ecoder takes the final  state of encoder’s final recurrent layer and uses it as an initial \\nstate to its initial,  recurrent layer , the input of t he decoder is  sequences that we want to get \\nFrench sentences . \\n \\nSome more example for better understanding:  \\n                                   \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='14d8cf74-a556-4698-b054-addcac50fd6f', embedding=None, metadata={'page_label': '240', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\n \\nP a g e  11 | 12 \\n \\n \\n \\nQ10. What is Context2Vec?  \\n \\nAnswer : \\nAssume a case where you have a sentence like.  I can’t find May. Word  May maybe refer s to a month 's \\nname or a person 's name. You use the words surround it (context) to help yourself to determine the \\nbest suitable option. Actually, this problem refers to  the Word Sense Disambiguation  task, on which \\nyou investigate the actual semantics of the word based on several sema ntic and linguistic techniques. \\nThe Context2Vec idea is taken from the original CBOW Word2Vec model, but instead of relying on \\naveraging the embedding of the words, it relies on a much more complex parametric model that is  \\nbased on one layer of Bi -LSTM.  Figure 1 shows the archi tecture of the CBOW model . \\nFigure1  \\n \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='609cd5d4-7f3f-422a-b946-f97c6bc8d2e2', embedding=None, metadata={'page_label': '241', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  12 | 12 \\n \\nContext2Vec applied the same concept  of windowing , but instead of using a simple average \\nfunction, it uses 3 stages to learn complex parametric networks . \\n\\uf0b7 A Bi -LSTM layer that takes left-to-right and right -to-left representations  \\n\\uf0b7 A feedforward network that takes the concatenated hidden representation  and produces a \\nhidden representation through learning the network parameters . \\n\\uf0b7 Finally, we apply the objective function to the network o utput . \\n \\nWe used the Word2Vec negative sampling idea to get better performance while calculating \\nthe loss value.  \\nThe following are some samples of the closest words to a given context . \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ebbfa7e2-7248-45fd-a410-420340cd72f0', embedding=None, metadata={'page_label': '242', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nDATA SCIENCE  \\nINTERVIEW \\nPREPARATION  \\n(30 Days of Interview  \\nPreparation)  \\n \\n# DAY 20  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='260caf05-e052-4380-9ad7-6b8e293a8e1b', embedding=None, metadata={'page_label': '243', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  1 | 12 \\n \\nQ1. Do you have any idea about Event2Mind  in NLP ? \\nAnswer:  \\nYes, it is based on NLP  research paper  to understand the common -sense  inference  from sentences.  \\n              Event2Mind: Common -sense Inference on Events, Intents, and Reactions  \\nThe study of “Commonsense Reasoning” in NLP deals with teaching computers how to gain and \\nemploy common sense knowledge. NLP systems require common sense to adapt quickly and \\nunderstand humans as we talk to each other in a natural environment.  \\nThis paper proposes a new task to teach systems commonsense reasoning: given an event described \\nin a short “event phrase” (e .g. “PersonX drinks coffee in the morning”) , the researchers teach a system \\nto reason  about the likely intents (“PersonX wants to stay awake”) and reactions (“PersonX feels \\nalert”) of the event’s participants.  \\n \\nUnderstanding a narrative requires common -sense reasoning about the mental states of people in \\nrelation to events. For example, i f “Robert  is dragging his feet at work ,” pragmatic implications about \\nRobert ’s intent  are that “ Robert  wants to avoid doing things” ( Above Fig). You  can also infer that \\nRobert ’s emotional reaction  might be feeling “bored” or “lazy.”  Furthermore, wh ile not explicitly \\nmentioned, you  can assume  that people other than Robert  are affected by the situation, and these \\npeople are likely to feel “ impatient” or “frustrated.”  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e3d8d460-e1c5-4387-a5ab-5b551a9f235c', embedding=None, metadata={'page_label': '244', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  2 | 12 \\n \\nThis type of pragmatic inference can likely  be useful for a wide range of NLP applications th at require \\naccurate anticipation of people’s intents and emotional react ions, even when they are not express ly \\nmentioned. For example, an ideal dialogue system should react in empathetic ways by reasoning \\nabout the human user’s mental state based on the ev ents the user has experienced, without the user \\nexplicitly stating how they are feeling. Furthermore , advertisement systems on social media should \\nbe able to reason about the emotional reactions of people after events such as mass shootings and \\nremove ads for guns , which  might increase social distress.  Also, the pragmatic inference is a \\nnecessary step toward automatic narrative understanding and generation . However, this type of \\ncommonsense social  reasoning goes far beyond the widely studied entailment task s and thus falls \\noutside the scope of existing benchmarks . \\n \\nQ2. What is SWAG in NLP?  \\nAnswer:  \\nSWAG stands for Situations with Adversarial Generations  is a dataset consisting of 113k multiple -\\nchoice questions about a rich spectrum of grounded situations.  \\n      Swag : A Large Scale Adversarial Dataset for Grounded Commonsense Inference  \\nAccording to NLP  research paper on SWAG is “Give n a partial description like “ he opened the hood \\nof the car,” humans can reason about the situation and anticipat e what might come next ( “then, he \\nexamined  the engine”). In this paper, you  introduce the task of grounded commonsense inference, \\nunifying natural language inference (NLI),  and common -sense reasoning.  \\nWe present S WAG , a dataset with 113k multiple -choice questions about the rich spectrum of \\ngrounded positions. To address recurring challenges of  annotation artifacts and human biases found \\nin many existing datasets, we propose  AF(Adversarial Filtering ), a novel procedure that constructs a \\nde-biased dataset by iteratively training an e nsemble of stylistic classifiers, and using them to filter \\nthe data. To account for the aggressive adversarial filtering, we use state -of-the-art language models \\nto oversample a diverse set of potential counterfactuals massively . Empirical results present  that \\nwhile humans can solve the resulting inference problems with high accuracy (88%), various \\ncompetitive models make an effort on our task. We provide a comprehensive analysis that indicates \\nsignificant opportunities for future research.  \\nWhen we read a t ale, we bring to it a large body  of implied knowledge about  the physical world. For \\ninstance, given the context “on  stage, a man takes a seat at the piano,” we can easily infer what the \\nsituation might  look like: a man is giving a piano perfor mance, with a  crowd watching him . We can \\nfurthermore  infer him  likely  next action: he will most likely set his  fingers on t he piano key  and start \\nplaying.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6c042b3e-95c3-41b8-81f3-14d6b5fd4fa0', embedding=None, metadata={'page_label': '245', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  3 | 12 \\n \\nThis type of natural language inference (NLI)  requires common -sense reasoni ng, substantially \\nbroadening  the scope o f prior work that focused primarily on linguistic entailment . Whereas the  \\ndominant  entailment paradigm asks if 2  natural language sentences (the ‘premise’ and the \\n‘hypothesis’) describe the same set of possible wor lds,  here we focus on whether a (multiple -choice) \\nending represent s a possible ( future ) world that can a  from the situation described in the premise, \\neven when it is not strictly entailed. Making such inference necessitates a rich understanding of \\neveryday physical condi tions, including object a ffordances  and frame semantics.  \\n \\n \\n \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='00259ba1-4eea-46f6-b8cc-c2272d44fb61', embedding=None, metadata={'page_label': '246', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  4 | 12 \\n \\nQ3. What is the Pix2Pix network?  \\nAnswer:  \\nPix2Pix network: It is a Conditional GANs  (cGAN)  that learn the mapping from an input image to \\noutput an image.  \\n \\nImage -To-Image Translation  is the  process for trans lating one representation of the image into \\nanother representation.  \\nThe image -to-image translation is another example of a task that GANs ( Generative Adversarial \\nNetworks ) are ideal ly suited for. These are  tasks in which it is nearly impossible to hard -code a loss \\nfunction. S tudies on GANs are concerned with novel image synthesis, translating from a random \\nvector z into an image. Image -to-Image translation converts one image to another like the edges of \\nthe bag below to the photo image. Another exciting  example of this is shown below:  \\n \\nIn Pix2Pix Dual Objective Function with an Adversarial and L1 Loss  \\nA naive way to do Image -to-Image tran slation would be to discard the adversarial framework \\naltogether. A so urce imag e would just be passed through a  parametric function , and the differenc e in \\nthe resulting image and the  ground truth output would be used to update the weights of the \\nnetwork.  However, designing this loss function with standard distance measures s uch as L1 and L2 \\nwill fail to capture many of the essential  distinctive characteristics be tween these images. However,  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6a064b34-8ec5-400b-9c5f-1b8214619c34', embedding=None, metadata={'page_label': '247', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  5 | 12 \\n \\nauthors do find some value to the L1 loss function as a weighted sidekick to the adversarial loss \\nfunction.  \\nThe Conditional -Adversarial L oss (Generator versus Discriminator) is very popularly formatted as \\nfollows:  \\n \\nThe L1 loss function previously mentioned is shown below:  \\n \\n \\n \\nCombining these functions results in:  \\n \\nIn the experiments, the authors report that they found the most success wit h the lambda parameter \\nequal to 100.  \\n \\n \\nQ4. Explain UNet Architecture?  \\nAnswer:  \\nU-Net architecture : It is built upon the Fully Convolutional Network and modified in a way that it \\nyields better segmentation in medical imaging. Compared to FCN -8, the two main differences are (a) \\nU-net is symmetric and (b ) the skip connections betwe en the downsampling path and  upsampling \\npath apply a concatenation operator instead of a sum. These skip connections intend to provide local \\ninformation to the global in formation while upsampling. Because of its symmetry, the network has a \\nlarge number of feature maps in the upsampling path, which allows t ransferring  information. By \\ncomparison, the underlying  FCN architecture only had  the number of classes  feature maps in  its \\nupsampling way. \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c6b8a617-70ef-44a0-9adb-f0d5d951efac', embedding=None, metadata={'page_label': '248', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  6 | 12 \\n \\n How does it work ? \\n \\nThe UNet architecture looks like a ‘U ,’ which justifies its name. This UNet architecture consists of \\n3 sections: The contraction, the bottleneck, and the expansion section. The contraction section is \\nmade of many contraction blocks. Each block takes an input that applies two 3X3 convolution layers , \\nfollowed by a 2X2 max pooling. The number of features  or kernel  maps after each block doubles so \\nthat UNet architecture can learn complex structures. B ottommost layer mediates between  the \\ncontraction layer and the  expansion layer. It uses two 3X3 CNN layers followed by 2X2 up \\nconvolution layer.  \\nBut the heart of t his architecture lies in the expansion section. Similar to the contraction layer, it also \\nhas several expansi on blocks. Each block passes  input to two 3X3 CNN layers , followed  by a 2X2 \\nupsampling layer. A fter each block number of feature maps used by the convolutional layer , get half \\nto maintain s ymmetry. However, every time  input is also get appended by feature maps of the \\ncorresponding contraction layer. This action would e nsure that features that are  learned while \\ncontracting the image will be used to re construct it. The number of expansion blocks is as same as \\nthe number of contraction block s. After that, the resultant mapping passes through another 3X3 CNN \\nlayer , with the number of feature maps equal to the number of segments desired.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2bf87983-5b0f-49e1-b151-19a955e81141', embedding=None, metadata={'page_label': '249', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  7 | 12 \\n \\n \\n \\n \\nQ5. What is pair 2vec?  \\nAnswer:  \\n This paper pre  trains  word pair representations  by maximizing  pointwise mutual information of \\npairs of words with th eir context. This encourages  a model to learn more meaningful representations \\nof word pairs than  with more general objectives, like modeling. The pre -trained representations are \\nuseful  in tasks like SQuAD and MultiNLI that requi re cross -sentence inference. You can expect to \\nsee more pretraining tasks that capture properties particularly suited to specific  downstream tasks and \\nare complementary to more general -purpose tasks like language modeling.  \\nReasoni ng about implied relationships  between pairs of words is crucial for cross sentence s inference \\nproblems like question answering (QA) and natural language i nference (NLI). In NLI, e.g., given a \\npremise such as “ golf is prohibitively expensive ,” inferring that the hypothesis “ golf is a cheap \\npastime ” is a contradiction requires one to know that  expensive  and cheap  are antonyms. Recent \\nwork  has shown that curre nt models, which rely heavily on unsupervised single -word embeddings, \\nstruggle to  grasp  such relationships. In this pair2vec paper, w e show that they can be learned with \\nword  pair2vec(pair vector) , which are trained, unsupervised, at a huge scale, and whic h significantly \\nimprove performance when added to existing cross -sentence attention mechanisms . \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cd378ec9-79ab-4935-b5db-ef08146590f1', embedding=None, metadata={'page_label': '250', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  8 | 12 \\n \\n \\nUnlike single word representations, which are typically trained by modeling the co -occurrence of a \\ntarget word  x with its context  c, our word -pair representat ions are learned by modeling the three -way \\nco-occurrence between two words  (x,y)  and the context  c that ties them together, as illustrated in \\nabove Table . While similar training signal  has been used to learn models for ontology \\nconstruction  and knowledge b ase completion , this paper shows, for the first time, that considerabl e \\nscale learning of pairwise embeddings can be used to improve the performance of neural cross -\\nsentence inference models directly . \\n \\n \\nQ6. What is Meta -Learning?  \\nAnswer:  \\nMeta -learning : It is an exciting a rea of research that tackles  the problem of learning to learn. The goal \\nis to design models that  can learn new skills or fastly  to adapt to  new environments with minimum  \\ntraining examples. Not only does this drama tically speed up and impro ve the design of ML(Machine \\nLearning ) pipelines or neural architectures, but it also allows us to replace hand -engineered \\nalgorithms with novel approach es learned in a data -driven way.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='04b071b3-f28b-4c2b-9302-951678c5c6a7', embedding=None, metadata={'page_label': '251', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  9 | 12 \\n \\nThe goa l of meta -learning is to train the  model on a variety of learnin g tasks, such that it can solve \\nnew learning tasks with only a small number of training samples. It tends to focus on finding model \\nagnostic  solutions, whereas multi -task learning remains deeply tied to model architecture.  \\nThus, meta -level AI algorithms ma ke AI systems:  \\n· Learn faster  \\n· Generalizable to many tasks  \\n· Adaptable to environmental changes like in Reinforcement Learning  \\nOne can solve any problem with a single model, but meta -learning should not be confused with one -\\nshot learning.  \\n \\n \\nQ7. What is A LiPy(Active Learning in Python)?  \\nAnswer:  \\nSupervised ML methods usually require a large set of labeled examples for model training. \\nHowever, in many real applications, there are ample  unlabeled data b ut limited labeled data; and  \\nacquisition of labels is costly. Active learning (AL) reduces  labeling cost s by iteratively selecting \\nthe most valuable data to query their labels from the annotator.  \\nActive learning is the leading  approach to learning with limited labe led data. It tries to reduce  \\nhuman efforts on data an notation by actively querying the most promine nt examples . \\nALiPy is a Python toolbox for active learning (AL) , which is suitable for various users. On  the one \\nhand, the entire  process of active learning has been well implemented. Users can e fficient ly \\nperfo rm experiments by  many  lines of codes to finish the entire process from data pre -proces ses to \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='34df62c7-4205-4fbc-a5a1-182135ab7a41', embedding=None, metadata={'page_label': '252', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  10 | 12 \\n \\nresult  in visualization. M ore than 20 commonly used active learning (AL)  methods ha ve been \\nimplemented in the toolbox, providing users many choices . \\n \\nQ8.What is the Lingvo model?  \\nAnswer:  \\nLingvo : It is a Tensorflow framework offering a complete solution for collaborative deep  learning \\nresearch, with a particular focus towards sequence -to-sequence models.  These  models are composed \\nof modular building blocks that are flexible and easily extensible, and experiment configurations are \\ncentralized and highly customizable. Distributed training and quantized inference are supported \\ndirectly within  a framework, and it contains existing implementations of an ample  number of util ities, \\nhelper functions, and  newest research ideas. This model  has been used in collaboration by dozens of \\nresearchers in more than 20 papers over the last two years . \\nWhy does this Lingvo re search matter?  \\nThe process of establishing a new deep learning (DL)  system  is quite complicated. It involves \\nexploring a n ampl e space  of design choices involving training data,  data processing logic, the size , \\nand type of model components, the opti mization procedures, and the path  to deploy ment. This \\ncomplexity requires  the framework that quickly facilitates  the production of new combinations and \\nthe modifi cations from existing documents and experiment s and shares these new results. It is a \\nworkspace ready to be u sed by deep learning researchers or developers . Nguyen  Says : “We ha ve \\nresearchers working on state -of-the-art(SOTA)  products and research algorithms, basing their \\nresearch off of  the same  codebase. This ensures that  code is battle -tested. Our collectiv e experience \\nis encoded in means  of good defaults and primitives that w e have found useful over these tasks.”  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='760b4f92-df95-433d-b161-48fa0c6ee936', embedding=None, metadata={'page_label': '253', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  11 | 12 \\n \\n \\n \\nQ9. What is Dropout Neural Networks?  \\nAnswer:  \\nThe term “dropout” refers to dropping out units (both hidden and visible) in a neural network.  \\nAt each training stage, individual nodes are either dropped out of the net with probability  1-p or kept \\nwith probability  p, so that a reduced network is left; incoming and outgoing edges to a dropped -out \\nnode are also removed.  \\nWhy do we need Dropout?  \\nThe answer to these questions is “to prevent over -fitting .” \\nA fully connected layer o ccupies most of the parameters, and hence, neurons develop co -dependency \\namongst each other during training , which curbs the individual power of each neuron leading to over -\\nfitting of training data.  \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4a7ed182-442a-4676-b4ec-fe5131c1eeb2', embedding=None, metadata={'page_label': '254', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  12 | 12 \\n \\nQ10. What is GAN?  \\nAnswer:  \\nA generative adversarial network  (GAN ): It is a class of  machine learning  systems invented by  Ian \\nGoodfellow  and his colleagues  in 2014.  Two neural networks  are contest ing with each other in a \\ngame  (in the idea of game theory , often but not always in  the form of a  zero-sum game ). Given a \\ntraining set, this technique lear ns to generate new data with  the same statistics as the training set. \\nE.g., a GAN trained on photographs can produc e original  picture s that look at least superficially \\nauthentic to human observers, having many realistic characteristics. Though initially proposed as a \\nform of  a generative model  for unsupervised learning , GANs have also proven useful for  semi -\\nsupervised learning ,[2] fully  supervised learning , and reinforcement learning . \\nExample of GAN  \\n \\n\\uf0b7 Given an image of  a face, the network can construct an image that represents how that person \\ncould look when they are old.  \\n \\nGenerative Adversarial Networks takes up a game -theoretic approach, unlike a conventional neural \\nnetwork. The network learns to generate from a train ing distribution through a 2 -player game. The \\ntwo entities are Generator and Discriminator. These two adversaries are in constant battle throughout \\nthe training process.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7e3e9acf-0953-4939-ba54-5d38201760bc', embedding=None, metadata={'page_label': '255', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  1 | 15 \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW  \\nPREPARATION  \\n   (30 Days of Interview \\nPreparation)  \\n \\n# Day21  \\n  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c6a017cc-dda8-42fe-bec1-e97ef92a0766', embedding=None, metadata={'page_label': '256', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  2 | 15 \\n \\nQ1. Explain Grad -CAM  architecture?  \\nAnswer:  \\nAccording to the research paper, “ We propose a technique  for making Convolutional Neural Network \\n(CNN) -based models more transparent by visualizing input regions that are ‘important’ for \\npredictions – producing  visual explanations . Our approach is  called Gradient -weighted Class \\nActivation Mapping (Grad -CAM), which uses class -specific gradient information to localize the \\ncrucial  regions. These localizations are combined with  the existing pixel -space visualizations to \\ncreate a n ew high-resolution , and class -discriminative display  called  the Guided Grad -CAM. These \\nmethods help better  to understand CNN -based models, including image captioning and  the apparent  \\nquestion answering (VQA) models. We evaluate our visual explanations by measuring the ability to \\ndiscriminate between  the classes and  to inspire trust in humans, and their correlation with the \\nocclusion maps. Grad -CAM provides a new way to understand the CNN -based models. ” \\nA technique for making CNN( Convolutional Neural Network) -based models more transparent by \\nvisualizing the regions of input that are “important” for predictions from these models — or visual \\nexplanations.  \\n \\nThis visualization is both high -resolution (when the class of interest is ‘tiger cat ,’ it identifies crucial  \\n‘tiger cat’ features like stripes, pointy ears and eyes) a nd class -discriminative (it shows the ‘tiger cat’ \\nbut not the ‘boxer (dog)’).  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e4be4291-c537-43bf-8368-e65fcc4891d7', embedding=None, metadata={'page_label': '257', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  3 | 15 \\n \\nQ2.Explain squeeze -net architecture?  \\nAnswer:  \\nNowadays, technology is at its peak. Self -driving cars and IoT is going to be household talks in the \\nnext few years to come. Therefore, everything is controlled remotely, say , e.g., in self -driving cars , \\nwe will need our system to co mmunicate with the servers regularly . So accordingly , if we have a \\nmodel that has a small size , then we can quick ly deploy it in the cloud. So that’ s why we needed a n \\narchitecture that is less in size and also achieve s the same level of accuracy  that other architecture \\nachieves.  \\nIt’s Architecture  \\n\\uf0b7 Repl ace 3x3 filters with 1x1 filter - We plan to use the maximum number of 1x1 filters as \\nusing a 1X1 filter rather than a 3X3 filter can reduce the number of parameters by 9X. We  \\nmay think that replacing 3X3 filters with 1X1 filters may perform badly  as it has less \\ninformation  to work on. But this is not a case . Typically 3X3 filter may capture the spatial \\ninformation of pixe ls close to each other while the 1X1 filter zeros in on pixel and captures  \\nfeatures amongst its channels.  \\n \\n\\uf0b7 Decrease number of input channels to 3x3 filters - to maintain a small total number of \\nparameters in a CNN,  and it is crucial  not only to decrease the number of 3x3 filters, but also \\nto decrease the  number of input channels to  3x3 filters. We reduc e the number of input \\nchannels to 3x3 filters using squeeze layers. The author of this  paper  has used a term called \\nthe “ fire module ,” in which  there is a squeeze layer and an expan ded layer. In  the squeeze \\nlayer , we are  using 1X1 filters , while in the expanded layer, we are using  a combo of 3X3 \\nfilters and 1X1 filters . The author is trying to limit  the number of inputs to  3X3 filters to \\nreduce the number of parameters in the layer.  \\n                      \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1a47db6a-f9c2-41fe-aff9-54ff16625a34', embedding=None, metadata={'page_label': '258', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  4 | 15 \\n \\n\\uf0b7 Downsample  late in  a network so that convolution layers have a large activation \\nmap - Having got an intuition about contracting  the sheer number of paramet ers we ar e \\nworking with, how the  model is g etting  most out of the remaining set of parameters. The \\nauthor in this  paper has downsampled the feature map in later layers , and this increases \\nthe accuracy. But this is a n excellen t contrast to networks like VGG where a large feature \\nmap is taken , and then it gets smaller as  network approach towards the end. This different \\napproach is too interesting , and they cite the  paper by K. He and H. Sun  that similarly \\napplies delayed downsampling that leads to higher classification accuracy.  \\nThis architecture consists of the fire module , which enables it to bring down the number \\nof parameters.  \\n \\nAnd other thing that surprises  me is  the lack of fully conn ected layers or dense layers at the end , \\nwhich one will see in a typical CNN architecture. The dense layers , in the end,  learn all the \\nrelationship s between the high -level features and the classes it is trying to identify. The fully \\nconnected layers are de signed to learn that noses and ears make up a face, and wheels and lights \\nindicate cars. However, in this architecture , that extra learning step seems to be embedded within the \\ntransformations between various “fire modules .” \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cc777f2a-004d-46f8-bd22-72356daa3209', embedding=None, metadata={'page_label': '259', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  5 | 15 \\n \\n \\nThe squeeze -net can accomplis h an accuracy nearly equal to AlexNet with 50X less number of \\nparameters. The most impressive part is that if we apply Deep compression to the already smaller \\nmodel , then it can reduce the size of the squeeze -net model to 510x times that of AlexNet.  \\n \\nQ3.ZFNet architecture  \\nAnswer:  \\n \\nThe architecture of  the network is an optimized version of the  last year’s winner - AlexNet. The \\nauthors spent some time to find out the bottlenecks of AlexNet and removing them, achieving \\nsuperior performance.  \\n \\n(a): First  layer ZFNET features without feature scale clipping. (b): the First  layer features fro m \\nAlexNet. Note that there are  lot of dead features - ones where the network did not learn any patterns. \\n(c): the First  layer features for ZFNet. Note that there are onl y a few de ad features. (d): Second  layer \\nfeatures from AlexNet. The grid -like patterns are so -called aliasing artifacts. They appear when \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='136f1835-74c8-47ff-b779-2c3590243957', embedding=None, metadata={'page_label': '260', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  6 | 15 \\n \\nreceptive fields of convolutional neurons overlap , and neighboring neurons learn similar structure s. \\n(e): 2nd layer features for ZFNet. Note that there are no aliasing artifacts. Source: original paper.  \\nIn particular, they reduced the filter size in the 1st convolutional layer from 11x11 to 7x7, which \\nresulted in fewer dead features learned in the first layer (see the i mage below for an example of that). \\nA dead feature is a situation where a convolutional kernel fails to learn any significant representation. \\nVisually it looks like a monotonic single -color image, where all the values are close to each other.  \\nIn addition t o changing the filter size, the authors of FZNet have doubled the number of filters in all \\nconvolutional layers and the number of neurons in the fully connected layers as compared to the \\nAlexNet. In the AlexNet, there were 48 -128-192-192-128-2048 -2048 kern els/neurons, and in the \\nZFNet , all these doubled to 96 -256-384-384-256-4096 -4096. This modification allowed the network \\nto increase the complexity of internal representations and as a result, decrease the error rate from \\n15.4% for last year’s winner, to 14 .8% to become the winner in 2013.  \\n \\n \\nQ4. What is NAS ( Neural Architecture Search)?  \\nAnswer:  \\nDeveloping the neural network models often requires significa nt architecture engineering. We  can \\nsometimes get by with transfer learning , but if we  want the best possible performance , it’s usually \\nbest to design your network. This requires speciali zed skills and is challenging in general; we may \\nnot even know the limits of the current state -of-the-art(SOTA)  techniques. It s a lot of trial and error, \\nand experimentation itself is time -consuming and expensive.  \\nThis is the NAS (Neural Architecture Search)  comes in. NAS (Neural Architecture Search)  is an \\nalgorith m that searches  for the best neural network architecture . Most of the algorithms work in th e \\nfollow ing way. Start off by defining the  set of “building blocks” that can be used for our network.  \\nE.g., the state -of-the-art(SOTA)  NASNet paper  proposes these commonly used blocks for an image \\nrecognition network - \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='13265514-09b4-4282-82bd-01e37ba2526f', embedding=None, metadata={'page_label': '261', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  7 | 15 \\n \\nIn the NAS algorithm,  the controller Recurrent Neural Network (RNN) samples the building blocks, \\nputting th em together to create some end to end architecture.  Architecture generally combines  the \\nsame style as state -of-the-art(SOTA)  networks, such as DenseNets  or ResNets, but uses  a much \\ndifferent combination and the configuration of  blocks.  \\nThis new network architecture is then trained to converge nce to obtain the least  accuracy on  the held-\\nout validation set. The resulting efficien cies are used to update the controller so that the c ontroller \\nwill generate better architectures over time, perhaps by selecting better blocks or making better \\nconnections. The controller weights are updated with a policy gradient. The whole end -to-end setup \\nis shown below.  \\n \\nIt’s a reasonab ly int uitive app roach! In simple means : have an algorithm grab different blocks and \\nput those blocks together to make the  network. Train and test out that network. B ased on our results, \\nadjust  the blocks we used to make the network and how you put them together!  \\n \\nQ5. What is SENets?  \\nAnswer:  \\n \\nSENets  stands for Squeeze -and-Excitation Networks  introduce s a building block for CNNs that \\nimproves channel interdependencies at almost no computational cost. They have used in the 2017 \\nImageNet competition and helped to improve the result from last year by 25%. Besides this large \\nperformance boost, they can be easily added to existing architectures. The  idea is this:  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='86d50f8b-e807-4c93-b6b3-4d98c5376f1b', embedding=None, metadata={'page_label': '262', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\n \\nP a g e  8 | 15 \\n \\nLet’s add  parameters to each channel of the  convolut ional block so that the network can adaptively \\nadjust the weighting of each feature map.  \\nAs simple as may it sound , this is it.  So, let’s take a closer look at why this works so well.  \\nWhy it works too well?  \\nCNN 's use s its convolutional filters to extract h ierarchal information from the images. Lower layers \\nfind little pieces of context like high frequencies  or edges , while upper layers can detect faces, text , \\nor other complex geometrical shapes. They extract w hatever is necessary to solve the task precisely . \\nAll of this works by fusing  spatial and channel information of an image. The different filters will first \\nfind the spatial features in each input channel before adding the information across all available \\noutput channels.   \\nAll we  need to understand for now is that  the network weights each of its ch annels equally when \\ncreating output feature maps. It is all about changing this by adding a content -aware mechanism to \\nweight each c hannel adaptively. In its too basic form, this could m ean adding  a single parameter  to \\neach channel and giving it  linear scalar how relevant each one is.  \\nHowever, the authors push it  a little further. First, they get the  global understanding of each channel \\nby squeezing  feature maps to a single numeric value.  This results in the vector of size  n, where  n is \\nequal to the number of convolutional channels. Afterward, it is fed through a two -layer neural \\nnetwork, which outputs a vector of the same size. These  n values can now be used as weights on the \\noriginal fea tures maps, scaling each channel based on its importance.  \\n \\nQ6. Feature Pyramid Network (FPN)  \\nAnswer:  \\n \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fb72d587-2773-4329-adc3-ac7f45b86459', embedding=None, metadata={'page_label': '263', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  9 | 15 \\n \\nThe Bottom -Up Pathway  \\nThe bottom -up pathway is feedforward computation of  backbone  ConvNet. It is known as one  \\npyramid level is f or each stage. The o utput  of last layer of each st ep will be used as the reference set \\nof feature maps for enriching the top -down pathway by lateral connection.  \\nTop-Down Pathway and Lateral Connection  \\n\\uf0b7 The higher resolution features are upsampled spatially coarser, but semantically  stronger, \\nfeature maps from higher pyramid levels. More particularly , the spatial resolution \\nis upsampled by a factor of 2 using nearest neighbor for simplicity.  \\n\\uf0b7 Each lateral connection adds feature maps of the sa me spatial size from  the bottom -up \\npathway  and top-down pathway.  \\n\\uf0b7 Specifically,  the feature maps from the bottom -up pathway undergo  1×1 \\nconvolutions to reduce  channel dimensions.  \\n\\uf0b7 And feature maps fro m the bottom -up pathway and top-down pathway are merged \\nby element -wise addition.  \\nPrediction  in FPN  \\n\\uf0b7 Finally,  the 3×3 convolution is appended on  each merged map to generate  a final fea ture \\nmap, which is to reduce  the aliasing effect of upsampling.  This last set of feature maps is \\ncalled {P2, P3, P4, P5}, corresponding to {C2, C3, C4, C5} that are respecti vely of  same \\nspatial sizes.  \\n\\uf0b7 Because all levels of  pyramid use shared classifiers/regressors as in a traditional featur ed \\nimage pyramid,  feature dimension at output  d is fixed with  d = 256. Thus, all extr a \\nconvolutional layers have 256 channel outputs.  \\n \\n \\nQ7. DeepID -Net( Def-Pooling Layer)  \\nAnswer:  \\nA new  def-pooling  (deformable constrained pooling ) layer  is used to  model  the deformation of the \\nobject parts with geometric constraint s and penalt ies. That means, except detecting the whole  object \\ndirectly, it is also important  to identify  object parts , which can then assist in detecting  the whol e \\nobject.   ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='37531d13-507c-41d7-87f8-1a11a02028f5', embedding=None, metadata={'page_label': '264', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  10 | 15 \\n \\n \\nThe steps in  black  color are the  old stuff  that existed in R -CNN . The st ages in red color do not \\nappear  in R-CNN . \\n1.  Selective Search  \\n \\n\\uf0b7 First, color similarities, texture similarities, region s size, and region filling are used as  non-\\nobject -based segmentation . Therefore you  obtain  many small segmented areas  as shown \\nat the bottom left of the image above.  \\n\\uf0b7 Then, the bottom -up approach is used that  small segmented areas are merged to form the \\nlarger segment  areas.  \\n\\uf0b7 Thus,  about 2K  region s, proposals (bounding box candidates) are generated , as shown \\nin the above image.  \\n \\n2. Box Rejection  \\nR-CNN is used to  reject bounding boxes that are most likel y to be the background.  \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5baa164b-2e5d-40eb-b57d-712a821cbf54', embedding=None, metadata={'page_label': '265', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  11 | 15 \\n \\n3. Pre train  Using Object -Level Annotations  \\n \\nUsually, pretraining is on  image -level annotation . It is  not good when  an object is too small within \\nthe image  because the object should occupy a large area within the bounding box created by the \\nselective search.  \\nThus,  pretraining is on object -level annotation . And  the deep learning (DL)  model can be any \\nmodels  such as ZFNet, VGGNet , and GoogLeNet.  \\n4. Def -Pooling Layer  \\n                                \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='edb7c707-80a9-4358-bcf4-2ef02ca46016', embedding=None, metadata={'page_label': '266', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  12 | 15 \\n \\n \\n \\nFor the def -pooling path,  output from conv5, goes through the C onv layer, then goes through the def-\\npooling layer, and then ha s a max -pooling layer.  \\nIn simple terms,  the summation of ac multiplied by dc,n, is the 5×5 deformation penalty  in the figure \\nabove.  The penalty of placing object part from  assumed the central  position.  \\nBy training  the DeepID -Net, object parts of the object to be detected will give a high activation value \\nafter the def-pooling layer if they are closed to their anchor places. And this output will conne ct to \\n200-class scores for improvement.  \\n5. Context Modeling  \\nIn object detection task s in ILSVRC, there are  200 classes . And there is also the  classification \\ncompetition task in ILSVRC for classifying and localizing 1000 -class objects. The contents are more  \\ndiverse compared with the object detection task. Hence,  1000 -class scores, obtained by \\nclassification network, are used  to refine  200-class scores.  \\n \\n6. The Model Averaging - \\nMultiple models  are used  to increase the accuracy, and  the results from all models are averaged . \\nThis technique has been used since AlexNet , LeNet, and so on.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b4a02221-e4eb-4233-83b3-a3ab39832e48', embedding=None, metadata={'page_label': '267', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  13 | 15 \\n \\n \\n7. Bounding Box Regression  \\nBounding box regression is to  fine-tune the bounding box location , which has been used in R -CNN.  \\nQ8. What is FractalNet  Architecture ? \\nAnswer:  \\nIn2015, after the invention of  ResNet , with numerous champion won, there are plenty of researche rs \\nworking on how to improve the  ResNet , such as  Pre-Activation ResNet , RiR, RoR, Stochastic D epth, \\nand WRN . In this story, conversely, a non -residual -network approach,  FractalNet , is shortly \\nreviewed. When  VGGNet  is starting to degrade when it goes from 16 layers (VGG -16) to 19 layers \\n(VGG -19), FractalNet can go up to 40 layers or even 80 layers.  \\nArchitecture  \\n \\nIn the above picture : A Simple Fractal Expansion ( on Left), Recursively Stacking of Fractal \\nExpansion as One Block ( in the Middle), 5 Blocks Cascaded as FractalNet ( on the Right)  \\nFor the base case, f1(z) is  the convolutional layer:  \\n                         \\nAfter that,  recursive fractals are:  \\n               \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6ee29455-bf78-404e-8e4d-c16ee9475f2d', embedding=None, metadata={'page_label': '268', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  14 | 15 \\n \\nWhere C is a  number of columns as in the middle of the  above  figure. The number  of the \\nconvolutional layers at the deepest path within the  block will have 2^(C -1). In this case,  C=4, thereby,  \\na number of convolutional layers are 2³=8 layers.  \\nFor the  join layer  (green),  the element -wise mean  is computed. It is not concatenation or addition.  \\nWith five blocks ( B=5) cascaded as FractalNet at the  right of the figure, then the number of \\nconvolutional layers at the most profound  path within the whole network is B×2^(C -1), i.e. , 5×2³= 40 \\nlayers . \\nIn between 2 blocks, 2×2 max pooling is done to reduce the size of feature maps. Batch Norm and \\nReLU are used after each convolution.  \\n \\nQ9. What is the SSPNet architecture?  \\nAnswer:  \\nSPPNet has introduced the  new technique in CNN called  Spatial Pyramid Pooling (SPP)  at the \\ntransition of the convolutional layer and fully connected layer. This is a work from  Microsoft . \\n \\nConventionally, at the trans forma tion of the C onv layer and FC layer, there is one single pooling \\nlayer or even no pooling layer. In SPPNet, it suggests having  multiple pooling layers with different \\nscales . \\nIn the  figure,  3-level SPP  is used. Suppose  conv5 layer has 256 feature maps. Then at the SPP layer,  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='56d11cff-8318-461c-b6bd-9334a0457d8a', embedding=None, metadata={'page_label': '269', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  15 | 15 \\n \\n1. first, each feature map is  pooled to become one value ( which is grey) . Thus 256-d vector \\nis formed . \\n2. Then, each feature map is  pooled to have four values ( which is green) , and form the  4×256 -\\nd vector . \\n3. Similarly, each feature map is  pooled to have 16 values ( in blue) , and form the  16×256 -d \\nvector . \\n4. The above three  vectors are concatenated to form a 1 -d vector . \\n5. Finally, this  1-d vector is going into FC layers  as usual.  \\nWith SPP, you  don’t need to crop the image to a fixed size, like AlexNet, before going into CNN.  Any \\nimage sizes can be inputted.  \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='323998f7-a7c3-466e-9c3f-a89bd032d26a', embedding=None, metadata={'page_label': '270', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  1 | 16 \\n \\n  \\n \\n \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW  \\nPREPARATION  \\n   (30 Days of Interview Preparation)  \\n \\n# Day22  \\n \\n  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='39b3664a-0710-4522-afff-920ce701841c', embedding=None, metadata={'page_label': '271', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  2 | 16 \\n \\nQ1. Explain V -Net (Volumetric Convolution) Architecture  with related \\nto Biomedical Image Segmentation?  \\nAnswer:  \\nThere were several  medical data used in clinical practice consists of 3D volumes, such as MRI \\nvolumes illustrate prostate, while most approaches are only able to process 2D images. A 3D image \\nsegmentation based on a volumetric, fully convolutional neural network is proposed in this work.  \\n \\n                              Slices from MRI volumes depicting prostate  \\nProstate  segmentation nevertheless is the  crucial  task having clinical relevance both during diagnosis, \\nwhere the volume of the prostate needs to be assessed and during treatment planning, where the \\nestimate of the anatomical boundary needs to be accurate.  \\nArchitecture  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b0953814-c3ab-4932-99ec-385ab820e928', embedding=None, metadata={'page_label': '272', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  3 | 16 \\n \\n\\uf0b7 V-Net, justifies by its name, it is shown as V-shape . The  left part  of the network consists of \\na compression path , while on the right part decompresses  signal  until its original size is \\nreached.  \\n\\uf0b7 This is the same as U -Net, but with some difference.  \\n \\nOn Left  \\n\\uf0b7 The left side of the network is divided in to different stages that operate at various  resolutions. \\nEach stage comprises one to 3 convolutional layers.  \\n\\uf0b7 At each stage, a  residual function is learn ed. The inp ut of each stage is used in \\nconvolutional l ayers and processed through non-linearit ies and added to the output of  the last \\nconvolutional layer of that stage to enable learning a residual function. This V-net \\narchitecture ensures convergence compared with non -residual learning network s such as  U-\\nNet. \\n\\uf0b7 The convolutions  performed in each stage use  volumetric kernels  having the size of  5×5×5 \\nvoxels . (A voxel represents a value on a regula r grid in 3D -space. The term voxel is \\ncommonly used in 3D m uch 3D space , just like voxelization in a point cloud.)  \\n\\uf0b7 Along the compression path,  the resolution is reduced by convolution with 2×2×2 voxels \\nfull kernels applied with stride 2 . Thus, the size of the resulting feature maps is halved, \\nwith a similar purpose as pooling layers . And  number of feature channels doubles at \\neach stage  of the compression path of  V-Net. \\n\\uf0b7 Replacing pooling operations with convolutional ones helps to have a smaller memory \\nfootprint during training because  no switches mapping the output of pooling layers back to \\ntheir inputs are needed for back -propagation.  \\n\\uf0b7 Downsampling helps to increase the receptive field.  \\n\\uf0b7 PReLU  is used as a non-linearity activation function.  \\n \\nOn Right  Part  \\n\\uf0b7 The network e xtracts features and  expands spatial support of the lower resolution feature \\nmaps to gather and assemble the necessary information to output a two -channel volumetric \\nsegmentation.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e7ab0530-5005-4d39-8e47-e5e0f4607730', embedding=None, metadata={'page_label': '273', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  4 | 16 \\n \\n \\n\\uf0b7 At each stage, a  deconvolution  operation is employed to increase the size of the inputs \\nfollowed by one to three convolutional layers,  involving  half the number of 5×5×5 \\nkernels  appli ed in the previous layer.  \\n\\uf0b7 The r esidual function  is learn ed, similar to left part of the network.  \\n\\uf0b7 The 2  features maps computed by  a very last convolutional layer , having  1×1×1 kernel \\nsize and producing  outputs of the same size as  input volume.  \\n\\uf0b7 These two output feature maps are  the probabilistic segmentation of the foreground and \\nbackground regions by applying soft -max voxelwise . \\n \\nQ2. Highway Networks - Gating Function to highway  \\n \\nAnswer:  \\nIt is found that difficulties are  optimizing a very deep neural network. However, it’s still an open \\nproblem  with why it is difficult to optimize  a deep network. (it is  due to gradient vanishing problem.) \\nInspired by LSTM  (Long Short -Term Memory ), authors thereby  make use of gating function to \\nadaptively bypass or transform  the signal so that the network can go deeper.  The deep network \\nwith more than 1000 layers can also  be optimized.  \\nPlain Network  \\nBefore going into Highway Networks, Let us  start with plain network which consists of  L layers \\nwhere the  l-th layer (with omitting the symbol for the layer):  \\n                           \\nWhere  x is input,  WH is the weight,  H is the transform function followed by an activation function , \\nand y is the output. And for  i-th unit:  \\n                                               \\nWe compute the  yi and pass it to the next layer.  \\nHighway Network  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cb5b85ad-4624-4b63-a195-c51cdaf1e101', embedding=None, metadata={'page_label': '274', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  5 | 16 \\n \\n \\nIn a highway network, 2  non-linear transf orms  T and C are introduced:  \\n \\nwhere  T is Transform Gate , and C is the Carry Gate . \\nIn particular,  C = 1 - T: \\n \\nWe can have below conditions for specific  T values:  \\n \\nWhen  T=0, we pass input as output directly , which creates an information highway. That’s  why \\nit is called the Highway Network.  \\nWhen  T=1, we use  non-linear activated transformed input as output.  \\nHere, in contrast to the  i-th unit in plain network, the authors introduce the  block  concept. For  i-th \\nblock , there is a  block state  Hi(x), and transf orm gate output  Ti(x). And the corresponding  block \\noutput  yi: \\n \\nwhich is connected to the next layer.  \\n\\uf0b7 Formally,  T(x) is the sigmoid function : \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b7cc10a2-853d-483b-ab15-450e77da3042', embedding=None, metadata={'page_label': '275', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  6 | 16 \\n \\n \\nSigmoid function caps the output between 0 to 1. When the input has a too-small value, it becomes \\n0. When the input has a too -large amount , it becomes 1.  Therefore, by learning  WT and bT, a \\nnetwork can adaptively pass  H(x) or pass  x to the next layer.  \\nAnd the author claims that this helps to have the  simple initialization scheme for  WT which is \\nindependent of  nature of  H. \\nbT can be initialized with the  negative value (e.g. , -1, -3, etc.) such that the network is initially biased \\ntowards carry ing behavio ur. \\nLSTM inspires the above idea  as the authors mentioned.  \\nAnd SGD( Stochastic Gradient Descent)  did not stall for networks with more than 1000 \\nlayers.  However, the exact results ha ve not been provided.  \\nQ3. What is DetNAS: Neural Architecture Search(NAS) on Object \\nDetection?  \\n \\nAnswer:  \\nObject detection is one of the most fundamental computer vision (OpenCV)  tasks and has been widely \\nused in real -world applications. The performance of object detectors highly rel ies on features \\nextracted by backbones. However, most works on object detection  directly use networks designed \\nfor classification as a backbone  the feature extractors, e.g., ResNet . The architectures optimized on \\nimage clas sification can not guarantee  performance  on object detection. It is  known that there is an \\nessential gap between these two different tasks. Image classification  basically  focuses on ”What” \\nmain object of  the image is, while obj ect detection aims at finding ”Where” and ”W hat” each object \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1b082d20-ef90-4702-8bd5-0ee9c44af098', embedding=None, metadata={'page_label': '276', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  7 | 16 \\n \\ninstance in an image. There ha ve been little works focusing on backbone design for object detector, \\nexcept the hand -craft network, DetNet.  \\nNeura l architecture search (NAS) has achieved significan t progress in image classification  and \\nsemantic segmentation. The networks produced by search have reached or even surpa ssed the \\nperformance of the hand -crafted ones on th is task. But object detection has never been supported by \\nNAS before. So me NAS (Neural architecture search)  work directly applies  architecture searched on \\nCIFAR -10 classification on object detection.  \\n \\nIn this work, we present  the first effort towards learning a backbone network for  object detection \\ntasks. Unlike previous NAS works, our method does not involve any architect ure-level transfer . We \\npropose DetNAS to conduct neural architecture search directly on the target tasks. The quest s are  \\neven perform ed with precise ly the same settings to  the target task. Training a n objector detector \\nusually needs several days and GPUs, no matter using a pre-train-finetune scheme or training from \\nscratch. Thus, it is not affordable  to directly use reinforcement learning (RL) or evolution algorithm \\n(EA) to  search the architectures independently. To overcome this obstacle, we formulate  this problem \\ninto searching  the optimal path in the  large gra ph or supernet. In simple terms , DetNAS consists of \\nthree steps: (1) training a supernet that includes all sub -networks in search space; (2) searching for \\nthe sub -network with the highest performance on the validation set with EA; (3) retraining the \\nresult ing network and evaluating it on the test set.  \\nQ4.You have any idea about ECE (Emotion cause extraction) . \\n \\nAnswer:  \\nEmotion cause extraction (ECE) aims at extracting potential causes that lead to emotion expressions \\nin the text. The ECE task was first proposed and defined as a word -level sequence labeling problem \\nin Lee et  al. To solve the shortcoming of extracting cau ses at the word level,  Gui et  al. 2016  released \\na new corpus which has received much attention in the following study and become s a benchmark \\ndataset for ECE research.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='61cdb85c-21f0-41db-a130-2b5501a24779', embedding=None, metadata={'page_label': '277', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  8 | 16 \\n \\nBelow Fig.  Displays an example from this corpus, there are five clauses in a document. T he emot ion \\n“happy” is contained in fourth clause. We denote this clause as  an emotion clause , which refers to a \\nterm that include s emotions. It has two corresponding causes: “a policeman visited the old  man with \\nthe lost money” in  the second clause and , “told him t hat the thief was caught” in  the third clause. We \\nname  them as  cause clause , which refers to a term that contains causes.  \\n \\nIn this work, we propose a new task:  emotion -cause pair extraction (ECPE), which aims to extract \\nall potential pairs of emotio ns and corresponding causes in the document. In Above Fig,  we show the \\ndifference between the traditional ECE task and our new ECPE task. The goal of ECE is to extrac t \\nthe corresponding cause clause of the given emotion. In addition to a document as the input, ECE \\nneeds to provide annotated feeling  at first before cause extraction.  \\nIn contrast, the output of our ECPE task is a pair of emotion -cause, without the need of  providing \\nemotion anno tation in advance. From Above fig., e.g. , given the annotat ion of feeling : “happy ,” the \\ngoal of ECE is to track the two corresponding cause clauses: “a policeman visited the old man with \\nthe lost money” and “and told him that the thi ef was caught .” While in the ECPE task, the goal is to \\ndirectly extract all pairs of emotion clause and cause clause, including (“The old man was delighted ”, \\n“a policeman visited the old man with the lost money”) and (“The old man was pleased ”, “and told \\nhim that the thief was caught”), without providing the emotion annotation “happy”.  \\nTo address this new ECPE task, we propose a two -step framework. Step 1 converts the emotion -\\ncause pair extraction task to two individual sub -tasks (emotion extraction and cau se extraction \\nrespectively) via two kinds of multi -task learning networks, intending  to extract a set of emotion \\nclauses and a set of cause clauses. Step 2 performs emotion -cause pairing and filtering. We combine \\nall the elements of the two sets into pairs  and finally train a filter to eliminate the couple s that do not \\ncontain a causal relationship.  \\nQ5.What is DST (Dialogue state tracking) ? \\nAnswer:  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='34c22384-8f2a-4199-8ff2-0e1a133e1c93', embedding=None, metadata={'page_label': '278', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  9 | 16 \\n \\nDialogue state tracking (DST) is a core component in task -oriented dialogue systems, such as \\nrestaurant reservation s or ticket booking s. The goal of DST is to extract user goals expressed during \\nconversation and to encode them as a compact set of the dialogue states, i.e., a set of slots and their \\ncorresponding values. E.g., as shown in below fig. , (slot, va lue) pairs such as  (price, \\ncheap)  and (area, centre)  are extracted from  the conversation. Accurate DST performance is \\nimportant  for appropriate dialogue management, where user intention determines the next system \\naction and the content to query from the da tabases.  \\n \\n State tracking approaches are based on the assumption that ontology is defined in advance, where \\nall slots and their values are known. Having a predefined ontology can simplify DST into a \\nclassification problem and improve performance  (Henders on et  al., 2014b ; Mrkšić et al., 2017 ; \\nZhong et  al., 2018 ). However, there are two significant  drawbacks to this approach: 1) A full \\nontology is hard to obtain in advance  (Xu and Hu,  2018 ). In the industry, databases are usually \\nexposed through an external API only, which is owned and maintained by others. It is not feasible to \\ngain access to enumerate all the possible va lues for each slot. 2) Even if a full ontology exists, the \\nnumber of possible slot values could be significant  and variable. For example, a restaurant name or \\na train departure time can contain a large number of possible values. Therefore, many of the prev ious \\nworks that are based on neural classification models may not be applicable in real scenario s. \\nQ6.What is NMT(Neural machine translation)?  \\nAnswer:  \\nNMT stands for Neural machi ne translation , which is the use of neural network models to learn the  \\nstatistical model for machine translation.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c90f1c72-9ea5-4d8f-b162-eb1458b0ec05', embedding=None, metadata={'page_label': '279', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  10 | 16 \\n \\nThe key be nefit to the approach is that the single system can be trained directly on the source and \\ntarget text, no longer requiring the pipeline of specialized method s used in statistical (ML) machine \\nlearning.  \\nUnlike the traditional phrase -based translation sys tem which consists of many  sub-components that \\nare tuned separately, neural machine translation attempts to build and train a single, large neural \\nnetwork that reads a sentence and outputs a correct transla tion. \\nAs such, neural machine translation (NMT)  systems are said to be end -to-end systems as only one \\nmodel is required for the translation.  \\n \\nIn Encoder  \\nThe task  of the encoder is to provide the representation of a  input s entence.The i nput sentence is  a \\nsequence of words , for which we first consult  embedding matrix. Then, as in the primary  language \\nmodel described previously, we process these words with a recurrent neural network (RNN) . This \\nresults in hidden states that encode each word with its left cont ext, i.e., all the preceding words. To \\nalso get the right context, we also build a recurrent neural network (RNN)  that runs right -to-left, or , \\nfrom the end of the sentence to  beginning. Having two recurrent neural networks (RNN)  running in \\ntwo directions is known as the  bidirectional recurrent neural network (RNN) . \\nIn Decoder  \\nThe decoder is the  recurrent neural network (RNN) . It takes some representation of  input context \\n(more on that in the next section on the attention mechanism) and  previous hidden state and  the output \\nword prediction, and generates a new hidden decoder state and the  new output word prediction.  \\nIf you  use LSTMs for the encoder, then you  also use  LSTMs for the decoder. From hidden state. You  \\nnow predict the output word. Thi s prediction takes t he form of the  probability distribution over entire \\noutput vocabulary. If you  have a vocabulary of, say, 50,000 words, then the prediction is a 50,000 \\ndimensional vector, each element corresponding to the probability predicted for one word in the \\nvocabular y. \\nQ7. What is Character -Level models (CLM)?  \\nAnswer:  \\nIn English , there is strong empirical evidence that the character sequence that create  up proper nouns \\ntend to be distinct ive. Even divorced of context, human reader can predict that “hoekstenberger” i s an \\nentity, but “abstractually”  is not . Some NER research explores  use of character -level features \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c72ca866-32be-4a37-9d4a-83f6d82d7a11', embedding=None, metadata={'page_label': '280', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  11 | 16 \\n \\nincluding capitalization, prefixes and suffixes  Cucerzan and Yarowsky ; Ratinov and Roth ( 2009 ), \\nand character -level models (CLMs)  Klein et  al. (2003 ) to improve the performance of NER, but to \\ndate there has been no  systematic study isolating utility of CLMs in capturing the distinctions between \\nname and non -name tokens in English or across other languages.  \\n \\nWe conduct the  experimental assessment of the discriminative power of CLMs for a rang e of \\nlanguages: English, Arabic , Amharic , Bengali, Farsi, Hindi, Somali, an d Tagalog. These languages \\nuse the  variety of scripts and ortho graphic conventions (e.g , only three  use capitalization), come from \\ndifferent language families, and vary in their morpholo gical complexity. We represent the \\neffectiveness of CLMs (character -level models)  in distinguishing name tokens from non -name t okens, \\nas illustrated by the above Figure, which shows confusion in histograms from a CLM trained on \\nentity tokens. Our models use  individual tokens, but perform extremely well in spite of taking no \\naccount of the word context.  \\nWe then assess the utility of directly adding simple features based on this CLM (character -level \\nmodel)  implementation to an existing NER s ystem, and show that they have the  significant positive \\nimpact on performance across many of the languages we tried. By adding very simple CLM -based \\nfeatures to the system, our scores approach those of a state -of-the-art(SOTA)  NER system  Lample \\net al. (2016 ) across m ultiple languages, representing  both the un ique importance and  broad utility of \\nthis approach.  \\n \\nQ8.What is LexNLP package?  \\nAnsw er: \\nOver the last 2 decades, many high -quality, open -source packages for natural language \\nprocessing (NLP)  and machine learning (ML) have been released. Developers and researchers  can \\nquickly write applic ations in languages such as Python, Java, and R that stand on shoulders of \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='10419360-abdb-4ccf-af6b-9c81b55ee595', embedding=None, metadata={'page_label': '281', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  12 | 16 \\n \\ncomprehen sive, well -tested libraries such as  Stanford NLP (Manning et  al. (2014 )), OpenNLP \\n(ApacheOpenNLP ( 2018 )), NLTK (Bi rd et al. (2009 )), spaCy (Honnibal and Montani ( 2017 ), scikit -\\nlearn  library  (Buitinck et  al. (2013 ), Pedregosa et  al. (2011 )), and G ensim (Řehůřek and Sojka \\n(2010 )). Con sequently, for most of the domains,  rate o f research has increased and  cost of the \\napplication development has decreased.  \\nFor some specialized areas like  marketing and medicines , there are focused libr aries and \\norganizations like  BioMedICUS (Consortium ( 2018 )), RadLex (Langlotz ( 2006 )), and the Open \\nHealth Natural Language Processing (NLP)  Consortium. Law, however, has received substantially \\nless attention than others, despite its ubiquity, societal importance, and the specialized form. LexNLP \\nis designed to fill this gap by providing bot h tools and data for developers and researcher s to work \\nwith real legal and regulatory text, including statutes, regulations, the court opinions, briefs, contracts, \\nand the other legal work product s. \\nLaw is the  domain driven by language, logic, and the conceptual relationships, ripe for computation \\nand analysis (Ruhl et  al. (2017 )). However, in our experience, natural language processing (NLP)  and \\nmachine learning (ML)  have not been applied  as fruitfully or widely  in legal as o ne might hope. We \\nbelieve that the  key i mpediment to academic and com mercial application has been  lack of tools that \\nallow users to turn the real, unstructured legal document  into structured data objects. The G oal of \\nLexNLP is to make this task simple , whether for the analysis of statutes, regul ations, court opinions, \\nbriefs or the migration of legacy contracts to smart contract or distributed ledger systems.  \\n \\nQ9.Explain The Architecture of LeNet -5.  \\nAnswer:  \\nYann LeCun, Leon Bottou,  Yosuha  Bengi o and Patrick Haffner proposed the  neural network \\narchitecture for the handwritten and machine -printed character recognition in the 1990’s which they \\ncalled them LeNet -5. The architecture is straightforward and too simple to understand that’s why it \\nis mostly used as a first step for teach ing (CNN) Convolutional Neural Network . \\nArchitecture  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b81298da-5853-4aa1-8967-49e6d36f9721', embedding=None, metadata={'page_label': '282', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  13 | 16 \\n \\nThis architecture consists of two sets of convolutional and average pooling layers, fo llowed by the \\nflatteni ng convolutional layer, then 2  fully-connected layers and finally the  softmax classifier.  \\nIn the First  Layer:  \\nThe input for LeNet -5 is the  32×32 grayscal e image which passes through  first convolutional layer \\nwith 6 feature maps or filters having size 5×5 and the stride of one. I mage dimensions changes from \\n32x32x1 to 28x28x6.  \\n \\nIn Second Layer:  \\nThen it  applies average pooling la yer or sub -sampling layer with the  filter size 2×2 and  stride  of two. \\nThe resulting image dimension  will be reduced to 14x14x6.  \\n \\nThird Layer:  \\nNext, there is the  second convolutional layer with 16 fea ture maps having size 5×5 and the stride of \\n1. In this layer, only ten out of sixteen  feature maps are co nnected to 6 feature maps of  previous layer \\nas shown below.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5f6836c5-276b-4d06-963d-fdbdcd85b83c', embedding=None, metadata={'page_label': '283', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  14 | 16 \\n \\n \\nThe main reason is to break  symme try in the network and keeps a  number of connections  within \\nreasonable bounds. That i s why the number of training parameters in this layers are 1516 in stead of \\n2400 and similarly,  number of connections are 151600 instead of 240000.  \\n \\nFourt h Layer:  \\nIn the fourth layer (S4) is an average pooling layer with  filter size 2×2 and stride of 2. This layer is \\nsame as  second layer (S2) exce pt it has 16 feature maps so  output will be reduced to 5x5x16.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='38541838-e620-456d-bc39-59ced5c48e0a', embedding=None, metadata={'page_label': '284', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  15 | 16 \\n \\n \\nFifth Layer:  \\nThe fifth layer (C5) is the fully connected convolutional layer with 120 feature maps each of the size \\n1×1. Each of  120 units in C5 is connected to all the 400 nodes (5x5x16) in the fourth layer S4.  \\n \\nSixth Layer:  \\nThe sixth layer is a lso fully connected layer (F6) with 84 units.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b7afa9a7-deb1-4886-baff-3991cade2698', embedding=None, metadata={'page_label': '285', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  16 | 16 \\n \\n \\nOutput Layer:  \\nFinally, there  is fully connected softmax output layer ŷ with 10 possi ble values corresponding to \\ndigits from 0 to 9.  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a68bdc82-c55a-4626-aac0-2b2faa179782', embedding=None, metadata={'page_label': '286', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 1 of 18 \\n \\n \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW PREPARATION  \\n(30 Days of Interview  \\nPreparation)  \\n \\n# DAY 23  \\n  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='56ea87a3-96a3-4c30-a20f-0d9fd38ac05f', embedding=None, metadata={'page_label': '287', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 2 of 18 \\n \\nQ1.Explain Overfeat in Object detection.  \\nAnswer:  \\nOverfeat : It is a typical model of integrating  object detection, localizat ion, and classification tasks whole  \\ninto one convolutional neural network (CNN) . The main idea is to do image classification at different \\nlocations on regions of multiple scales of the image in a sliding window fashion, and second, predict \\nbounding box location s with the regressor trained on top of the same convolution layers.  \\nThis model arc hitecture is too  similar to  Alex Net. This model  is trained as follows:  \\n \\n1. Train a CNN model ( identical to AlexNet) on  image classification task s. \\n2. Then, we replace  top classifier layers by the  regression network and train ed it to predict object \\nbounding boxes at each  spatial location and scale. R egressor is class -specific, each generated for \\none class image . \\n• Input: Images with classification and bounding box.  \\n• Output:  (xleft,xright,ytop,ybottom)(xleft,xright,ytop,ybottom) , 4  values in total, \\nrepresenting the coordinates of the bounding box edges.  \\n• Loss: The regressor is trained to minimize  l2 norm between the generated bounding box \\nand truth for each training example.  \\nAt the detection time,  \\n1. It Perform s classification at each l ocation using the pretrained CNN model.  \\n2. It Predict s object bounding boxes on all classified regions generated by the classifier.  \\n3. Merge bounding boxes with sufficient overlap from localization and sufficient confidence of being \\nthe same object from the classifier.  \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a1c4f66f-2502-4d17-b549-293b59e1f394', embedding=None, metadata={'page_label': '288', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 3 of 18 \\n \\nQ2. What is Multipath : Multiple Probabilistic Anchor Trajectory \\nHypotheses for Behavior Prediction?  \\nAnswer:  \\nIn this paper, we focus on  problem of predicting  future agent states, which is the  crucial task for robot \\nplanning in real -world environments. We are specifically  interested in addressing this proble m for self -\\ndriving vehicles, application with a potentially enormous societal impact. Mainly , predicting the future \\nof other agents in this  domain is vital for safe, comfortable , and efficient operation. E.g., it is importa nt \\nto know whether to yield to the  vehicle if they are going to cut in front of our robot or when would be \\nthe best time to add into traffic. Such future prediction req uires an understanding of a  static and dynamic \\nworld context: road semantics ( like lane connectivity, stop lines), traffic light information s, and past \\nobservations of other agents, as in below Fig . \\nA fundamental aspect of the future state prediction is that it is inherently  stochastic , as agents can’t know \\neach other’s motivations. When we are driving, we can never really be sure what other drivers will do \\nnext, and it is essential  to consider multiple outcomes and their likelihood.  \\nWe seek the model of the future that can provide both (i ) a weighted, parsimonious set of discr ete \\ntrajectories that covers space of likely outcomes and (ii ) a closed -form evaluation of  the likelih ood of \\nany trajectory. These two  attributes enable efficient reasoning in releva nt planning use -cases, e.g., \\nhuman -like reactions to discrete trajectory hypotheses ( e.g., yielding, following), and probabilistic \\nqueries such as the expected risk of collision in a space -time region.  \\nThis model addresses these is sues with critical  insigh t: it employs a fixed set of  trajectory anchors  as the \\nbasis of our modeling. This lets us factor stochastic uncertainty hierarchically: First,  intent \\nuncertainty  captures the uncertainty of  what  an agent intends to do and is encoded as a distribution over  \\nthe set of anchor trajectories. Second, given an intent,  control uncertainty  represents  our uncertainty \\nover how they might achieve it. We assume control uncertainty is normally distributed at each future \\ntime step  [Thrun05 ], parameterized such that the mean corresponds to a context -specific offset from the \\nanchor state, with the associated covariance capturing the unimodal aleatoric uncertainty  [Kendall17 ]. In \\nFig. Illustrates a typical scenario where there are three  likely intents given the scene co ntext, with control \\nmean o ffset refinements respecting  road geometry, and control uncertainty intuitively growing over time.  \\nOur trajectory anchors are modes found in our training data in state -sequence space via unsupervised \\nlearning. These anchors provide templates for coarse -granularity futures for an agent and might \\ncorrespond to semantic concepts like “change lanes ,” or “slow down” (although to be clear, we don’t use \\nany semantic concepts in our modeling).  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1f89ac89-5d0f-468b-9d66-e317f42c19cc', embedding=None, metadata={'page_label': '289', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 4 of 18 \\n \\nOur complete model predicts a Gaussian m ixture model (GMM) at each time step, with the mixture \\nweights (intent distribution) fixed over time. Given such a parametric distribution model, we can directly \\nevaluate the likelihood of any future trajectory and have a simple way to obtain a compact, di verse \\nweighted set of trajectory samples: the MAP sample from each anchor -intent.  \\n \\nQ3. An Object detection approach using MR -CNN  \\nAnswer:  \\nMulti -Region CNN (MR -CNN) : Object representation using multiple regions to capture several diffe rent \\naspects of one  object.  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d0bc0bcc-5d05-4afc-98b1-81f1fb78a9f3', embedding=None, metadata={'page_label': '290', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 5 of 18 \\n \\nNetwork Architecture  of MR -CNN  \\n• First, the input image goes through  Activation Maps Module, as shown above, and outputs the \\nactivation map . \\n• Bounding box  or Region proposals candidates are generated  using Selective Search.  \\n• For each bounding box candidate  B, a set of regions { Ri}, with  i=1 to  k, are generated, that is  why \\nit is known as  multi-region. More details about  the choices of multi ple areas are described in  next \\nsub-section.  \\n• ROI pooling is performed for each region  Ri,cropped  or pooled  area goes through  fully connected \\n(FC) layers, at each Region Adaptation Module.  \\n• Finally, the output  from all FC layers are added  together to form a 1D feature vector , which is an \\nobject representation of the bounding box  B. \\n• Here,  VGG -16 ImageNet pre -trained model is used. The ma x-pooling layer after the last conv \\nlayer is removed.  \\nQ4. Object detection using Segmentation -aware CNN  \\nAnswer:  \\n             \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='004f9f22-a3fb-440e-9760-8a83acae508a', embedding=None, metadata={'page_label': '291', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 6 of 18 \\n \\n \\n• There are close connection s between segmentation and detectio n. And segmentation \\nrelated ques  are empirically known to help object detection ofte n. \\n• Two modules are added:  Activation maps module for semantic segmentation -aware \\nfeatures , and  region s adaptation module for grammarly segmentation -aware \\nfeature . \\n• There is no additional annotation used fo r training here.  \\n• FCN  is used for  an activation map module.  \\n• The last FC7 layer channels number is changed from 4096 to 512.  \\n \\n• The w eakly supervised training  strategy is used.  Artificial foreground c lass-specific \\nsegmentation mask is  created using bounding box annotations . \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e6416b94-8f58-496d-ad77-ffee020ca6fd', embedding=None, metadata={'page_label': '292', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 7 of 18 \\n \\n• More particularly , the ground truth bounding boxes of an image are projected on the spatial \\ndomain of the last hidden la yer of the  FCN , and the ”pixels” that lay inside the projected boxes \\nare labelled as foreground while the rest are labelled as background.  \\n• After traini ng the  FCN  using the mask, the last classification layer is dropped. Only the rest \\nof FCN  is used.  \\n• Though it is weakly supervised training, the foreground probabilities shown as above still carry \\nsome information, as shown above.  \\n• The bound ing box used is 1.5× larger than the original bounding box.  \\n \\nQ5. What is CRAFT ( Object detection)?  \\nAnswer:  \\nCRAFT stands for C ascade  Region -proposal -network  And FasT R-CNN . It is reviewed by the \\nChinese Academy of Sciences  and Tsinghua University.  In Faster R -CNN , region proposal \\nnetwork  is used to generate proposals. These proposals, after ROI pooling, are going through \\nnetwork for classification. Howe ver, CRAFT is found that there is a  core problem in  Faster \\nR-CNN : \\n• In proposal generation, there is still a large proportion of background regions. The exis tence of many \\nbackground sample causes many false positives . ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7c8be58d-b737-4e78-af31-6b11b94575e7', embedding=None, metadata={'page_label': '293', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 8 of 18 \\n \\n \\nIn CRAFT (Cascade  Region -proposal -network ), as shown above, another CNN (Convolutional neural \\nnetwork)  is added after RPN to  generate fewer proposals (i.e. , 300 here) . Then, clas sification is performed \\non 300 proposals and outputs about 20 first detection results. For each primitive result , refined object \\ndetection  is performed using one -vs-rest classification.   \\nCascade Proposal Generation  \\n   Baseline RPN  \\n• An ideal proposal generator should generate as few proposal  as possible while covering almost all \\nobject instances. Due to  resolution loss caused by CNN pooling operation and the fixed aspect ratio \\nof the sliding window, RPN is weak at covering objects with extreme  shapes  or scales . \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='49c9e87b-a3a5-4b77-8302-bef8ff428c21', embedding=None, metadata={'page_label': '294', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 9 of 18 \\n \\n \\n     Recall Rates ( is in %), Overall is 94.87%, lower than 94.87% is bold in the text. \\n• The above results are  baseline RPN based on  VGG_M  trained using PASCAL VOC \\n2007 train+val, and tested on the test set.  \\n• The recall rate on each object category varies a lot. Object  with extreme aspect ratio and scale are \\nhard to be detected, such as boat and bottle.  \\n \\n \\n \\n \\n \\n \\nProposed Cascade Structure  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='583e31b9-ca3e-48ec-8a28-e62bf9e5d8d4', embedding=None, metadata={'page_label': '295', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 10 of 18 \\n \\n \\n          The concatenation  classification network after RPN is denoted as FRCN Net here  \\n• An additional classification network that comes after  RPN.  \\n• The additional network is the  2- class detection network  denoted as FRCN net in above figure. It \\nuses output of RPN as training data.  \\n• After RPN net is trained, the 2000 first proposals of each training image are used as training data \\nfor the FRCN net.  \\n• During training, +ve and -ve sampling are based on 0.7 IoU for negati ves and below 0.3 IoU for \\nnegatives , respectively.  \\n• There are 2 advanta ges: \\n• 1) First,  additional FRCN net further  improves  quality of the object proposals  and shrinks \\nmore background regions , making proposals fit better with  task requirement.  \\n• 2) Second,  proposals from multiple sources can be merged  as the input of the FRCN net so that \\ncomplementary information can be used.  \\n \\n \\n \\n \\nQ6. Explain YOLOv1 for Object Detection.  \\nAnswer:  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='18fc03c9-e4d4-42ed-b667-73d8022ac78c', embedding=None, metadata={'page_label': '296', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 11 of 18 \\n \\nYOLOv1 stands for You Look Only Once, it is reviewed by FAIR (Facebook AI Research). The network \\nonly looks  at the image once to detect multiple objects.  \\n \\nBy just looking  image once,  the detection speed is in real -time (45 fps) . Fast YOLOv1 achieves 155 fps.   \\nYOLO suggests having  a unified network to perform all a t once. Also, an end-to-end training network can \\nbe achieved.  \\n \\nThe input image is divided into  the S×S grid (S=7) . If the center of the object falls into  the grid cell, that \\ngrid cell is responsible for detecting that object.  \\nEach grid cell predict  B bounding boxes (B=2) and confidence scores for those boxes . These confidence \\nscore reflect how confident  model is that the box contains an object,  i.e., any objects in the box, P(Objects).  \\nEach bounding box consists of five  predictions: x, y, w, h, and confidence.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0897137c-01a3-4bd7-9e62-c8032b589f65', embedding=None, metadata={'page_label': '297', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 12 of 18 \\n \\n• The (x, y) coordinates represent  center o f the box relative to the bound  of the grid cell.  \\n• The height h and width w  are predicted relative to  whole image.  \\n• The confidence represents the IOU (Intersection Over Union ) between the predicted box and any \\nground truth box.  \\nEach grid cell also predicts condition al class probabilities, P(Class |Object). (Total number of classes=20)  \\n \\nThe output size becomes: 7×7×(2×5+20)=1470  \\n \\n \\n \\nNetwork Architecture of YOLOv1  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b4553229-4333-4773-9bc5-0a46a69dae22', embedding=None, metadata={'page_label': '298', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 13 of 18 \\n \\n \\nThe model consists of 24 convolutional layers , followed by two fully connected layers.  Alternating 1×1  \\nconvolutional layers reduce  features sp ace from preceding layers. (1×1 )Conv has been used in GoogLeNet \\nfor reducing the number of parameters.)  \\nFast YOLO fewer convolutional layers (9 instead of 24) and fewer filters  in those layers.  The network \\npipeline is summarized like below:  \\n \\nTherefore, we can see that the input image goes through network once and then objects can be detected. \\nAnd we can have  end-to-end learning . \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3ec03c4e-9948-46b8-8c6c-8aae95f1df17', embedding=None, metadata={'page_label': '299', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 14 of 18 \\n \\nQ7. Adversarial Examples Improve Image Recognition  \\nAnswer:  \\nAdversarial examples crafted by adding imperceptible perturbations to images can lead  to \\n(ConvNets )Convolutional Neural Networks  to make wrong predictions. The existence of adversari al \\nexamples not only reveal  limited generalization ability of ConvNets, but also poses security threats on \\nthe real -world deployment of these models. Since the first discovery of the vulnerability of Co nvNets to \\nadversarial attacks, many efforts  have been made to improve network robustness.  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='62f87b05-ef05-428d-b0fa-75d8bf58574c', embedding=None, metadata={'page_label': '300', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 15 of 18 \\n \\nAbove Fig. : AdvProp improves image recognition . By training model  on ImageNet, AdvProp helps \\nEfficientNet -B7 to ach ieve 85.2% accuracy on ImageNet , 52.9% mCE (mean corruption error, lo wer is \\nbetter) on ImageNet -C, 44.7% accuracy on ImageNet -A and 26.6 % accuracy on Stylized -ImageNet , \\nbeating its vanilla counterpart by 0. 7%, 6.5%, 7.0% and 4.8%, respectively. Theses sample image s are \\nrandomly selected from  category “goldfinch .” \\nIn this paper, rather than focusing on defending against adversarial examples, we shift our attention to \\nleveraging adversarial examples to improve  accuracy. Previous works show that training with adversarial \\nexamples can enhance model generalization but are restricted to certain situations —the improvement is \\nonly observed either on small datasets ( e.g., MNIST) in the fully -supervised setting  [5], or on larger \\ndatasets but in the semi -supervised setting  [21, 22]. Meanwhile, recent works  [15, 13, 31] also suggest \\nthat training with adversarial examples on large datasets,  e.g., ImageNet  [23], with supervised learning \\nresults in performance degradation on clean images. To summarize, it remains an open question of how \\nadversarial examples can be used effectively to help vision models.  \\nWe observe all previous  methods jointly train over clean images and adversarial examples without \\ndistinction , even though they should be drawn from different underlying distributions. We hypothesize \\nthis distribution mismatch between fresh  examples and adversarial examples is a key factor that causes \\nperformance degradation in previous works . \\nQ8. Advancing NLP with Cognitive Language Processing Signals  \\nAnswer:  \\nWhen reading, humans process language “automatically” without reflecting on each step \\u2009—\\u2009Humans \\nstring words together into sentences, understand the meaning of spoken and written ideas, and process \\nlanguage without overthinking  about how the underlying cognitive process happens. This process \\ngenerates cognitive signals that could potentially f acilitate natural language processing tasks.  \\nIn recent years, collecting these signals has become increasingly accessible  and less \\nexpensive  Papoutsaki et  al. (2016 ); as a result, using cognitive features to improve NLP tasks has become \\nmore popular. For example, researchers have proposed a rang e of work that uses eye -tracking or gaze \\nsignals to improve part -of-speech tagging  (Barrett et  al., 2016 ), sentiment analysis  (Mishra e t al., 2017 ), \\nnamed entity recognition  Hollenstein and Zhang ( 2019 ), among other tasks. Moreover, these signals have \\nbeen used successfully to regularize attention in neural networks for NLP  Barrett et  al. (2018 ). \\nHowever, most previous work leverages only eye -tracking data, presumably because it is the most \\naccessible form of cognitive language processing signal. Also , most state -of-the-artwork(SOTA) focused \\non improving a single task with a single type of cognitiv e signal. But  can cognitive processing signal s \\nbring consistent improvements across modality (e.g., eye -tracking and EEG) and across various NLP ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9167a16d-97e1-484c-a16f-acadea514c70', embedding=None, metadata={'page_label': '301', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 16 of 18 \\n \\ntasks?  And if so,  does the combination of different sources of cognitive signals bring incremental \\nimprovements?  \\n \\nQ8. Do you have any idea how can we use NLP on News headlines to \\npredict index trends?  \\nAnswer:  \\nTraders generally look up information about the company they are looking to buy shares into, for long \\nand short trading. A frequent source of information is news media, which provide s updates about the \\ncompany’s activi ties, such as expansion, better or worse revenues than expected, new products and much \\nmore . Depending on the news, trader can determine a bearish or bullish trend and decide to invest s in it. \\nWe may be able to correlate overall public sentiment s towards as the company and its stock price: Apple \\nis generally well -liked by the public, receives daily  news coverage of its new product  and financial \\nstability , and its stock has been growing steadily. These facts may be correlated but first may not cause \\nthe second ; we will analyze if news coverage can be used to predict the market trend. To do so, we wil l \\nexamin e the top 25 news headline s of each open -market day from 2008 to late 2015 and try to predict \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8994b18b-c7d4-4c96-bf89-4a83a0dc0b8e', embedding=None, metadata={'page_label': '302', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 17 of 18 \\n \\nthe end -of-day value of  DJIA index for the same day. The theory be hind predicting same day value is \\nthat traders will resp ond to news quickly and thus,  the market will adjust within an hour of release. \\nTherefore in the  single business day, if the news is spread du ring business hours, its effect  may be \\nmeasur ed before  closing bell of the market.  \\nThe motivation behind this analys is is that humans take decisio n using most of the available information. \\nThis usually takes several minutes to dis cover new information and take the  decision. An algorithm is \\ncapable of processing gigabytes of text s from  multi -source streams in second . We could potentially \\nexploit this  difference in order to create  a trading strategy.  \\nNLP (Natural Language Processing ) techniques can be used to extract different information from \\nheadlines such as sentiment , subjectivity, context and named entiti es. We obtain an indicator vector  using \\neach of these techniques, which allow s us to train diff erent algorithms to predict a trend. To predict these \\nvalues, we can use several method s that should be well suited for this type of information: Linear \\nregression, Support Vector Machine (SVM) , Long Short -Term Memory (LSTM)  recurrent neural \\nnetwork , and a dense feed -forward (MLP)  neural network. We included  techniques used by Bollen et al \\n(2010), which resulted in state -of-the-art(SOTA)  results. We will also analyze the method  used in other \\nstudie s with a similar context  \\n Information in headlines  \\nLatent Sentiment Analysis is done by building up a corpus of labeled words which usually connote a \\ndegree of +ve or -ve sentiment. We can extend the corpus to include emotico ns (i.e. “: -)” ) and expression , \\nwhich often correlate s to strong emotions. Naive sentiment analysis consists  of a lookup of each word in \\nsentence to be analyzed and evaluation of a score for  sentence overall. This approach is limited by its \\nknown vocabulary, which can be mitigated by context an alysis and introduction of synonyms.S econd \\nlimitation is sarcasm, which is prevalent in twitter feed analysi s. The sentiment inferred by  words is \\noppos ed to the sentiment assum ed by the user.  This is mitigated by technique  detecting sarcasm which \\nlead to a polarity flip of such tweets.  \\nSentiment analysis gives insight on how favorable the media is and maybe the bias traders may have \\ntowards buying or se lling.  \\nAnother NLP technique which gave promising results was context analysis. This is a recent deep learning \\napproach where you rely on a large corpus of text in order to learn and predict the words around a target. \\nYou can then deduce in what context it  usually appears. The result is a vector representing each word. \\nOther vectors with little distance are usually synonyms. The representation also allows us to do algebra, \\nsuch as the famous “king - man + woman = queen”  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f5a33e64-0048-4960-99ed-aaf4c86beb2c', embedding=None, metadata={'page_label': '303', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 18 of 18 \\n \\nLearning this representation offers t he possibility of associating a specific context with a bullish or \\nbearish market.  \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ee2dbb34-170d-4254-ae3b-ac5ba98c0ab4', embedding=None, metadata={'page_label': '304', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  1 | 14 \\n \\n \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW  \\nPREPARATION  \\n(30 Days of Interview Preparation)  \\n \\n# Day24  \\n  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2369ec64-6640-487c-85ab-2e14eea709d8', embedding=None, metadata={'page_label': '305', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  2 | 14 \\n \\nQ1.What is STN?  \\nAnswer:  \\nSTN stands for Spatial Transformer Network for image classification. Google Deepmind briefly reviews \\nit. STN helps to c rop out and scale -normalizes  appropriate region, which can simplify the subsequent \\nclassification task and lead to better classification performance as below:  \\n \\n                   \\n(a) Input Image with Random Translation, Scale, Rotation, and Clutter, (b) STN Applied to \\nInput Image, (c) Output of STN, (d) Classification Prediction  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cf7a8c15-46d1-4bc8-ad6c-16797df5d194', embedding=None, metadata={'page_label': '306', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  3 | 14 \\n \\n \\nSpatial Transformer Network (STN)  \\n \\n \\n \\n\\uf0b7 STN is composed of  Localisation Net , Grid Generator , and Sampler . \\n \\nLocali zation Net  \\n\\uf0b7 With  input feature map  U, with (width  )W, (height ) H, and C channels,  outputs are  θ, \\nparameters of transformation  Tθ. It can be learned  as affine transform as above. Or to be more \\nconstrained , such as the used for attention which only contains scaling and translation as below:  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6b8e1008-3115-4b96-baf6-f2acaf58cc93', embedding=None, metadata={'page_label': '307', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  4 | 14 \\n \\n \\nGrid Generator  \\n\\uf0b7 Suppose we have a regular grid G, this G is a set of points with  source coordinates (xs_i, ys_i) , \\nwhich act as  input . \\n\\uf0b7 Then we  apply transformation  Tθ on G, i.e., Tθ(G). \\n\\uf0b7 After  Tθ(G), a set of points with  destination coordinates (xt_i, yt_i) is outputted . These points \\nhave been altered  based on the transformation parameters. It can be Translation, Scale, Rotation \\nor More Generic Warping depending on how we set  θ as mentioned above.  \\n \\nSampler  \\n \\n\\uf0b7 Based on the new set of coordinates (xt_i, yt_i), we generate a transformed output feature \\nmap  V. This  V is translated, scaled, rotated, warped, projective transformed or affined, whatever.  \\n\\uf0b7 It is noted that STN can be applied to not only input image but also intermediate feature maps.  \\nQ2.What is decaNLP?  \\nAnswer:  \\nWe introduce d the Natural Language Decathlon  (decaNLP) to explore models that generalize to many \\ndifferent kinds of Natural Language Processing( NLP ) tasks. decaNLP  encourages single model to \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5df471d1-8f56-4165-b276-ffe09253fd2f', embedding=None, metadata={'page_label': '308', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  5 | 14 \\n \\nsimultaneously optimize for 10 tasks: question answering, machine translation,  document \\nsummarization, semantic parsing, sentiment analysis, natural language inference (NLI) , semantic role \\nlabeling, relation extraction, goal -oriented dialogue, and pronoun resolution.  \\nWe frame all the tasks as question answering  [Kumar et  al., 2016 ] by allowin g task specification to take  \\nthe form of a natural language question  q: all inputs have a context, quest ion, and answer (Fig.  1). \\nTraditionally, NLP examples have inputs  x and output  y, and the underlying task  t is provided through \\nexplicit modeling constraints. Meta -learning approaches include  t as additional in put. Our approach \\ndoes not use the  single representation for any  t but instead uses natural language questions that  describe  \\nunderlying tasks. This allows single models to multitask effectively  and makes them more suitable as \\npre-trained models for transfer learning and meta -learning: natu ral language questions allow a model to \\ngeneralize to entirely new tasks through different but related task descriptions.  \\nThe MQAN ( multit ask question answering network) is designed for decaNLP  and makes use of a novel \\ndual attention and multi -pointer -gene rator decoder to multitask across all tasks in decaNLP. Our results \\nrepresent  that training the MQAN  jointly on all tasks with the right anti -curriculum strategy can achieve \\nperformance comparable to that of ten separate MQANs, each trained separately. A n MQAN  pretrained \\non decaNLP  shows improvements in transfer learning for machine translation and named entity \\nrecognition (NER) , domain adaptation for sentiment analysis and natural language inference (NLI) , and \\nzero-shot capabilities for text classification. Though not explicitly designed for any one job, \\nMQAN  proves to be a robust model in  a single -task setting as well, achieving state -of-the-art results on \\nthe semantic parsing component of decaNLP.  \\nIn the above figure : Overview of the decaNLP  dataset with one examp le from each decaNLP  task in  the \\norder presented in Section  2. They show how the datasets were pre -processed to become question \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0c08555a-cfa4-4f52-83c8-5ffb262d4e50', embedding=None, metadata={'page_label': '309', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  6 | 14 \\n \\nanswering problems. Answer words in red are generated by pointing to the context, in green from the \\nissue , and in blue if they are  made  from a classifier over the output vocabulary.  \\nQ3.Universal Transformers  \\nAnswer:  \\nConvolutional and fully -attentional feed -forward architectures such as  the Transformer model have \\nrecently emerged as viable alternatives to RNNs(R ecurrent neural networks ) (for the  range of sequence \\nmodeling tasks, notably machine translation  (JonasFaceNet2017 ; transformer,  ). These architectures \\naddress the  significant shortcoming of RNNs, namely their inherently sequential computation , which \\nprevents parall elization across elements of  input seque nce whil e still addressing  vanishing gradients \\nproblem  (vanishing -exploding -gradient ). Transformer model , in particular,  achiev es this by relying \\nentirely on  the self-attention mechanism  (decomposableAttnModel ; lin2017structured ) to compute  \\nseries of context -informed vec tor-space representations of  symbols in its input and output, which are \\nthen used to predict distributions over subsequent s ymbols as the model predicts  output sequence \\nsymbol -by-symbol . Not only in  this mechanism straightfor ward to parallelize , but as each symbol’s \\nrepresentation is also directl y informed by all other symbols  representations, this results in an active \\nglobal receptive field. This stands , in contrast,  to, e.g., convolutional arch itecture,  which typically ha s \\nlimited receptive field.  \\nNotably, however,  Transformer foregoes the (Recurrent Neural Network) RNN’s inductive bias towards \\nlearning  recursive  or iterative  transformations. Our experiments indicate that this inductive bias may be \\nimportant for several algorithmic and language understanding tasks of varying complexity: in contrast \\nto models su ch as the Neural Turing Machine, the Neural GPU, or Stack RNNs, the Transformer does \\nnot generalize well to input lengths not encountered during training.  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='16d4bc89-8fa1-4afb-afc7-cd913a76ce4d', embedding=None, metadata={'page_label': '310', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  7 | 14 \\n \\nIn this  paper, we propose a Universal Transformer . It combines the parallelizability a nd global receptive \\nfield of  a Transformer model with the recurrent inductive bias of RNNs , whic h seems to be better suited \\nto range of algorithmic and natural language understanding (NLU)  sequence -to-sequence problems. As \\nthe name implies, in contrast to  standard Transforme r, under certain assumptions , a Universal \\nTransformer can be shown to be com putationally universal . \\nIn each step, the Universal Transformer iteratively  refine its represent ations for all positions in sequence \\nin parallel with  self-attention mechanism  decomposableAttnModel () ; lin2017structured () , followed by \\nthe recurrent transformation consisting of a depth -wise separable convolution  (xception2016  ) or a \\nposition -wise fully -connected layer (see above Fig ). We also extend ed the Unive rsal Transformer by \\nemploying an adaptive computation time m echanism at each position in  \\nsequence  (graves2016adaptive ), allowing  model to choose the required number of refinement steps for \\neach symbol dynamically.  \\nWhen running for  fixed number of steps, the Universa l Transformer is equivalent to  a multi -layer \\nTransformer w ith a tied parameter  across its layers. However, another, and possibly more inform ative, \\nway of characterizing Universal Transformer is as recurrent function evolving per -symbol hidden states \\nin pa rallel, based at each step on  a sequence of the previous un known state . In this way, it is simi lar to \\narchitectures such as  Neural GPU  and the Neural Turing Machine . The Universal Transformer thereby \\nretains the attractive computational efficiency of original feed -forward Transformer model, but with an \\nadded recurrent inductive bias of RNNs . In its adaptive form, we  show that the Universal Transformer \\ncan effectively interpolate between the feed -forward, fixed -depth Transformer , and a gated, recurrent \\narchitecture running for several  steps depend ing on the inp ut data.  \\nOur experimental results s how that its recurrence improve  results in machine translation, where Universal \\nTransformer outperforms t he standard Transformer with  a same no.of parameters. In experiments o n \\nseveral algorithmic tasks,  Universal Transfo rmer consistently i mproves significantly over LSTM(Long \\nShort Term Memory) RNNs and the standard T ransformer. Furthermore, on bAbI and LAMBADA text \\nunderstanding data sets, the Universal Transformer achieves a new state of the art.  \\nQ4. What is StarSpace in  NLP?  \\nAnswer:  \\nWe intro duce StarSpace, the  neural embedding model that is general enough to solve a wide variety of \\nproblems:  \\n\\uf0b7 Other labeling tasks , or Text classification , e.g. , sentiment classification.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='75f2c5a4-3984-4675-a0f9-adaaac198e22', embedding=None, metadata={'page_label': '311', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  8 | 14 \\n \\n\\uf0b7 Ranking of the set  of entities, e.g. , a classification  of web documents given a query.  \\n\\uf0b7 Collaborative filtering -based recommendation, e.g. , recommending documents, \\nvideos or music . \\n\\uf0b7 Content -based recommendation where content is defined with discrete features, e.g. , \\nwords of documents.  \\n\\uf0b7 Embedding graphs, e.g. , multi -relational graphs such as Freebase.  \\n\\uf0b7 Learning word, sentence , or document embeddings.  \\nIt can be viewed as a straight -forward and efficient strong baseline for an y of these \\ntasks. In experiment,  it is shown to be on par with or outperform ing several \\ncompeting methods whil e being generally applicable to c ases where many of that \\nmethod  are not.  \\nThe method works by learning entity embeddings with discrete feature \\nrepresentation  from relations among collections of those entities directly for t he task \\nof ranking or classification of interest. In the general case, StarSpace em beds objects \\nof different types  into a vectorial embedding space ; hence , the “star” (“* ,” meaning \\nall types) and “space” in  a name and in that familiar  space compares them a gainst  \\neach other. It learns to rank the  set of entities, documents , or objects given a query \\nentity, document , or object, where the query is not necessarily of the same type as \\nthe items in the set.  \\nQ5. TransferTransfo in NLP  \\nAnswer:  \\nNon-goal-oriented dialogue systems (chatbots) are interesting test -bed for interactive Natural \\nLanguage Processing (NLP) systems a nd are also directly useful in  wide range of applications ranging \\nfrom technical support services to e ntertainment. However, building intelligen t conversational agent \\nremains an unsolved problem in artificial intelligence (AI) research. Recently, recurrent neural \\nnetwork (RNN)  based models with sufficient capacity and acces s to large datasets attracted large \\ninterest when first attempted.  It showed that they were capable of generating meaningful responses \\nin some chit -chat settings. Still, further inquiries in the capabilities of these neural network ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6c8ad4bf-5012-4e02-b860-65f6fb5f2ff5', embedding=None, metadata={'page_label': '312', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  9 | 14 \\n \\narchitectures and developments  indicated that they were limited which made communicating with them a rather \\nunsatisfying experience for human beings.  \\nThe main issues with these architectures can be summarized as:  \\n\\uf0b7 (i) the wildly inconsistent outputs and the lack of a consistent personality  (Li and Jurafsky,  2016 ), \\n\\uf0b7 (ii) the absence of long -term memory as these models have difficulties in taking  into account \\nmore than the last dialogue utterance; and  \\n\\uf0b7 (iii) a tendency to produce consensual and generi c responses that  are vague and not engaging for \\nhumans  (Li, Monroe, and Jurafsky,  2016 ). \\nIn this work, we take a step toward more consistent and relevant data -driven conversational agents by \\nproposing a model architecture, associated training and generation algorithms which are able to \\nsignificantly improv e over the traditional seq -2-seq and information -retrieval baselines in terms of (i) \\nrelevance of the answer (ii) coherence with a predefined personality and dialog history, and (iii) \\ngrammaticality and fluency as evaluated by auto.  \\n \\nQ6. Wizard of Wikipedia: Knowledge -Powered Conversational Agents   \\nAnswer:  \\nArguably, one of the  critical  goals of AI and the ultimate goal of natural language research  is for the \\nhuman  to be able to talk to the machi ne. In order to get close to t his goal, machines must master the  no. \\nof skills: to be able to comprehend language, employ memory to retain and recall knowledge,  to reason \\nabout these concept together, and finally output a response that both fulfills fun ctional  goals in \\nconversation while simultaneously being captivating to their human speaking partner. The current state -\\nof-the-art(SOTA)  approaches, sequence to sequence (seq2seq)  models of various kinds  (Sutskever \\net al., 2014 ; Vinyals & Le,  2015 ; Serban et  al., 2016 ; Vaswani et  al., 2017 ) attempt to address some of \\nthese skil ls, but generally suffer from  inability to bring memory and knowledge to bear; as indicated by \\ntheir name, they involve encoding  input sequence, providing limit ed reasoning by transforming th eir \\nhidden state given  input, and then decoding to the output . To converse intelligently on the given topic, \\nthe speaker needs knowledge of that subject, and it is our contention here that more direct knowledge \\nmemory mechani sms need to be employed. In this work , we consider setups where this can be naturally \\nmeasured and built.  \\nWe consider the task of open -domain dialogue, where two speakers conduct open -ended chit -chat given \\nan initial starting topic, and during the conversa tion, the topic can broaden or focus on related themes. \\nDuring such conversations, an interlocutor can glean new information and personal points of view from ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='edd733ce-3777-4ee9-8c3e-f4262026b809', embedding=None, metadata={'page_label': '313', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  10 | 14 \\n \\ntheir speaking partner, while providing themselves similarly . This is a challenging task as it req uires \\nseveral components not found in many standard models. We design a set of architectures specifically for \\nthis goal that combine elements of Memory Network architectures  (Sukhbaatar et  al., 2015 ) to retrieve \\nknowledge and read and condition on it, and Transformer architectures  (Vaswani et  al., 2017 ) to provide \\nstate-of-the-art text representations and sequence models for generating outputs, which we term \\nTransformer Memory Networks.  \\n \\nQ7. ERASER: A Benchmark to Evaluate Rationalized NLP Models  \\nAnswer:  \\n \\nInterest has recently grown in interpretable (Natural Language Processing) NLP systems that can \\nreveal  how and why model  make their predictions. But work in this direction has been conducted on the \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='330d2599-60b6-40f1-97cf-93314acf34d8', embedding=None, metadata={'page_label': '314', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  11 | 14 \\n \\ndifferent dataset with correspond ingly different metrics, and inherent subjectiv ity in defining what \\nconstitute  ‘interpretability’  has translated into researcher  using different metrics to quantify performance. \\nWe aim ed to facilitate measurable progress on designing interpretable NLP (Natural Language \\nProcessing) models by releasing the  standardized benchmark of datasets — augmented and repurposed \\nfrom pre -existing corpora, and spanning the range of NLP tasks — and associated metrics for measuring \\nthe quality of rati onales. We refer to this as ERASER( Evaluating Rationales And Simple English \\nReasoning ) benchmark.  \\nIn curating and releasing ERA SER we take inspiration from stickiness of  GLUE  (Wang  et \\nal., 2019b ) and SuperGLUE  Wang  et al.  (2019a ) benchmarks for evaluating progress in natural language \\nunderstanding (NLU)  tasks. These have enabled rapid growth  in models for inclusive  language \\nrepresen tation learning. We believe still somewhat nascent subfield of interpretable NLP (Natural \\nLanguage Processing)  stands to similarly benefit from  the analogous coll ection of standardized datasets \\nor tasks and metric . \\n‘Interpretability’ is the  broad topic with  many possible realizations  Doshi -Velez and Kim ( 2017 ); Lipton \\n(2016 ). In ERASER , we focus es specifically on  rationales , i.e., snippets of text from the  source \\ndocu ment that support a specific categorization. All datasets contained in ERASER include such rational , \\nexplicitly marked by annotators as supporting specific  classific ations. By definition , rationales should \\nbe sufficient  to categorize document , but they may  not be comprehensi ve. Therefore, for some dataset,  \\nwe have collected  comp lete rationales, i.e., in which  all evidence supporting the classification has been \\nmarked.  \\nHow one measures  ‘quality’ of extracted rationales will invariably depend on their intended use. With \\nthis in mind, we propose the suite of metrics to evaluate rationales that might be appropriate for different \\nscenarios. Widely , this includes measures of agreement with h uman -provided rationales  and assessment \\nof faithfulness . The latter aim to capture  extent to  which rationales provided by the model , in fact, \\ninformed its prediction . \\nWhile we propose metrics that we t hink are reasonable, we view  a problem of designing met rics for \\nevaluating rationales -especially for capturing faithfulness — as a topic for further research that we hope \\nthat ERASER will help fa cilitate. We plan to revisit  metrics proposed h ere in future iterations of \\nbenchm ark, ideally with input from  commun ity. Notably, while we provide a ‘leaderboard ,’ this is \\nperhaps better viewed as the  ‘results boa rd’; we do not privilege any  particular metric. Instead, we hope \\nthat ERASER permits comparison between models that provide rationales wrt different criteria o f \\ninterest.  \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='953d0296-ecce-4c4b-a7d0-3d62af966d3f', embedding=None, metadata={'page_label': '315', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  12 | 14 \\n \\nQ8. End to End memory networks  \\nAnswer:  \\nTwo grand challenges in artificial intelligence (AI) research have been to build a model \\nthat can m ake multiple computational step in the service of answering the question or \\ncompleting the  task, and models that can describe long term dependencies in sequential \\ndata. \\nRecently there has been the  resurgence in models of computation using explicit st orage \\nand a notion of attention ; manipulating such storage offers an approach to both of  these \\nchallenges. In, the storage is endowed with  continuous representati on; reads from and \\nwrites to  storage, as well as other proce ssing steps, are modeled by actions of neural \\nnetworks.  \\nIn this work, we present the  new recurrent neural netw ork (RNN) architectu re where \\nrecurrence reads from  possibly large external memory mu ltiple times before outputting  \\nsymbo l. Our model can be considered  the continuous form of the Mem ory Network \\nimplemented in . The model in that work was not easy to train via back -propagation a nd \\nrequired supervision at each layer of a  network. The conti nuity of  model we present here \\nmeans that it can be trained end -to-end from input -output pairs, and so applies  to more \\ntasks, i.e. , tasks where such supervision is not available, like in language  modeling or \\nrealistically supervised question answering tasks.  Our model can also be seen as  version \\nof RNNsearch  with multiple computational steps per output symbol. We will show \\nexperimentally that  various  hops over the long -term memory are crucial to excellent  \\nperformance of our model on these tasks, and that training the memory representation \\ncan be integrated in a scalable manner into our end -to-end neural network model.  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3a7b0768-fb63-4d1b-acc0-507bf89f40c6', embedding=None, metadata={'page_label': '316', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  13 | 14 \\n \\n \\nQ9. What is LinkNet?  \\nAnswer:  \\nFrom my experience, LinkNet  is lightning fast, which is one of the main improvements the authors site \\nin their summary. LinkNet is a relatively lightweight network with around 11.5 million parameters ; \\nnetworks like VGG have more than 10x that amount.  \\nThe structure of LinkNet is to u se a series of encoder and decoder blocks to break down the image and \\nbuild it back up before passing it through a few final convolutional layers. The structure of the network \\nwas designed to minimize the number of parameters so that segmentation could be done in real -time.  \\nI performed some tests of the LinkNet architecture but did not spend too much time iterating to \\nimproving the models.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6b79077b-fe57-443e-9d29-9133fc3c1cb1', embedding=None, metadata={'page_label': '317', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  14 | 14 \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6bb9b3b3-7783-4b53-839f-c88ba1290151', embedding=None, metadata={'page_label': '318', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 1 of 15 \\n \\n \\n \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW PREPARATION  \\n(30 Days of  Interview  \\nPreparation ) \\n# Day -25  \\n \\n \\n \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b8289f06-62f1-49b6-b738-97251c91a14a', embedding=None, metadata={'page_label': '319', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 2 of 15 \\n \\nQ1. What is WRN?  \\nAnswer:  \\nWRN: It stands for Wide Residual Networks  is presented. By  widening Residual Network ( ResNet ), the \\nnetwork can be more superficial or shallow with  same accuracy or improved accuracy.  More extern al \\nnetwork  means:  \\n\\uf0b7  the n umber of layers can be reduced.  \\n\\uf0b7 Training time can be shorter , as well.  \\n \\nProblems on Residual Network ( ResNet ) \\nCircuit Complexity Theory  \\nThe authors of residual networks (ResNet)  tried to make them  as thin as  possible in favor of increasing \\ntheir depth and having less parameters  and even introduced a «bottleneck» block , which makes ResNet \\nblocks even thinner.  \\nDiminishing Feature Reuse  \\nHoweve r, As gradient flows through  network , there is nothing to force it to go through residual block \\nweights , and it can avoid learning anything during training, so there may be  either  only few blocks that \\nlearn useful representations  or many blocks share very little information with a small contr ibution  to the \\nfinal goal. This problem was formulated as  a diminishing feature reuse . \\nWRNs (Wide Residual Networks)  \\nIn WRNs, plenty of parameters are tested like the design of  ResNet block, how deep (deepening factor  l), \\nand how extensiv e (widening factor  k ) within  the ResNet block.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4ad85d93-aa32-4558-9169-14285e17d1b2', embedding=None, metadata={'page_label': '320', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 3 of 15 \\n \\nWhen  k=1, it has  the same width as the  ResNet . While  k>1, it is  k time wider than  ResNet . \\nWRN -d-k: means the WRN has  a depth of  d and with widening factor  k. \\n\\uf0b7 Pre-Activation ResNet  is used in CIFAR -10, CIFAR -100, and SVHN datasets. Original  ResNet  is \\nused in the ImageNet dataset.  \\n\\uf0b7 The significant  difference is that  Pre-Activation ResNet  has the  structure of performing batch \\nnorm and ReLU before convolution (i.e. , BN-ReLU -Conv) while original  ResNet  has the  \\nstructure of Conv -BN-ReLU. And Pre-Activation ResNet  is generally better than the original one, \\nbut it has no visible  improvement in ImageNet when layers are only around 100.  \\nThe design of the ResNet block  \\n \\n\\uf0b7  \\n\\uf0b7 B(3;3) : Orig inal «basic» block, in the above  figure a.  \\n\\uf0b7 B(3;1;3) : With one extra  ( 1×1 ) layer in between the two 3×3 layers  \\n\\uf0b7 B(1;3;1) : With   the  same dimensionality of all convolutions,  bottleneck  \\n\\uf0b7 B(1;3) : The network has the alternating (1×1, 3×3 ) convolutions . \\n\\uf0b7 B(3;1) : The network has  the alternating ( 3×3, 1×1 ) convolutions . \\n\\uf0b7 B(3;1;1) : This is Network in Network style block . \\nB(3;3) has the smallest error rate (5.73%).  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6cfd8f6b-7173-4867-b5a7-a2973b3a2886', embedding=None, metadata={'page_label': '321', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 4 of 15 \\n \\nNote: The Number of depths (layers) is different is to keep the number of parameters close to each \\nother.  \\nQ2.What is SIMCO: SIMilarity -based object C ounting ? \\nAnswer:  \\nMost approaches for counting similar objects in imag es assume a single object class ; when is no t, ad-hoc \\nlearning is necessary . None of them are genuine ly agnostic and multi -class, i.e., able to capture  repeated \\npatterns of different type s without any tuning. Counting appr oaches are based on density or regression  \\nestimation; here , we focus on counting  by detection,  so the counted  objects are individually detected first.  \\nResearch on agnostic counting is vital in many fields. It serve s for obvious  question answering , where \\ncounting questions could be made on too -specific entities outside the semanti c span of the available \\nclasses  (e.g., “What is the  most occurrent thing?” in below Fig. ). In representation learning, unsupervised \\ncounting of visual primitives (i.e., vis ible “things”) is crucial to obt ain a rich image representation . \\nCounting is a hot topic in cognitive robotics , where autonomous agents learn by  separating sensory input \\ninto the  finite number of classes (without a precise semantics), building the  classification system that \\ncounts on each of them.  \\nApplication -wise, agnostic counting may help the man ual tagging of trainin g images , providing a starting \\nguess for the annotator on single - or multi -spectral  images. Inpainting filters may benefit from a magic \\nwand capturing repeated instances to remove.  \\n \\nFigure : SIMCO on obvious  question answering: the most occurrent  object? SIMCO finds 47 LEGO \\nheads.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='993b4548-550c-4b9c-bc52-d58829e1d31b', embedding=None, metadata={'page_label': '322', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 5 of 15 \\n \\nIn this paper, we present the SIMCO ( SIMilarity -based object COunting ) approach, which is entirely \\nagnostic, i.e. , with no need for any ad-hoc class -specific fine -tuning, and multi -class, i.e. , finding \\ndifferent types of repeated patterns. Two main ideas characteri ze SIMCO.  \\nFirst, every object to be counted is considered as a specialization of a basic 2D shape: this is particularly \\ntrue with many and small objects  (see in above Fig : LEGO heads c an be approximated as circles). \\nSIMCO incorp orates this idea building upon the  novel Mask -RCNN -based classifier, fine -tuned just once \\non a novel synthetic shape dataset,  InShape . \\nThe second idea is that leveraging on the 2D shape approximation of objects ; one can naturally perform \\nunsupervised grouping of the detected objects (grouping circles with circles , etc.), \\ndiscovering  different  types of repeated entities (without resorting to a particular set of classes). SIMCO \\nrealizes this with a head branch in th e network architecture implementing triplet losses, which provides \\na 64-dim embedding that maps objects close if they share the same shape class plus some appearance \\nattributes. Aff inity propagation clustering  finds groups over this embedding.  \\nQ3. What is  Voice -Face Cross -modal Matching and Retrieval ? \\nAnswer:  \\nStudies in biology and neuroscience have shown that human’s appearances a re associated with their \\nvoices . Both the facial features and voice -controlling organs of individuals are affected by h ormones and \\ngenetic information . Human beings can recogniz e this association. For example: when hearing from the \\nphone call, we can guess the gender, the approximate age of the person on the other end of the line. Wh en \\nwatching an unvoiced TV show.  We can imagin e an approximate voice by ob serving the face movement \\nof the protagonist. With the recent advances of deep le arning, face recognition models , and speaker \\nrecognition models  have achieved ex ceptional ly high precision. Can the associations between voices and \\nfaces be discovered algorithmically by machines? The research on this problem can benefit a lot of \\napplications such as synchronizing video faces and talking sound , generating faces according to voices.  \\nIn recent  years, much research attention  has been paid  on the voice -face cross -modal learning tasks, \\nwhich have shown the feasibility of recognizing voice -face associations. This problem is generally \\nformulated as a voice -face matching task and the  voice -face retrieval task , as shown in Figure  1. Given \\na set of voice audios and faces, voice -face matching is to tell which look makes the voice when machine \\nhearing voice audio. Voice -face retrieval is to present a sorted sequence of faces in the order of  the ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8d5d545e-0c58-468a-8fd3-5eed80ccc66a', embedding=None, metadata={'page_label': '323', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 6 of 15 \\n \\nestimated match from a query of voice recording.\\n \\nSVHF  is the prior of voice -face cross -modal learning, which studies the performance of CNN -based deep \\nnetwork on this problem. The human’s baseline for  the voice -face matchi ng task is also proposed in the  \\npaper. Both the “voice to face” and the “face to voice” matching tasks are studied in the Pins and \\nHoriguchi’s work, which exhibits similar performance on these two tasks. The c urriculum learning \\nschedule is introduced in Pins for hard negative mining. Various visualizations of the embedding vectors \\nare presented to show the learned audio -visual associations in Kim’s work. DIMNet  learns the common \\nrepresentations for faces and voices by leveraging their relationship with some covariates such as gender \\nand nationality. DIMNet obtains an accuracy of  84.12% on the 1:2 matching , which exceeds th e human \\nlevel.  \\nResearch on this problem is still in the early stage. Datasets used by previous research are always tiny, \\nwhich can’t evaluate the generalization ability of models sufficiently. Traditional test schemes based on  \\nrandom tuple minin g tend to have low confidence. The  benchmark for this problem needs to be \\nestablished. This paper presents the  voice -face cross -modal matching and retrieval framework, a dataset \\nfrom Chinese speakers and a data collection tool. In the frame, cross -modal embeddings a re learned with \\nCNN -based networks , and triplet loss in a voice anchored metric space with L2 -Norm constraint. An \\nidentity -based example sampling method is adopted to improve the model efficiency. The proposed \\nframework achieves state -of-the-art performanc e on multiple tasks. For example, the result of 1:2 \\nmatching tested on 10 million triplets (thousands of people) reach ed 84.48%, which is also higher than \\nDIMNet tested on 189 people. We have evaluated the various modules of the CNN -based framework \\nand pro vided our recommendations. Even  matching and retrieval based on the average of multiple voices \\nand multiple faces are also attempted, which can  further improve the performance. This task is the \\nsimplest way of analyzing video data. Large -scale datasets are  used in this problem to ensure the \\ngeneralization ability required in a real application. The cross -language transfer capability of the model \\nis studied on the voice -face dataset of Ch inese speakers we constructed. The  series of performance \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='06912b7c-e3b9-416e-a547-19220f1ca017', embedding=None, metadata={'page_label': '324', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 7 of 15 \\n \\nmetrics are pr esented on the  tasks by extensive experiments. The source code of the paper and the dataset \\ncollection tool will be published along with the article . \\nQ4. What is CenterNet: Object Detection with Keypoint Triplets ? \\nAnswer:  \\nObject detection has been significantly improved and advanced with the help of deep learning, especiall y \\nconvolutional neural networks (CNNs). In the current era, one of the most pop ular flowcharts is anchor -\\nbased , which placed the  set of rectangles with pre -defined sizes, and regressed them to the desired place \\nwith the help of  the ground -truth objects. These approaches often need a large number of anchors to \\nensure the sufficiently high IoU (intersection over union) rate with the ground -truth objects, and the size \\nand aspect ratio of each anchor box needs to be manually designed. Also , anchors are usually not aligned \\nwith the ground -truth boxe s, which is not conducive to  bounding box classification task s. \\nTo overcome the drawbacks of anchor based approaches, a keypoint -based object detection p ipeline \\nnamed CornerNet was proposed. It represented each object by a pair of corner key  points, which bypassed \\nthe need for anchor boxes and achieved the state -of-the-art one -stage object dete ction accuracy. \\nNevertheless, the performance of CornerNet is still restricted by its relatively weak ability to refer  to the \\nglobal information of an object. That is to say since a pair of corners construct each object , the algorithm \\nis sensitive to detec t the boundary of objects, meanwhile not being aware of which pairs of critical points \\nshould be grouped into  the objects. Consequently, as shown in Figure  a, it often generates some incorrect \\nbounding boxes, most of w hich could be easily filtered out with complementary information,  e.g., the \\naspect ratio.  \\n \\nTo address this issue, we equip CornerNet with an ability to perceive  the visual patterns within each \\nproposed region, so that it can identify the correctness of ea ch bounding box by its elf. In this paper, we \\npresent the  low-cost yet effective solution named  CenterNet , which explores the central part of the  \\nproposal,  i.e., the region that is close to the geometric center, with one extra keypo int. Our intuition is \\nthat, if the  predicted bounding box has a high IoU with the ground -truth box, then the probability that \\nthe center key  point in its central region is predicted as the same class is high, and vice versa.  Thus, \\nduring inference, after the  proposal is generat ed as a pair of corner keypoints, we determine if the p lan \\nis indeed an object by checking if there is a c rucial central point of the same class falling within its central \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bf18157b-4a78-43f3-948a-8956596974fa', embedding=None, metadata={'page_label': '325', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 8 of 15 \\n \\nregion. The idea, as shown in Figure  a, is to use a triplet instead of a pair of key  points to represent each \\nobject.  \\nAccordingly, for better de tecting  the center keypoints and corners, we propose two strategies to enrich \\ncenter and corner information, respectively. The first strategy is named  as center pooling , which is used \\nin the branch for predicting the center keypoints . Center pooling helps the center keypoints obtain more \\nrecognizable visual patterns within objects, w hich makes the central part of the  proposal be better \\nperceived. We achieve this by getting out the max summed response in both horizontal and vertical \\ndirections of the center key  point on a feature map for predicting center keypoints. The second strategy \\nis named  cascade corner pooling , which equips the  original corner pooling module  with the ability to \\nperceive  internal information. We achieve this by ge tting out the max summed response in both boundary \\nand in ner directions of objects on a feature map for predicting corners. Em pirically, we verify that such \\nthe two-directional pooling method is more stable,  i.e., being more robust to the feature -level noi ses, \\nwhich contributes to the improvement of both precision and recall.  \\nWe evaluate the proposed C enterNet on the MS -COCO dataset , one of the most popular benchmarks for  \\nlarge scale object detection. CenterNet, with both center pooling and  the cascade corner pooling \\nincorporated, reports an AP of  47.0%  on the test -dev set, which outperforms all e xisting one -stage \\ndetectors by the  extensiv e margin. With an average inference time of  270ms  using a 52 -layer hourglass \\nbackbone  and 340ms  using  a 104 -layer hourglass backbone  per image, CenterNet is quite efficient yet \\nclosely matches the state -of-the-art performance of the other two -stage detectors.  \\nQ5. What is Task2Vec:  Task Embedding for Meta -Learning ? \\nAnswer:  \\nThe s uccess of Deep Learning hinges in part on the fact that models learned for one task can be used on \\nthe other related tasks. Yet, no general framework exists to describe and learn relations between tasks. \\nWe introduce task2vec  embedding, the  technique to represent tasks as elements of the  vector space is \\nbased on  the Fisher Information Matrix. The norm s of the embedding corre lates with the complexity of  \\nthe task, while the distance between embeddings captures  the semantic similarities between tasks \\n(Fig.  1). When other natural dist ances are available, such as  taxonomical distance in  the biological \\nclassification, we find that the embedding distance correlates positively with it (Fig.  2). Moreover, we \\nintroduce an asymmetric distance on tasks that correlates with the transferability between tasks.  \\nComputation  of the embedding l everages the  duality between network parameters (weights) and outputs \\n(activations) in a deep neural network (DNN): Just as the activations of a DNN trained on the compl ex \\nvisual recognition task are the  rich representation of the input images, we show that the gradients of the \\nweights relati ve to a task -specific loss are the  rich representation of the task itself. Specifi cally, given a \\ntask defined by the  dataset  D={(xi,yi)}Ni=1  of labeled samples, we fe ed the data through a pre -trained ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ad58dfc1-5de4-423e-a140-f4e5da06a7a0', embedding=None, metadata={'page_label': '326', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 9 of 15 \\n \\nreference convolutional neural network which we call “ probe network ”, and compute the diagonal Fisher \\nInformation Matrix (FIM) of the network filter parameters ,  to capture t he structure of task. Since the \\narchitecture and  weights of the probe network are fixed, t he FIM provides the  fixed -dimensional \\nrepresentation of the task s. We show this embedding encodes the “difficulty” of the task s, characteristic s \\nof the input domain, and  features of the probe networ k are useful to solve it . \\nOur task embedding can be used to reason about the space of  the tasks and solve meta -tasks. As a \\nmotivating example, we study the problem of selecting the best pre -trained feature extractor to solve a \\nnew task. This can be particularly valuable w hen there is insufficient data to train or fine -tune a generic \\nmodel, and the transfer of knowledge is essential.  task2vec  depends solely on the task and ignores \\ninteractions with the model , which may , however,  play an essential  role. To address this, we l earn about \\nthe joint task and model embedding, called  model2vec , in such a way that models whose embeddings \\nare close to a task exhibit excellent  performance on the task. We use  this to select an expert from the  \\ngiven collection, improving performance rela tive to fine -tuning a generic model trained on ImageNet \\nand obtaining close to the ground -truth optimal selection.  \\n \\n \\n \\nQ6. What is GLMNet: Graph Learning -Matching Networks for Feature  \\n Matching ? \\nAnswer:  \\nMany problems of interest in computer vision and pattern recognition area can be formulated as a \\nproblem of finding consistent correspondences between two sets of features , which are known as feature \\nmatching problem. Feature set that incorporates the pairwise constraint can be represented via an attribu te \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='57cac555-559c-4e0f-99e7-1ada07e5448d', embedding=None, metadata={'page_label': '327', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 10 of 15 \\n \\ngraph whose nodes represent the unary descriptors of feature points , and edges encode the pairwise \\nrelationships among different feature points. Based on this graph representation, feature matching can \\nthen be reformulated as a graph node matching probl em. \\nGraph matching generally first operates with both node and edge affinities that encode similarities \\nbetween the node and edge descriptors in two graphs. Then, it can be formulated mathematically as an \\nIntegral Quadratic Programming (IQP) problem with p ermutation constraint on related solution s to \\nencode the one-to-one matching constraints . It is known to be NP -hard. Thus, many methods usually \\nsolve it approximately by relaxing the discrete permutation constraint and finding local ly optimal \\nsolutions . Also , to obtain better node/edge affinities, learning methods have been investigated to \\ndetermine the more optimal parameters in  node/edge affinity computation . Recently, deep learning \\nmethods have also been  developed for matching problem s. The main benefi t of deep learning matching \\nmethods is that they can conduct visual feature representation, node/edge affinity learning , and matching \\noptimization together in an en d-to-end manner. Zanfir et al.  propose an end -to-end graph matching \\nmodel , which makes it po ssible to learn all the parameters of the graph  matching process. Wang et al.  \\nrecently aim to explore graph convolutional networks (GCNs) for graph matching which conducts graph \\nnode embedding and matching simultaneously in a unified system . \\nInspired by re cent deep graph matching methods, in this paper, we propose a novel Graph Learning -\\nMatching Network (GLMNet) for graph matching problem s. Overall, the main contributions of this paper \\nare three aspects.  \\nFirst, a critical  aspect of (feature) graph matching is the construction of two matching graphs. Exis ting \\ndeep graph matching models  generally use fixed structure graphs, such as k -NN, Delaunay graph, etc., \\nwhich thus are not guaranteed to serve the parallel task best . To address this issue, we propose to \\nadaptively learn a pair of optimal graphs for the matching task and integrate  graph learning  and graph \\nmatching  simultaneously in a unified end -to-end network architecture.  \\nSecond, the existing  GCN based graph matching model  adopts the general smoothing ba sed graph \\nconvolution operation  for graph node embedding , which may encourage the feature embedding of each \\nnode becoming more similar to  those of its neighboring nodes . This is desirable for graph node labelin g \\nor classification tasks , but undesirable for t he matching task because extensive smoothing convolution \\nmay dilute the discriminatory information. To alleviate this effect, we propose to incorporate a Laplacian \\nsharpening ba sed graph convolution operation  for graph node embedding and matching task s. Laplacian \\nsharpening process can be regarded as the counterpart of Laplacian smoothing which encourages the \\nembedding of each node farther away from its neighbors.  \\nThird, existing deep graph matching methods generally utilize a doubly stochastic normalizatio n for the \\nfinal matching prediction . This usually ignores the discrete one -to-one matching constraints in matching ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ac66cd72-2cef-4865-b0b0-38e3593a16f9', embedding=None, metadata={'page_label': '328', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 11 of 15 \\n \\noptimization/prediction. To overcome this issue, we develop a novel constraint regularized loss to further \\nincorporate the one -to-one matchin g constraints in matching prediction.  \\nExperimental results , including ablation studies , demonstrate the effectiveness of our GLMNet and \\nadvantages of devised components , including graph learning -matching architecture, Laplacian \\nsharpening convolution for d iscriminative embedding, and constraint regularized loss to encode one -to-\\none matching constraints.  \\n \\nQ7. What is SSAP: Single -Shot Instance Segmentation With Affinity  \\n Pyramid ? \\nAnswer:  \\nThe rapid develo pment of Convolutional networks  has revolutionized various vision tasks, enabling us \\nto move towards a more fine -grained understanding of images. Instead of classic boun ding-box level \\nobject detection  or cl ass-level semantic segmentation , instance segmentation provides in -depth \\nknowledg e by segmenting all the objects and distinguish  different object instances. Researchers are \\nshowing increasing interests in instance segmentation recently.  \\nCurrent state -of-the-art solutions to this challenging problem can be classified into the  proposal -\\nbased and proposal -free approaches . The proposal -based method s regard it as an extension to th e classic \\nobject detection task . After localizing ea ch object with a bounding box, the  foreground mask is predicted \\nwithin each bounding box proposal. Ho wever, the  performances of the  scheme  based m ethods are highly \\nlimited by  the quality of the bounding box predictions , and the two -stage pipel ine also limits the speed \\nof systems. By contrast, the proposal -free approach has the advantage of its efficient and straightforward  \\ndesign. This work also focuses on the proposal -free paradigm.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a5b7e5d4-67f0-4c40-8486-49b51d2dca70', embedding=None, metadata={'page_label': '329', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 12 of 15 \\n \\n \\nThe proposal -free methods mostly start by producing instance -agnostic pi xel-level semantic class labels , \\nfollowed by clustering them into the different object instances with particularly designed instance -aware \\nfeatures. However, previous methods mainly treat the two sub -processes as  the two separate stages and \\nemploy multiple modules, which is suboptimal. The mutual benefits between the two sub -tasks can be \\nexploited, which w ill further improve the performance of  the instance , segmentation. Moreover, \\nemploying multiple modules may result in  additional computational costs for  real-world applications.  \\nTo cope with the above issues, this work proposes a single -shot proposal -free instance segmentation \\nmethod, which jointly learns the pixel -level semantic class segmentation and object instance \\ndifferentiating in a unified model with a single backbone network, as shown in Fig.  1. Specifically, for \\ndistinguishing different object instances, an affinity pyramid is proposed , which can be jointly learned \\nwith the labeling of semantic classes. The pixel -pair affinity computes the probability that two pixels \\nbelong to the same instance. In this work, the short -range relationship s for pixels close to each other are \\nderived with dense small learning windows. Simultaneously, the long -range connection s for pixels \\ndistant from each other are also required to group objects with large scales or nonadjacent parts. Instead \\nof enlarging the windows, the multi -range relationship s are decou pled, and long -range connection s are \\nsparsely derived from the instance maps with lower resolutions. After that, we propose learning the \\naffinity pyramid at multiple scales along the hierarchy of a U -shape network, where the short -range and \\nlong-range affi nities are effectively learned from the feature levels with  the higher and lower resolutions \\nrespectively. Experiments in Table  3 show that the pixel -level semantic segmentation and the pixel -pair \\naffinity pyramid based grouping are indeed mutually benefited from the proposed joint learning schem e. \\nThe overall instance  of segmentation is thus further improved.  \\nThen, to utilize the cues about global context reasoning, this work e mploys a graph partition method  to \\nderive instances from the learned affinities. Unlike previous time -consuming methods, the cascaded \\ngraph partition module is presented to incorporate the graph partition process with the hierarchical \\nmanner of the affinity pyramid and finally provides both acceleration and performance improvements. \\nConcretely, with the learne d pixel -pair af finity pyramid, the  graph is constructed by regarding each pixel \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ac0eb828-d5ea-4d4d-b06e-07ab82ba3fc8', embedding=None, metadata={'page_label': '330', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 13 of 15 \\n \\nas the  node and transforming affinities into the edge scores. Graph partition is then employed from \\nhigher -level lower -resolution layers to the lower -level higher -resolution layers progressively. Instance \\nsegmentation predictions from the lower resolutions produce confident proposals, which significantly \\nreduce node numbers at higher resolutions. Thus the whole process is accelerated.  \\nQ8. What is TENER: Adapting Transformer Encoder for Name Entity  \\nRecognition ? \\nAnswer:  \\nThe named entity recognition (NER) is the task of finding the  start and end of an entity in the  sentence \\nand assigning a class for this entity. NER has been widely studied in the field of natural language \\nprocessing (NLP) because of its potential assistance in question generation  Zhou et  al. (2017 ), relation \\nextraction  Miwa and Bansal ( 2016 ), and co reference resolution  Fragkou ( 2017 ). Since  Collobert et  al. \\n(2011 ), various neural models have been introduced to avoid the hand -crafted features  Huang et  al. \\n(2015 ); Ma and Hovy ( 2016 ); Lample et  al.  \\nNER is usually viewed as a sequence labeling task, the neural models typic ally contain three components: \\nword embedding layer, context encoder layer, and decoder layer  Huang et  al. (2015 ); Ma and Hovy \\n(2016 ); Lample et  al. (2016 ); Chiu and Nichols ( 2016 ); Chen et  al.  Zhang et  al. (2018 ); Gui et  al. \\n(2019b ). The difference between various NER models mainly lies in the variance in these components.  \\nRecurrent Neural Networks (RNNs) are widely employed in NLP tasks due to its sequential \\ncharacteristic, which is  aligned well with the language. Specifically, bidirectional extended  short -term \\nmemory networks (BiLSTM)  Hochreiter and Schmidhuber ( 1997 ) is one of the most widely used RNN \\nstructures.  (Huang et  al., 2015 ) was the first one to apply the BiLSTM and  the Conditional Random \\nFields (CRF)  Lafferty et  al. (2001 ) to sequence  the labeling tasks. Owing to BiLSTM’s  high power to \\nlearn the contextual representation of words, it has been adopted by the majority of  the NER models as \\nthe encoder  Ma and Hovy ( 2016 ); Lample et  al. (2016 ); Zhang et  al. (2018 ); Gui et  al. \\nRecently, Transformer  Vaswani et  al. (2017 ) began to  prevail in the various NLP tasks, like machine \\ntranslation  Vaswani et  al. ( 2017 ), language modeling  Radford et  al. ( 2018 ), and pretraining \\nmodels  Devlin et  al. (2018 ). The Transformer encoder adopts the  fully -connected self -attention structure \\nto model the long -range context, which is the we akness of RNNs. Moreover, the Transformer has better \\nparallelism ability than RNNs. However, in the NER task, Transformer encoder has been reported to \\nperform poorly  Guo et  al. (2019 ), our experiments also confirm this result. Therefore, it is intriguing to \\nexplore the reason why the Transformer does not work well in the NER task.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8f681875-2d00-472e-b038-58a5ee51b55b', embedding=None, metadata={'page_label': '331', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 14 of 15 \\n \\n \\nThe first is that the sinusoidal position embeddi ng used in the vanilla Transformer is relative distance \\nsensitive and direction -agnostic, but this property will lose when used in the vanilla Transformer. \\nHowever, both the direction and relative distance information are essential  in the NER task. For exa mple, \\nwords after “in” are more likely to be a location or time than words before it, and words before “Inc.” is \\nmost likely to be of the entity type “ORG .” Besides, an entity is a continuous span of words. Therefore, \\nthe awareness of relative distance mig ht help the word better recognizes its neighbor. To endow the \\nTransformer with the ability of directionality and relative distance awareness, we adopt direction -aware \\nattention with the relative positional encoding  Shaw et  al. (2018 ); Huang et  al. (2019 ); Dai et  al. (2019 ). \\nWe propose a revised relative positional encoding that uses fewer parameters and performs better.  \\nThe second i s an empirical finding. The attention distribution of the vanilla Transformer is scaled and \\nsmooth. But for NER, sparse attention is suitable since not all words are n ecessary to be attended. Given \\nthe current word, a few contextual words are enough to jud ge its label. The smooth attention could \\ninclude some noisy information. Therefore, we abandon the scale factor of dot -production considera tion \\nand the use of un-scaled and sharp attention.  \\nWith the above improvements, we can significan tly boost the perfor mance of the Transformer encoder \\nfor NER.  \\nQ9. What is Subword ELMo ? \\nAnswer:  \\nRecently, pre -trained language representation has shown to be useful for improving many NLP tasks . \\nEmbeddings from Language Models is one of the most outstanding works, which uses the character -\\naware language model to augment word representation.  \\nAn essential challenge in training word -based language models is how to control  the vocabulary size for \\nbetter rare word representation. No matter how large the vocabulary is, uniqu e words are always \\ninsufficiently trained. Besides, an extensive vocabulary takes too much time and computational resource s \\nfor the model to converge. Whereas, if the diction ary is too small, the out -of-vocabulary (OOV) issue \\nwill har m the model performance slowly . To obtain  effective word representation,  Jozefowicz  et \\nal. (2016 ) introduce character -driven word embedding using the convolutional neural network (CNN) . \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0c2de31e-c71d-4256-8026-fe6138b06b76', embedding=None, metadata={'page_label': '332', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nPage 15 of 15 \\n \\nHowever,  potential insufficiency when modeling word from characters which hold little linguistic sense, \\nespecially, the morphological source . Only 86 roles are adopted in English writing, making the input too \\ncoarse for embedding learning. As we argue that for the better representation from a refined granularity, \\nthe word is too large , and character is too small, it is natural for us to consider subword unit s between \\ncharacter and the word levels.  \\nSplitting the word into subwords and using them to augment the word representation may recover the \\nlatent sy ntactic or semantic information . For example,  uselessness  could be divided  into the following \\nsubwords:  Previous work usually considers linguistic knowledge -based methods to tokenize each word \\ninto the subwords (namely, morphemes). However, such treatment may encounter the three main \\ninconveniences. First, the subwords from linguistic knowledge, typically i ncluding the morphological \\nsuffix, prefix, and  stem, may not be suitable for the  targeted NLP task  Banerjee and Bhattacharyya or \\nmislead the representation of some words, like the meaning of  understand ing cannot be formed \\nby under  and stand . Second, lingui stic knowledge, including related annotated lexicons or corpora,  may \\nnot even be available for the  specific low -resource language. Due to these limitations, we focus on the \\ncomputationally motivated subword tokenization approaches in this work.  \\nIn this pap er, we propose Embedding from Subword -aware Language Models (ESuLMo), which takes \\nsubword as input to augment word representation and release a sizeable pre -trained language model \\nresearch communities. Evaluations show that the pre -trained language models of ESuLMo outperform \\nall RNN -based language models, including ELMo, in terms of PPL and ESuLMo beats state -of-the-art \\nresults in three of four downstream NLP tasks.  \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n----------------------------------------------------------------------------------------------------------------------------- - \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='850ff339-332a-42ad-bacc-082f2bb80c3c', embedding=None, metadata={'page_label': '333', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  1 | 16 \\n \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW  \\nPREPARATION  \\n(30 Days of Interview Preparation)  \\n \\n# Day26  \\n  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a3727ee0-3708-4a97-bb4e-d081b12fcfae', embedding=None, metadata={'page_label': '334', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  2 | 16 \\n \\nQ1.What is DCGANs ( Deep Convolutional Generative Advers arial \\nNetworks)?  \\nAnswer:  \\nGANs stands for Generative adversarial networks, which  is introduced by Ian Goodfellow in 2014.  GANs \\nis entirely  new way of teaching computers how they do complex tasks through a generative process.   \\nGANs have two components . \\n\\uf0b7 A Generator ( An artist ) neural network.  \\n\\uf0b7 A Discriminator ( An art critic ) neural network.  \\nGenerator ( An artist ) generates an image.  Generator  does not know anything about the real images and \\nlearns by interacting with the  Discriminator . The Discriminator ( An art critic ) determines whether an \\nobject is “real”  and “fake”  (usually represented by a value close to 1 or 0) . \\nHigh -Level DCGAN  Architecture Diagram  \\n \\nOriginal DCGAN architecture ( Unsupervised Representation Learning with Deep Convolutional \\nGenerative Adversarial Networks ) have four convolutional  layers  for  Discriminator  and “four \\nfractionally -strided convolutions”  layers for  Generator.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0ea7a958-1e27-4ee0-9568-03eeb6a25fb6', embedding=None, metadata={'page_label': '335', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  3 | 16 \\n \\n \\nThe Discriminator Network  \\nThe Discriminator  is a “art critic”  who tries to distinguish between  “real”  and “fake”  images. This is a \\nconvolutional neural network (CNN)  for image classification.  \\nThe Discriminator  is 4 layers strided convolutions with batch normalization (except its input layer) and \\nleaky ReLU activations.  Leaky ReLU  helps the gr adients flow easier through the architecture.  \\n \\nThe Generator Network  \\nThe Generator  is“An artist ” who tries to create an image  that looks as “ real”  as possible, to \\nfool Discriminator . \\nThe Generator  is four  layers fractional -strided  convolutions with batch normalization (except its input \\nlayer) and use  Hyperbolic Tangent ( tanh)activation in the final output layer and  Leaky ReLU  in rest of \\nthe layers.  \\nTraining of DCGANs  \\nThe following steps are repeated in training  \\n\\uf0b7 First Generator  creates some new examples.  \\n\\uf0b7 And t he Discriminator  is trained using real data and generated data.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fbbe3e70-b997-443d-aa3d-987082d53a72', embedding=None, metadata={'page_label': '336', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  4 | 16 \\n \\n\\uf0b7 After  Discriminator  has been trained,  models are trained together.  \\n\\uf0b7 The Discriminator ’s weights are frozen, bu t its gradients are used in Generator  model so \\nthat  Generator  can update it’s weights.  \\n \\nQ2. Explain EnAET . \\nAnswer:  \\nEnAET : Self -Trained Ensemble AutoEncoding Transformations for Semi -Supervised Learning  \\nDeep neural network has shown its sweeping successes in learning from large -scale labeled datasets like \\nImageNet. However, such successe s hinge on the availability of  large amount of labeled examples that \\nare expensive to collect. Moreover, deep  neural networks usually have large number of parameters that \\nare prone to over -fitting. Thus, we hope that semi -supervised lea rning can not only deal with  limited \\nlabels but also alleviate the over -fitting problem by exploring unlabeled data. In this paper, we \\nsuccessfully prove that both goa ls can be achieved by training the  semi -supervised model built upon se lf-\\nsupervised representations.  \\nSemi -Supervised Learning (SSL)  has been extensively studied due to its great potential for addressing \\nthe challenge with limited labels. Most state -of-the-art approaches can be divided into two categories. \\nOne is confident pr edictions, which improves a model’s confidence by encouraging low entropy \\nprediction on unlabeled data. The other category imposes consistency regularization  by minimizing \\ndiscrepancy among the predictions by different models. The two approaches employ rea sonable \\nobjectives since good models should make confident predictions that are consistent with each other.  \\nOn the other hand, a goo d model should also recognize the  object even if it is transformed in different \\nways. With deep networks, this is usually ac hieved  by training a  model with augmented labeled data. \\nHowever, unsupervised data augmentat ion is preferred to explore effect of various transformations on \\nunlabeled data. For this reason, we will show that self -supervised representations learned from aut o-\\nending the  ensemble of spatial and non -spatial transformations can play a key role in significantly \\nenhancing semi -supervised models. To this end, we will present an Ensemble of Auto -Encoding \\nTransformations (AETs)  to self -train semi -supervised classifie rs with various transformations by \\ncombining the advantages of both existing semi -supervised approaches and the newly developed self -\\nsupervised representations.  \\nOur contributions are summarized as follows:  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='20ebf208-762b-489a-88b9-2b9ec3f0bebf', embedding=None, metadata={'page_label': '337', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  5 | 16 \\n \\n\\uf0b7 We propose first method that employs  ensemble of b oth spatial and non -spatial transformations \\nfrom both  labeled and unlabeled data in the  self-supervised fashion to train a semi -supervised \\nnetwork.  \\n\\uf0b7 We apply an ensemble of AETs to learn robust features under various transformations, and \\nimprove the consist ency of the predictions on transformed images by minimizing their KL \\ndivergence.  \\n\\uf0b7 We demonstrate EnAET outperforms the state -of-the-art models on all benchmark datasets in \\nboth semi -supervised and fully -supervised tasks.  \\n\\uf0b7 We show in the ablation study that exploring an ensemble of transformations plays a key role in \\nachieving new record performances rather than simply applying AET as a regularize . \\nQ3. What is Data Embedding Learning ? \\nAnswer:  \\nUnsupervised  word embeddings have become  basis for word representation in NLP tasks. Models such \\nas skip -gram  and Glove  capture statistics of large corpus and have good pr operties that corresponds to  \\nsemantics of word . However there are certain problems with unsupervi sed word embeddings, such as  \\ndifficulty in modeling some fine -grained word semantics. For  e.g., words in the same category but with \\ndifferent polarities are often confused because those words share common statistic s in the corpus . \\nIn supervised NLP (Natural Language Processing) tasks, these unsupervised word embeddings are often \\nused in one of 2 ways: keeping fixed or using as initialization (fine -tuning). T he decision is made based \\non amount of available training data in order to avoid overfitting. Nonetheless, underfitting with  keeping \\nfixed and certain degree  of overfitting with fine -tuning is  inevitable. Because this all are none \\noptimization of  word embeddings lacks contro l over the learning process,  embeddings are not trained to \\nan optimal point, which can result  in subopti mal task performance . \\nIn this paper, we propose  delta embedding learning , the novel  method that aims to address a  above \\nproblems together: using regularization to find  optimal fine -tuning of word embeddings. Better task \\nperformance can be reached with prop erly optimized e mbeddings. At the same time,  regularized fine -\\ntuning effectively combines semantics from supervised learning and unsupervised learning, which \\naddresses some limitations in unsuperv ised embeddings and improves  quality of embeddings.  \\nUnlike r etrofitting , which learns directly from lexical resources, our method provides the  way to learn \\nword semantics from supervised NLP (Natural Language Processing)  tasks. Embeddings usually become \\ntask-specific and lose its gene rality when trained along with t he model to maximize a task objective. \\nSome approach tried to learn reusable embeddings from NLP (Natural Language Processing)  tasks \\ninclude multi -task learning, where one predict  context words and e xternal labels at the same time , and ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='228dbf90-0f5a-4213-bdbe-ca7854763cd1', embedding=None, metadata={'page_label': '338', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  6 | 16 \\n \\nspecially designed gradient descent algorithms for fine -tuning  .Our method learns reusable supervised \\nembeddings by fine -tuning unsupervised embeddings on  supervised task with a simple modification. The \\nmethod also makes it possible to examine and interpret the learned seman tics. \\n \\nThe aim of  a method is to combine the benefits of unsupervised learning and supervised learning to learn \\nbetter word embeddings. U nsupervised word embeddi ngs like skip -gram, trained on  large corpus (like \\nWikipedia), gives good -quality word  represen tations. We use such  embedding  Wunsup  as the  starting \\npoint and learn a delta embedding  wΔ on top of it:  \\n \\nThe unsupervised embedding  Wunsup  is fixed to preserve good propertie s of the embedding space and  \\nword  semantics learned from large corpus. Delta embedding  wΔ is used to capture discriminative word \\nsemantics from supervised NLP (Natural Language Processing)  tasks and is trained together with  model \\nfor the supervised task. In order to  learn  useful word seman tics rather than task -specific peculiarities that \\nresults  from fitting (or overfitting) the  specific task, we impose  L21 loss, one kind of structured \\nregularization on  wΔ: \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0d62f04a-dc64-49ca-8606-d1b9c3f85e0d', embedding=None, metadata={'page_label': '339', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  7 | 16 \\n \\nThe regularization loss is added  as extra term to loss of  supervised task.  \\nThe effect of  L21 loss on  wΔ has straightforward  interpretation: to minimize  total moving distance of \\nword vectors in embedding space while reaching optimal task performance. The  L2part of  a \\nregularization keeps  change of word vectors small, so that it does n ot lose its original semantics. \\nThe L1 part of  regularization induces sparsity o n delta embeddings, that only small number of words get \\nnon-zero delta embeddings, while majority of words are kept intact. The combined effect is selective \\nfine-tuning with mo deration: delta embedding capture  only significant word semantics that is conta ined \\nin the training data of a  task while absent in the unsupervised embedding.  \\nQ4. Do you have any idea about Rookie ? \\nAnswer:  \\nRookie:  A unique approach for exploring news archives  \\nNews archives offer  the rich historical record. But if the  reader or the journalist wants to learn about new \\ntopic with a traditional search engine, they must enter  query and begin reading or skimming old articles \\none-by-one, slowly piecing together  intricate web of people, organizations, events, places, topics, \\nconcepts and social forces that make up “the news.”  \\nWe propose  Rookie , which began as  attempt to build a u seful tool for journalists. With  Rookie , a user’s \\nquery generates an interactive timeline, a list of important related subjects, and summary of matching \\narticles —all displayed together as a collection of  interactive linked views . Users click and drag along  the \\ntimeline to select certain date ranges, automatically regenerating the summary and subject list at \\ninteractive speed. The cumulative effect: users can fluidly investigate complex news stories as they \\nevolve across time. Quantitative user testing shows  how this system helps users better understand \\ncomplex topics from documents and finish a historical sensemaking task 37% faster than with a \\ntraditional interface. Qualitative studies with student journalists also validate the approach.  \\nWe built the final version of  Rookie  following eighteen months of iterative design and development in \\nconsultation with reporters and editors. Because the system aimed to help real -world journalists, the \\nsoftware which emerged from the design process is dramatically differen t from similar academic efforts . \\nSpecifically,  Rookie  was forced to cope with limitations in the speed, accuracy and interpretability of \\ncurrent natural langu age processing techniques . We think that understanding and designing around such \\nlimitations is vi tal to successfully using NLP in journalism applications; a topic which, to our knowledge, \\nhas not been explored in prior work at the intersection of two fields.  \\nHow it works?  \\nAt any given time,  Rookie ’s state is defined with the  user selection state , the triple  (Q,F,T)  where:  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8d98ab5e-fb33-4d21-8c70-92b344805978', embedding=None, metadata={'page_label': '340', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  8 | 16 \\n \\n\\uf0b7 Q is the  free text query string (e.g.  “Bashar al -Assad”)  \\n\\uf0b7 F is related subject string (e.g.  “30 years”) or is  null \\n\\uf0b7 T is time-span (e.g. Mar. 2000 –Sep. 2000) ; by default, this is set to  span of publication dates in \\nthe corpus.  \\nUsers first interact with  Rookie  by entering a query,  Q into a search query bar using a web browser. For \\nexample, in below figure , a user seeking to understand the roots of the Syrian civil war has entered  Q = \\n“Bashar al -Assad”. In response,  Rookie  renders  inter active time series visualization showing the \\nfrequency of match ing documents from the corpus , a list of subje cts in the matching documents , \\ncalled  subjects -sum and a textual summary of those do cuments , called  sentence -sum.1In this example, \\nthe corpus is the collection of  New York Times  world news articles from 1987 to 2007 that contain the \\nstring “Syria”. All of the country -specific examples in this study are subsets of the same  New York \\nTimes  LDC corpus.  \\nAfter entering  Q, user might notice that “Bashar al -Assad” is mainly mentioned from 1999 onwards. To \\ninvestigate, they might adjust  time series slider to a spike in early mentions of Bashar al -Assad,  T =Mar. \\n2000 –Sep. 2000 . \\nWhen  user adjusts  T to Mar. 2000 –Sep. 2000,  sente nce-sum and subjects -sum change to reflect the new \\ntimespan below figure(c) . subjects -sum now shows subj ects like “TRANSITION IN \\nSYRIA”, 2Formatting from NYT section header style.  “President Assad”, “eldest son” and “30 years” \\nwhich are important to  Q durin g T. (Bashar al -Assad’s father ruled for 30 years).  \\n \\na) A user enters  Q =“Bashar al -Assad” in order to learn more about the Syrian civil war.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='df87c1af-ca33-4d34-948b-6cbb150ce5a7', embedding=None, metadata={'page_label': '341', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  9 | 16 \\n \\nAt this point,  user might expl ore further by investigating  related subject,  F =“President Assad” —clicking \\nto select.  sentence -sum now attempts to summarize the relationship between  Q=“Bashar al -Assad” \\nand F =“President Assad” during  T =Mar. 2000 –Sep. 2000 figure(d).  For instance,  sentence -sum now \\nshows the sentence: “Bashar al -Assad was born on Sept. 11, 1965, in  Damascus, the third of President  \\nAssad’s five children.” If a user wants to understand this sentence  in context, they can click sentence —\\nwhich opens underlying document in the modal dialog.  \\nF and Q are assigned re d and blue colors throughout interface, al lowing user  to quickly scan for \\ninformation. Bolding  Q and F give additional clarity, and helps ensure that  Rookie  still work  for \\ncolorblind users.  \\nThis example demonstrates how  Rookie ’s visualization and summarization techniques work together to \\noffer lin ked views of the underlying corpus. Linked views (a.k.a.  multiple co ordinated views) \\ninterfaces are common t ools for structured information : each view displays the same selected data in a \\ndifferent dimension (e.g. a geographic map of a city which  also shows a histogram of housing costs \\nwhen a user selects a neighborhood). In  Rookie ’s case, linked views display different levels of \\nresolution. The time series visualization offers a  temporal view  of query -responsive \\ndocuments,  subjects -sum displays a  medium -level  lexical view  of important subjects within the \\ndocuments, and  sentence -sum displays a low -level  text view  of parts of the underlying documents. The \\ndocuments themselves, available by clicking extracted sentences, offer the most detailed level of zoom. \\nThus  Rookie  supports the commonly advised visualization pathway: “overview first, zoom and filter, \\nand details on demand”  (Shneiderman1996 ). \\nNote that we use  term summarization  to mean selecting  short text, or sequence of short texts, to \\nrepresent  a body of text. By this definition, both  subjects -sum and sentence -sum are a form of \\nsummarization, as each offers a textual representation of the corpus —albeit at two different levels of \\nresolution, phrases and sentences.  \\nQ5.SECRET: Semantically Enhanced  Classification of Real -world \\nTasks  \\nAnswer:  \\nSignificant progress has been made in NLP( natural language processing ) and supervised machine \\nlearning ( ML) algorithms over the past 2  decades. NLP successes incl ude machine translation, speech or \\nemotion or sentiment recognition, machine re ading, and social media mining . Hence, NLP (Natural \\nLanguage Processing)  is beginning to become widely used in real -world applica tions that include either \\ntext or speech . Supervised Machine Learning( ML) algorithms excel at m odeling the data -label \\nrelationship while maximizing performance and minimizing energy consumption and latency.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='babab8ee-2815-45a4-984f-626938b7d61e', embedding=None, metadata={'page_label': '342', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  10 | 16 \\n \\nSupervised ML algorithms train on feature -label pairs to model the application of interest and predict \\nlabels. The label involves semantic i nformation.  use this information through vector repr esentations of \\nwords to find  novel class within the  dataset. Karpathy and Fei -Fei generate figure captions based on the \\ncollective use of image datasets and word embeddings. Such stu dies indicate that data fe ature and \\nsemantic relationship  correlate well. However, current supervised (Machine Learning) ML algorithms do \\nnot utilize such correlations in the decision -making ( or prediction) p rocess. Their decisions are  based on \\nthe feature -label relationship, while neglecting signi ficant information hidden in  labels, i.e., meaning -\\nbased (sema ntic) relationships among label . Thus, they are not able to exploit synergies between feature \\nand semantic space . \\nIn this article, we show  above synergies  can be exploited to improve  prediction performance of Machine \\nLearning( ML) algorithms. Our method, called SECRET, combines vector representations of label in  \\nsemantic space with available data in  feature space within various operations (e.g., ML hyperpara meter \\noptimization and confidenc e score computation) to make  final decisions (assign labels to the datapoints). \\nSince SECRET does not target any particular Machine Learning( ML) algorithm or data structure, it is \\nwidely applicable.  \\nThe main contributions of  this article are as follows:  \\n\\uf0b7 We introduce the  dual-space Machine Learning( ML) decision proces s called SECRET. It \\ncombines  new dim ension (semantic space) with  traditional (single -space)  classifiers that operate \\nin feature space. Thus, SECRET not only utilizes available data-label pairs, but also take  \\nadvantage of meaning -based (semantic) relationships among labels  to perform classification for \\nthe given real -world task.  \\n\\uf0b7 We demonstrate the general applicability of S ECRET on various supervised Machine \\nLearning( ML) algorithms and  wide range of datasets for various real -world tasks.  \\n\\uf0b7 We demonstrate  advantages of SECRET’s new dimension (semantic space) through detailed \\ncomparisons with traditional Machine Learning( ML) app roaches that have  same processing and \\ninformation (except semantic) resources.  \\n\\uf0b7 We compare the semantic space Machine Learning( ML) model with traditional approaches. We \\nshed light on how SECRET builds  semantic space component and its impact on overall \\nclass ification performance.  \\nQ6. Semantic bottleneck for computer vision tasks  \\nAnswer:  \\nImage -to-text tasks have made tremendous progress since the advent of deep learning (DL) approaches . \\nThe work presented in this paper builds on these new types of image -to-text functions to evaluate  \\ncapacity of textual representations to s emantically and fully encode  visual content of images for ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='76568a67-e4f4-45fc-995f-d071587d893f', embedding=None, metadata={'page_label': '343', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  11 | 16 \\n \\ndemanding app lications, in order to allow  prediction function to host  semantic bottleneck  somewhere i n \\nits processing pipeline. The mai n objective of  semantic bottleneck is to play role of  explanation  of the \\npredic tion process since it offers  opportunity to examine meaningfully on what ground will further \\npredictions be made, and potentially decide to reject  them either using human commo n-sense knowledge \\nand experience, or automatically throu gh dedicated algorithms. Such the  explainable se mantic bottleneck \\ninstantiates  good tradeoff between prediction accuracy and interpretability.  \\nReliably eval uating the quality of  explanation is not str aightforward . In this work, we propose to evaluate \\nthe explainability power of the semantic bottleneck by measuring its capacity to detect the failure of the \\nprediction function, either t hrough an automated detector as , or through human judgme nt. Our proposal \\nto generate  surrogate semantic representation is to associate the  global and generic textual im age \\ndescription (caption) and visual quiz in the form o f small list of questions and answers that are expected \\nto refine contextually the generic captio n. The production of this representation is adapted to  vision task \\nand learned from the annotated data.  \\n \\nFigure : Semantic bottleneck approach: images are replaced by purely but rich textual representations, \\nfor tasks such as multi -label classification or  image retrieval.  \\n \\nQ7. Gender Bias in Neural Natural Language Processing  \\nAnswer:  \\nNatural language processing (NLP) with neural networks has grown in importance over  last few years. \\nThey provide state -of-the-art(SOTA)  models for tasks like coreference  resolution, language mo deling, \\nand machine translation . However, since these models are tr ained on human language texts, natural \\nquestion is whether they exhibit bias based on gender or other characteristics, and, if so, how should thi s \\nbias be mitigated.  This is a  question that we address in this paper.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='68740a54-a43a-436c-91b7-949035c9b805', embedding=None, metadata={'page_label': '344', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  12 | 16 \\n \\nPrior work provides evidence of b ias in autocomplete suggestions  and differences in accuracy of speech \\nrecognit ion based on gender and dialect  on popular online platforms. Word embeddings, initial pre -\\nprocessors in many (Natural Language Processing) NLP tasks, embed words of a natural language into a \\nvector space of limited dimension to use as  their semantic representation.  Observed that popular word \\nembeddings including  word  exhibit gender bias  mirroring stereotypical gender ass ociations such as the \\neponymous  \"Man is to computer programmer as a Woman is to homemaker”.  \\nYet the ques tion of how to measure bias in general way for neural (Natural Language Processing) NLP \\ntasks has not been studied . Our first contribution is  general benchm ark to quantify gender bias in  variety \\nof neural (Natural Language Processing) NLP tasks. Our defini tion of bias loosely follows  idea of causal \\ntesting: matched pairs of individuals that differ in only the targeted conc ept (li ke gender) are evaluated \\nby the model and the difference in outcomes ( or scores) is interpreted as  causal  influence of the concept \\nin scrutinized model. The  definition is parametric in  scoring function and the target concept. Natural \\nscoring func tions exist for  number of neural natural language processing (NLP)  tasks.  \\nWe instantiate  definition for two important tasks —coreference resolution and language modelin g. \\nCoreference resolution is a  task of finding words and expressions referring to the s ame e ntity in the  \\nnatural language text. The goal of l anguage modeling is to model  distribution of word sequences. For \\nneural coreference r esolution models, we measure  gender coreference score disparity between gender -\\nneutral words and gendered words like the d isparity between “doctor” and “he” relative to “doctor” and \\n“she” pic tured as edge weights in below Fig. . For  language models, we measure  disparities of emission \\nlog-likelihood of gender -neutral words conditioned on gendered sente nce prefixes as is shown i n below \\nFig. Our empirical evaluation with state -of-the-art(SOTA)  neural coreference resolution and tex tbook \\nRNN -based language models  trained on benchmark datasets fi nds gender bias in these models. Note  that \\nthese results have practical significance. Both coreference resolution and language modeling are core \\nnatural language processing (NLP)  tasks in that they form the basis of many practical systems for \\ninformation extraction , text generation, speech rec ognition  and machine translation.  \\nNext we turn our attention to mitigating the bias.  Bolukbasi et  al. (2016 ) introduced the technique \\nfor debiasing  word embeddings which has been shown to mitigate unwanted associations in an alogy \\ntasks while preserving embedding’s seman tic properties. Given their spread use, a natural question is \\nwhether this technique is sufficient to eliminate bias from do wnstream tasks like coreference resolution \\nand language modeling. As our 2nd contribution, we explore this question emp irically. We find that \\nwhile  technique does reduce bias,  residual bias is considerable. We further discover that debiasing \\nmodels that ma ke use of embeddings that are co -trained with their other parameters exhibit a significant \\ndrop in accuracy.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f4e08169-25b3-49a7-a0b9-3d9235cbbebd', embedding=None, metadata={'page_label': '345', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  13 | 16 \\n \\n \\n \\nQ8. DSReg: Using Distant Supervision as a Regularizer  \\nAnswer:  \\nConsider the following sentences in a text classification task, in which we want to identify sentences \\ncontaining revenue values:  \\n\\uf0b7 S1: The revenue of education sector is 1 million. (positive)  \\n\\uf0b7 S2: The revenue of education sector increased a lot. (hard -negative)  \\n\\uf0b7 S3: Education is a fundamental driver of global development. (easy -negative)  \\nS1 is a positive example since it contains precise value for the revenue, while both S2 and S3 are negative \\nbecause they do not have the concrete information of revenue value. However, since S2 is highly similar \\nto S1, it is hard for a binary classifier t o make a correct prediction on S2. As another example, in reading \\ncomprehension tasks like NarrativeQA  (Kočiskỳ et al., 2018 ) or MS-MARCO  (Nguyen et  al., 2016 ), \\ntruth answers are human -generated ones and might not have exact matches in the original corpus. A \\ncommonly adopted strategy is to first locate similar sequences from the original corpus using a ROUGE -\\nL threshold and then treat these sequences as a positive training examples. Sequences that are \\nsemantically similar but right below this threshold will be treated as negative examples and thus \\ninevitably introduce massive noise in training.  \\nThis problem is ubiquitous in a wide range of NLP tasks, i.e., when some of the negative examples are \\nhighly similar to the positive examples. We refer to these negative examples  as hard -negative \\nexamples  for the rest of this paper. Also, we refer to those negative examples that are not similar to the \\npositive examples as  easy-negative examples . If hard -negative examples significantly outnumber \\npositive ones, features that they sh are in common will contribute significantly to the negative example \\ncategory.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c914e8ad-5696-470d-a35c-16d58421325b', embedding=None, metadata={'page_label': '346', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  14 | 16 \\n \\nTo tackle this issue, we propose using the idea of distant supervision  to regulate the training. We first \\nharvest hard -negative examples using distant supervisio n. This process c an be done by the  method as \\nsimple as using word overlapping metrics (e.g., ROUGE, BLEU or whether a sentence contains a certain \\nkeyword). With the harvested hard -negative examples, we transform the original binary classification \\ntask to a multi -task learn ing task, in which we jointly optimize the original target objective of \\ndistinguishing positive examples from negative examples along with an auxiliary objective of \\ndistinguishing hard -negative examples plus positive examples from easy -negative examples. F or a neural \\nnetwork model, this goal can be easily achieved by using different softmax functions to readout the final -\\nlayer representations. In this way, both the difference and the similarity between positive examples and \\nhard-negative examples can be cap tured by the model. It is worth noting that there are several key \\ndifferences between this work and the mainstream works in distant supe rvision for relation extraction , at \\nboth the setup level and the model level. In traditional work on distant supervision  for relation extraction, \\nthere is no training data initially and the distant supervision is used to get positive training data. In our \\ncase, we do have a labeled dataset, from which we retrieve hard -negative examples using the distant \\nsupervision.   \\nQ9. What is Multimodal Emotion Classification?  \\nAnswer:  \\nEmotion is any  experience characterized by intense mental activity and  certain degree of pleasure or \\ndispleasure. It primarily reflects all aspec ts of our daily lives, playing the  vital role in our decision -\\nmaking and relationships. In recent years, there have been growing interest in a  development of \\ntechnologies to recognize emotional s tates of individuals. Due to  escalating use of social media, emotion -\\nrich content is being gene rated at  increasing rate, encouraging research on automatic emoji classification \\ntechniques. Social media posts are mainly composed of ima ges and captions. Each of  modalities has \\nvery distinct statistical properties and fusing these modalities helps us le arn useful representations of  \\ndata. Emotion recognition is the  process that uses low -level signal cues to predict hig h-level emotion \\nlabels. With  rapid increase in usage of emojis, researchers started using them as labels  to train \\nclassification models. A survey conducted by secondary sc hool students suggested that  use of emoticons \\ncan help reinf orce the meaning of the message . Researchers found that emoticons  when used in \\nconjunction with  written message, can help to increase the “intensity” of its intende d meaning.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c6f0f46b-be79-460e-ada1-83a3ea240ae2', embedding=None, metadata={'page_label': '347', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  15 | 16 \\n \\n \\nEmojis are being used for  visual depictions o f human emotions . Em otions help us to determine \\ninteractions among human beings. The context of emotions specifically brings out complex and bizarre \\nsocial communication. These social comm unications are identified as  judgment of other person ’s mood \\nbased on his emoji usage  (Rajhi,  2017 ). According to the  study made by Rajhi et al.  (Rajhi,  2017 ), the \\nreal-time use of emojis can detect the human emotions in different scenes, lighting conditions as well as \\nangles in real -time. Studies have shown that  emojis when embedded with text to express emotion make \\ntone and tenor of  message clearer. This further helps in reducing or eliminating the chances of \\nmisunderstanding, often associated with plain text messages. A recent study proved that co -occurrence \\nallows users to express their sentiment more effectively . \\nPsychological studies conducted in the early ’80s provide us strong evidence that human emotion is \\nclosely related to the visual content. Images ca n both express and affect people’s emotions. Hence it  is \\nintriguing and important to understand how emotions are conveyed  and how they are implied by  visual \\ncontent of images. With this as the reference, many computer scientists have been working to relate and \\nlearn different visual features from images to c lassify emotional intent. Convolutional Neural Ne tworks \\n(CNNs) have served as  baselines for major Image processing tasks. These deep Convolutional Neural \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4d29cf0d-45de-4797-a424-91e086872168', embedding=None, metadata={'page_label': '348', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  16 | 16 \\n \\nNetworks( CNNs ) combine the high and low -level features and classify images in an end -to-end multi \\nlayer fashion.  \\nEarlier  most researchers working in  field of social Natural Language Processing( NLP ) have used either \\ntextual features or visual features, but there are hardly any instances where researchers have combined \\nboth these features. Recen t works by Barbieri et al.’s, Illendula et al.’s,  on multimodal emoji predi ction \\nand Apostolova et al.  work on information extraction fusing visual and textual features have shown that \\ncombining both modalities helps in improving the accuracies. While a hi gh percentage of social media \\nposts are composed of both images and cap tion, researchers have not looked at  multimodal aspect for \\nemot ion classification. Consider  post in  above Firgure  where a user is sad and posts the  image when a \\nperson close to him le aves him. The image represents the disturbed heart and has the  textual description \\n“sometimes tough if your love leaves you #sad #hurting” conveys a sad emotion. S imilarly  emoji used  \\nconveys  emotion of being depressed. We hypothesiz e that all the modalities  from the  social media post \\nincluding visual, textual, and emoji featur es, contribute to predicting  emotion of the user. Consequently, \\nwe seek to learn  importance of different modalities towards emotion prediction task.  \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a14aa245-05d5-4d27-92be-79fb807eb92a', embedding=None, metadata={'page_label': '349', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  1 | 18 \\n \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW  \\nPREPARATION  \\n   (30 Days of Interview \\nPreparation)  \\n \\n                # Day2 7  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3e194455-9687-4893-a991-c420a5ccff70', embedding=None, metadata={'page_label': '350', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  2 | 18 \\n \\nQ1. Learning to Caption Images with Two -Stream Attention and   \\nSentence Auto -Encoder  \\nAnswer:  \\nUnderstanding the world around us via visual representations , and communicating this extracted visual \\ninformation via language is one of the fundamental skills of human intell igence. The goal of recreating \\na similar level of intellectual ability in artificial intelligence (AI) has motivated researchers from \\ncomputer vision and natural language communities to introduce the problem of automatic image \\ncaptioning. Image captioning, wh ich is to describe  the content of an image in the  natural language, has \\nbeen an active area of research and widely applied to video and image  under standing in multiple \\ndomains. The  ideal model for this challenging task must have two characteristics: understan ding of an  \\nimage content well and generating descriptive sentences which is coherent with the image content . \\nMany image captioning me thods  propose various encoder -decoder models to satisfy these  needs where \\nencoder extracts the embedding from an  image , and decoder generates the text based on the \\nembedding. These two parts are typically built with a Convolutional Neural Network (CNN) and  a \\nRecurrent Neural Network (RNN) , respectively.  \\n \\nFig.: This Image captioning decoder w ith two -stream attention and the A uxillary decoder “finds” and \\n“localizes ” relevant words better than  general caption -attention baselines  \\nOne of the challenging question  in encoder -decoder ar chitectures is how to design  interface that \\ncontrols the information flow between a CNN and  RNN. While earl y work  employs  static \\nrepresentation for  interface such that the CNN compresses an entire i mage into a fixed vector , and a n \\nRNN decodes  representation into natural language sentences, this strategy is shown to perform poorly \\nwhen  target sentence is prolonged, and the image is rea sonably cluttered. Inspired from , Xu et \\nal. propos e the power ful dynamic interface, namely attentio n mechanism, that identifies  relevant p arts \\nof a image embedding to estimate the next  word. RNN model then predicts the  word based on the \\ncontext vector associated with the rel ated image regions and the previously gene rated words. The \\nattentional interface is shown to obtain significant p erformance improvements over  static one , and \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d42aada7-6bce-4070-aff1-6a76fdab1050', embedding=None, metadata={'page_label': '351', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  3 | 18 \\n \\nsince then , it has become the  key component in all state -of-the-art(SOTA ) image captioning models . \\nAlthough  this interface is substantially effe ctive and flexible, it com es with critical  shortcoming.  \\nNevertheless , visual represent ations that are learned by Convolutional Neural Network(CNNs ) have \\nbeen rapidly improving the state -of-the-art(SOTA) recognition performance in variou s image \\nrecognition tasks in  past few years . They can still be inaccurate when applied to noisy images and \\nperform poorly to describe their visual content s. Such noisy representations can lead to in correct  \\nassociation  between words and image s regions and potentially drive the  language model to poor textua l \\ndescriptions. To address these shortcomings, we propose two impr ovements that can be used in  \\nstandard encoder -decoder b ased image captioning framework.  \\nFirst, we propose the novel and powerful attention mechanism that can more accurately attend to \\nrelevant image regions and better cope with ambiguities b etween words and image regions. It \\nautomatically identifies  latent categories  that capture high -level semantic concepts based on visual and \\ntextual cues, as illustrated in the second fig . The two -stream attention is modeled as a neural network \\nwhere each stream specialize s in orthogonal tasks: the first one soft -labels each image region with the \\nlatent categories , and the second one finds the most relevant area for each group . Then their predictions \\nare c ombined to obtain a context vector that is passed to a decoder.  \\nSecond, inspired by sequence -to-sequence (seq2seq) machine translati on methods , we introduce a new \\nregularization technique that forces the image encoder coupled with the attention block to ge nerate a \\nmore robust context vector for the following RNN model. In particular, we design and train an \\nadditional seq2seq sentence auto -encoder model (“SAE”) that first reads in a whol e sentence as input, \\ngenerates the  fixed dimensional vector, then the ve ctor is further used to recon struct  input sente nce. \\nSAE is trained to learn  structure of the input (sentence) space in an offline manner, Once it is trained, \\nwe freeze its parameters and incorporate  only its decoder part (SAE -Dec) to o ur captioning model \\n(“IC”) as the  auxiliary  decoder branch . SAE-Dec is employed along with  the original image captioning \\ndecoder (“IC -Dec”) to output  target sentence s during training and removed in test time. We show that \\nthe proposed SAE -Dec regularizer  improves the captioning performance for IC -Dec and do es not bring \\nany additional computation load in test time.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='617b39f2-904a-4905-9c4e-8b8807194abf', embedding=None, metadata={'page_label': '352', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  4 | 18 \\n \\n \\n \\nQ2.Explain PQ-NET. \\nAnswer:  \\nPQ-NET : A Generative Part Seq2Seq Network for 3D Shapes .Learning generative models of 3D \\nshapes is a crucial  problem in both computer vision and computer graphics. While graphics are mainly \\nconcerned with 3D shap e modeling, in inverse graphics , a significant  line of work in computer vision, \\none aims to infer, often from a single image, a disentangled representat ion wrt 3D shape and scene \\nstructures . Lately, there has been a steady stream of works on developing deep neural networks for 3D \\nshape generation using different shape rep resentations, e.g., voxel grids, point clouds, meshes , and , \\nmost recently, implicit f unctions . However, most of these works produce  unstructured  3D shapes, even \\nthough  object perception is generally believed to be a process of  a structural understanding , i.e., to \\ninfer shape parts, their compositions, and inter -part relations.  \\nIn this pape r, we introduce a deep neural network that represents and generates 3D shapes \\nvia sequential part assembly , as shown in both Fig. In a way, we regard  assembly sequence as a \\n“sentence ,” which organizes and de scribes the parts constituting the  3D shape. Our approach is \\ninspired, in part, by the resembl ance between speech and shape perception, as suggested b y the seminal \\nwork of Biederman  on recognition -by-components (RBC). Another related observation is that the \\nphase structure rules for langua ge parsing, first introdu ced by Noam Chomsky, take on  the view that \\nsentence is both a linear string of words and a hierarchical structure  with phrases nested in phrases . In \\nthe context of shape structure presentations, our network adheres to linear  part o rders, while other \\nworks  have opted for  hierarchical  part organizations.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='de8a515e-4e16-4184-aa69-2ade9d54b541', embedding=None, metadata={'page_label': '353', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  5 | 18 \\n \\n \\nFig 1: Our network, PQ -NET,  learns 3D shape representation as a sequential part assembly . It can be \\nadapted to generative tasks such as random 3D shape generation, single -view 3D reconstruction (from \\nRGB or depth images), and shape completion.  \\n \\nFig2: The architecture of PQ -NET: our part Seq2Seq generative network for 3D shapes.  \\nThe input to our network is a 3D shape segmented into parts, where each part is first encoded into a \\nfeature representation using  a part autoencoder; see Fig2 (a). The core component of our network is \\na  Seq2Seq  autoencoder , which encodes a sequence of part features into  the latent vector of fixed size, \\nand the decoder reconstr ucts the 3D shape, one part at the time, resulting in sequential assembly; see \\nFig 2(b). With its part -wise Seq2Seq architecture, our network is coined  PQ-NET. The latent space \\nformed by  Seq2Seq encoder enables us to adapt the decoder to perform several generative tasks , \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='56422a06-29b7-41aa-8fd8-5888f14716d7', embedding=None, metadata={'page_label': '354', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  6 | 18 \\n \\nincluding shape autoencoding, interpolation, new shape generation,  and single -view 3D reconstruction, \\nwhere all  generated shapes are composed of meaningful parts.  \\nAs training data, we take the s egmented 3D shapes from PartNet, which was built on ShapeNet . It is \\nimportant to note that we do not enforce an y particular part  order or consistency across  input shapes. \\nThe shape  parts are always specified in  the file following some linear order in the dataset; our network \\ntakes whatever part order that is in a shapefile. We train the part and Seq2Seq autoencoders of PQ -NET \\nsepar ately, either per shape category or across all shape categories, of PartNet.  \\nOur part autoencoder adapts IM -NET  to encode shape parts, rather than whole shapes, with the decoder \\nproducing an implicit field. The part Seq2Seq autoencoder follo ws a similar ar chitecture as  the original \\nSeq2Seq network developed for m achine translation . Specifically, the encoder is a bidirectional stacked  \\nrecurrent neural network (RNN)  that inputs two sequences of part features, in opposite orders, and \\noutputs a latent vector. The decoder is also a stacked RNN, which decodes the latent vector \\nrepresenting the whole shape into a sequential part assembly.  \\nPQ-NET is the first  fully generative  network that learns a 3D shape representation in the form of \\nsequential part assembly. The  only prior part sequence model was 3D -PRNN , which generates part \\nboxes, not their geometry — our network jointly encodes and decodes part structure and geometry. PQ -\\nNET can be easily adapted to various generative tasks , including shape autoencoding, novel  shape \\ngeneration,  structured  single -view 3D reconstruction from both RGB and depth images, and shape \\ncompletion. Through extensive expe riments, we demonstrate that  performance and output quality of \\nour network is comparable or superior to state -of-the-art gene rative models, including 3D -PRNN , IM-\\nNET, and StructureNet . \\n \\nQ3. What is EDIT ? \\nAnswer:  \\nEDIT : Exemplar -Domain Aware Image -to-Image Translation  \\nA scene can be expressed in various manners using sketches, semantic maps, photographs, and \\npainting,artworks , to name just a  few. The way that one portrays the scene and expresses his/her vision \\nis the so-called style, which can reflect the characteristic of either a class/domain or a specific case. \\nImage-to-image translation (I2IT)  refers to the process of conv erting an image  I of a particular  style to \\nanother of the target style  St with the content preserved. Formally, seeking the desired translator  T can \\nbe written in the following form:  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9064d55e-3be1-492a-9113-9e7e3e684ce1', embedding=None, metadata={'page_label': '355', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  7 | 18 \\n \\n \\n \\nFigure 1 : Several results by the proposed EDIT. Our EDIT can take arbitrary exemplars as reference \\nfor translating images across multiple domains , including photo -painting, shoe -edge, and semantic \\nmap-facade in  one model.  \\nWith the emergence of deep techniques, a variety of I2IT strategies have been proposed with excellen t \\nprogress made over the last decade. In what follows, we briefly review contemporary works along two \\nmain technical lines,  i.e., one-to-one translation and many -to-many translation.  \\nOne-to-one Translation.  Methods in this category aim at mapping images from a source domain to a \\ntarget domain. Benefiting from the generat ive adversarial networks (GANs) , the style of translated \\nresults satisfies the distribution of the target domain  Y, achieved by  S(It,St):=D(It,Y) , where  D(It,  \\nY) represents a discri minator to distinguish if  It is real with respect to  Y. An early attempt by Isola  et \\nal. uses conditional GANs to learn mappings between two domains. The paired data supervise the \\ncontent preservation , i.e., C(It, I):=C(It,  Igtt) with Igtt, the ground -truth target. However, in real -world \\nsituations, acquiring such paired datasets, if not impossible, is impractical. To alleviate the pressure \\nfrom data, inspired by the concept  of cycle consistency, cycleGAN  in an unsupervised fashion was \\nproposed, which adopt s C(It,I):=C(FY →X(FX→Y(I)),I)  with FX→Y the generator from \\ndomain  X to Y and FY→X the reverse one. Afterward , StarGAN  further extends the translation \\nbetween two domains that cross multiple areas in a single model. Though the effectiveness of the \\nmentioned methods has been witnessed by a broad  spectrum of specific applic ations such as photo -\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2ab77a3a-8243-40bb-aacb-6510e4952a27', embedding=None, metadata={'page_label': '356', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  8 | 18 \\n \\ncaricature, making up -makeup removal, and face manipulation , their main drawback comes from the \\nnature of d eterministic (uncontrollable) one -to-one mapping.  \\nMany -to-many Translation.  The goal of approaches in this class is to transfer the style controlled by an \\nexemplar image to a source image with content maintained. Arguably, the m ost representative work \\ngoes to, which use s the pre -trained VGG16 network  to extract the content and style features, then \\ntransfer style information by minimizing the distance between Gram matrices constructed from the \\ngenerated image and the exemplar  E, say S(It,St):=S( Gram (It),Gram (E)). Since then, numerous \\napplications on the 3D scene, face swap , portrait stylizatio n and font design  have been done. \\nFurthermore, several  schemes have also been  developed towards relieving limitations  in terms of speed \\nand flexibility. For example, to tackle the requirement of training for every new exemplar (style), \\nShen  et al.  built a meta -network, which takes in the style image and produces a corresponding image \\ntransformation network directly. Risser  et al.  proposed the histogram loss to improve the  training \\ninstability. Huang and Belongie  designed a more suitable normalization manner,  i.e., AdaIN, for style \\ntransfer. Li  et al.  replaced the Gram matrices with an alternative distribution alignment manner from \\nthe perspective of domain adaption. Johnso n et al.  trained the network with a specific style image and \\nmultiple content images while keeping the parameters at the inference stage. Chen  et al.  introduced a \\nstyle -bank layer containing several filter -banks, each of which represents a specific style. Gu et \\nal. proposed a style loss to make parameterized , and non -parameterized methods complement each \\nother. Huang  et al. designed a new temporal loss to ensure the style consistency between frames of a \\nvideo. Also , to mitigate the deterministic nature of on e-to-one translati on, several works, for instance,  \\nadvocated to separately take care of content  c(I) and style  s(I) subject to  I≃c(I)∘s(I) with ∘ the \\ncombin ed operation. They manage to control the translated results by combining the content of an \\nimage with the style of the target,  i.e., c(I)∘s(E). Besides , one domain pair requires one independent \\nmodel, their performance, as observed from comparisons, is infer ior to our method in visual quality, \\ndiversity, and style preservation. Please see the above Fig. , For example produced by our approach  \\nthat handles multiple domains and arbitrary exemplars in a unified model.  \\n \\nQ4. What is Doctor2Vec ? \\nAnswer:  \\nDoctor2Vec : Dynamic Doctor Representation Learning for   \\nClinical Trial Recruitment  \\nThe rapid growth of electronic health record (EHR) data and other health data enables the training of \\ncomplex deep learning models to learn patient representations for disease diagnosis, risk prediction, ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ab2f52c0-272e-44e0-b597-59e50d2aabfe', embedding=None, metadata={'page_label': '357', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  9 | 18 \\n \\npatient subtyping, and medication recommendation. Ho wever, almost all current  works focus on \\nmodeling patients. Deep neural networks for doctor representation learning are lacking.  \\nDoctors play pivotal roles in connecting patients and treatments, including recruiting patients into \\nclinical trials for drug d evelopment and treating and caring for their patients. Thus an effective doctor \\nrepresentation will better support a broader range of health analytic tasks. For example, identifying the \\nright doctors to conduct the trial  site selection  to improve the chanc e of completion of the \\ntrials  [hurtado2017improving ] and doctor recommendation for patients.  \\nIn this work, we focus on studying the  clinical trial recruitment  problem using doctor representation \\nlearning. Current standard practice calcul ates the median enr ollment rate . Enrollment rate of a doctor is \\nthe number of patients enrolled by a doctor to the trial.  For the therapeutic area as  the predicted  \\nenrollment success rate for whole  participating do ctors, which is often incorrect . Also , some develop a \\nmulti -step manual matching process for site selection , which is labor -intensive. Recently, deep neural \\nnetworks were applied on site selection tasks via static medical concept embedding using only frequent \\nmedical codes and simple term matching to trials. Despite the success, two challenges remain open.  \\n1. Existing works do not capture the time -evolving patterns of doctors experience and expertise \\nencoded in EHR data of patients that the doctor have seen;  \\n2. Current  jobs learn a static doctor representation. Howe ver, in practice , given a trial for a \\nparticular disease, the doctor’s experience of relevant diseases are more important. Hence the \\ndoctor representation should change based on the corresponding trial representation.  \\nTo fill the gap, we propose Doctor2Vec , which simultaneously learns i ) doctor representations from \\nlongitudinal patient EHR data and ii ) trial embedding from the multimodal trial description. In \\nparticular, Doctor2Vec leverages a dynamic memory network where the observ ations of patients seen \\nby the doctor are stored as memory while trial embedding serves as queries for retrieving from the \\nmind. Doctor2Vec has the following contributions.  \\n1. Patient embedding as a  memory for dynamic doctor s representation learning . We \\nrepresent doctors’ evolving experience based on the representations from the doctors’ patients. \\nThe patient representations are stored as a memory for dynamic doctor representation \\nextraction.  \\n2. Trial embedding as  a query for improved doctor s selection . We learn hierarchical clinical \\ntrial embedding where the unstructured trial descriptions were embedded using \\nBERT  [devlin2018bert ]. The trial embedding serves as queries of the memory network and will \\nattend over patient representation and dynamically assign weights based on the relevanc e of \\ndoctor experience and trial representation to obtain the final context vector for an optimized \\ndoctor representation for a specific t est. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b31c5424-28ad-43cf-8275-c0235674bbc8', embedding=None, metadata={'page_label': '358', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  10 | 18 \\n \\nWe evaluated Doctor2Vec using large scale real -world EHR and trial data for predicting trial \\nenrollment rates of doctors. Doctor2Vec demonstrated improved performance in the site selection task \\nover the best baselines by up to  8.7%  in PR -AUC. We also show ed that the Doctor2Vec embedding \\ncould be transferred to benefit data insufficiency settings , including trial recr uitment in less \\npopulated/newly explored countries or for rare diseases. Experimental results show for the country \\ntransfer, Doctor2Vec achieved  13.7%  relative improvement in PR -AUC over the best baseline. While \\nfor embedding transfer to uniqu e disease tri als, Doctor2Vec made  8.1% relative improvement s in PR -\\nAUC over the best b enchmark . \\n \\nQ5. Explain PAG -Net. \\nAnswer:  \\nPAG : Progressive Attention Guided Depth Super -resolution Network  \\nA geometric description of a scene, the high-quality depth map is useful in many computer vision \\napplications, such as 3D reconstruction, virtual reality, scene understanding, intelligent driving, and \\nrobot navigation. Literature mainly contains two classes of techniques for depth information \\nacquisition, which are passive methods  and active sensors. Firstly, passive methods infer depth maps \\nfrom the most widely used dense stereo matching algorithms , but they are time -consuming. Despite the \\nadvances in technology, the depth information from passive methods is still inaccurate in occluded and \\nlow-texture regions. The acquisition of high -quality depth maps is more challenging to obtain than \\nRGB images.  \\nDepth acquisition from active sensors has become increasingly popular in our daily life and ubiquitous \\nto many consumer applications, due to their simplicity, portability, and inexpensive. Unlike passive \\nmethods, the depth of a scene can be acquired in real -time, and they are more robust in low -textured \\nregions by low -cost sensors such as Time -of-Flight camera  and Microsoft Kinect. Current  sensing \\ntechniques measure depth information of a scene by using echoed light rays from the s tage. Time -of-\\nFlight sensor (ToF) is one of the mainstream type s which computes depth at each pixel between camera \\nand subject, by measur ing the round trip time. Although depth -sensing technology has attracted much \\nattention, it still suffers from several quality degradations.  \\nDepth information captured by ToF sensors suffer s from low -spatial resolutions \\n(e.g.,  176×144 , 200×200  or 512×424 ) and noise when compared with the corresponding color images. \\nDue to the offset between projector and sensor, depth maps captured by Microsoft Kinect sensors \\ncontain structural missing along discontinuities and random missing at homogeneous regions. These \\nissues restrict the use of depth maps in the development of depth -dependent applications. High -quality \\ndepth is significant in many computer vision applications. Therefore, there is a need for restoration of ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cde88a62-d820-453a-8725-896e529c7bac', embedding=None, metadata={'page_label': '359', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  11 | 18 \\n \\ndepth maps before using in applications. In this work, we consider the problem of depth map super -\\nresolution from a given low -resolution depth map and its corresponding high -resolution color image.  \\n \\nExisting depth super -resolution (DSR) methods can be roughly categorized into three groups: filter \\ndesign -based, optimization -based, and learning -based algorithms. Many of the existing technique s \\nassumed that a corresponding high -resolution color image helps to improve  the quality of depth maps \\nand used aligned RGB image as guidance for depth SR. However, significant  artifacts including texture \\ncopying and edge blurring , may occur when the assumption violated. The c olor texture w ill be \\ntransferred to the super -resolved depth maps if the smooth surface contains rich textures in the \\ncorresponding color image. Se condly, depth and color edges might not align in all the cases . \\nSubsequently, it leads to ambiguity. Hence, there is a need for optimal guidance for the high -resolution \\ndepth map.  \\nAlthough there have been many algorithms  proposed in the literature for the depth super -resolution \\n(DSR), most of them still suffer from edge -blurring and texture copying artifacts. In this paper, we \\noffer  a novel method for attention guided depth map supe r-resolution. It is based on dense residual  \\nnetworks  and involves a unique  attention mechanism. The attention used here to suppress the texture \\ncopying problem arises due to improper guidance by RGB images and transfer only the salient features \\nfrom the guidance stream. The attention module mainly involves providing spatial attention to the \\nguidance image based on the depth features. The entire architecture for the example of super -resolution \\nby the factor of  8 is shown in Above Fig . \\n \\nQ6. An End -to-End Audi o Classification System based on Raw \\nWaveforms and Mix -Training Strategy   \\nAnswer:   \\nSound is the  indispensable medium for  information transmission of  surrounding environment. When \\nsome sounds happen, such as baby crying, glass breaking , and so on, we usual ly expect that we can \\n“hear”  sounds immediately , even if we are not around. In this case, an audio classification that aims to \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='11e067b9-311f-4257-b96f-fe028be5eb83', embedding=None, metadata={'page_label': '360', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  12 | 18 \\n \\npredict whether an acoustic event appears has gained significan t attention in recent years. It has many \\npractical applications in  remote surveillance, home automation, and public security.  \\nIn real life, an audio clip usually contains mult iple overlapping sounds, and  types of sounds are various, \\nranging from natural soundscapes to human activities. It is challenging to predict  a presence o r absence \\nof audio events in  an audio clip. Audio Set  is the  common large -scale dataset in this task, which \\ncontains about two million multi -label audio clips covering 527 classes. Recently, some methods have \\nbeen proposed to learn audio tags on this  dataset. Among them, a multi -level attention model  achieved \\nstate-of-the-art(SOTA)  performance, which outperforms Google’s baseline. However, the shortcoming \\nof these models is that the input signal is the published bottleneck feature, which causes inform ation \\nloss. Considering that the actual length of sound events is different and the handcrafted features may \\nthrow away relevant information at a short time scale, raw waveforms containing more valuable  \\ninformation is a better choice for multi -label classi fication. In the audio tagging task of DCASE 2017, \\n2018 challenge, some works  [5, 6] combined handcrafted features with  raw waveforms as input signal \\non a small dataset consisting of 17 or 41 classes. To our knowledge, none of the works proposes an \\nend-to-end network taking raw waveforms as input in the Audio Set classification task.  \\nIn this paper, we propose a classification system based on two variants of ResNet , which directly \\nextracts features from raw waveforms. Firstly, we use  one-dimension (1D) ResNe t for feature \\nextraction. Then,  two-dimension (2D) ResNet with multi -level prediction and attention structure is \\nused for classification. For obtaining better classification performance further, a mix -training strategy \\nis implemented in our system. In this  training process, the network is trained with mixed data , which \\nextends  training distribution and then transferred to the target domain using raw data.  \\nIn this work, the main contributions are as follows:  \\n1. The novel end -to-end Audio Set classific ation system is proposed. To  best of our knowledge, it \\nis first time to take raw waveforms as input on Audio Set and combine 1D ResNet with 2D \\nResNet for feature extraction at different time scales.  \\n2. A mix -training strategy is introduced to improve the utilizatio n of limited training data \\neffectively . Experiments show that it is robust  in multi -label audio classification compared to \\nthe existing data augmentation methods.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d680ecc3-9c8f-42b1-ac7d-ea3ea86a232e', embedding=None, metadata={'page_label': '361', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  13 | 18 \\n \\n \\nFigure 1:  Architecture of the end -to-end audio classification network. The raw waveform (1D  vecto r) is \\nthe input signal. First, the  1D ResNet is applied to extract audio features. Then, the element s are \\ntransposed from  C×1×T to 1×C×T . Finally, a 2D ResNet with a multi -level prediction structure \\nperforms audio c lassification. The output of  network has multip le labels and is the mean of  multi -level \\nprediction results. The Block is composed of  n bottleneck blocks, where  n is relate d to a number of \\nlayers in ResNet.  \\n \\nQ7. What is Cnak ? : Cluster Number Assisted  K-means  \\nAnswer:  \\nCnak stands for  Cluster Number Assisted  K-means  \\nIn cluster ana lysis, it is required to group the  set of data points in a multi -dimensional s pace so that data \\npoints in  same group are more similar to each other than to those in other groups. These groups are \\ncalled  clusters. Various distance func tions may be used to compute  degree of dissimilarity or similarity \\namong these data points. Typically Euclidean distance function is widely used in clustering. Th is \\nunsupervised technique aim s to increas e homogeneity in the  group and heterogeneity between groups. \\nSeveral clustering methods with different characteristics have been proposed for different purposes. \\nSome well -known methods inc lude partition -based clustering , hierarchical clustering  [Hierarchy1963 ], \\nspectral clust ering  [onspectral2001 ], density -based clustering  [DBSCAN ]. However, they require the \\nknowledge of cluster number for a given dataset a \\npriori  [Lloyd57 ; onspectral2001 ; DBSCAN ; DBCLASD ; DENCLUE ]. \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='133c0e88-5072-4e0e-9182-31931f7053d5', embedding=None, metadata={'page_label': '362', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  14 | 18 \\n \\nNevertheless, estimation of the  number of clusters is  difficult problem as the underlying data \\ndistribution is unknown. Readers can find several existing techniques for determining cluster number \\nin [survey_cluster_number2017 ; R3_Chiang2010 ]. We have followed the terminology  used \\nin R3_Chiang2010  for categor izing different methods for the prediction of cluster number s. In this \\nwork, we choose to focus only on three approaches: 1) variance -based approach, 2) Structural \\napproach, and 3) the Resampling approach. Variance -based plans are based on measuring compac tness \\nwithin a cluster. Structural approaches include between -cluster separation as well as within -cluster \\nvariance. We have chosen these approaches as they are either more suitable for handling big data, or \\nappear in a comparative study by several researc hers. Some well -known approaches are Calinski -\\nHarabaz  [CH], Silhouette Coefficient  [sil], Davies -Bouldin  [DB], Jump  [jump ], Gap statistic  [gap], etc. \\nThese approaches are not appropriate for handling big data, as they are computationally intensive and \\nrequire ampl e storage space. It requires a scalable solution  [kluster2018 ; ISI_LL_LML2018 ] for \\nidentifying the number of clusters. Resampling -based  approa ches can be considered in such  scenario. \\nRecently, the concept of stability in clustering has become popular. A few \\nmethods  [instability2012 ; CV_A ] utilize the idea of clustering robustness against the randomness in the \\nchoice of sampled datasets to  explore clustering stability.  \\n \\nQ8. What is D3S ? \\nAnswer:  \\nD3S – A Discriminative Single Shot Segmentation Tracker .Visual object tracking is one of the core \\ncomputer vision problems. The most common formulation considers the task of reporting the target \\nlocation in each frame of the video given a single training image. Currently, the dominant tracking \\nparadigm, performing best in evaluations  [kristan_vot2017 , kristan_vot2018 ], is correlation bounding \\nbox tracking where the target represented by a multi -channel rectangular template is localized by cross -\\ncorrelation between the template and a search region.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='41cd7def-c20a-4787-ac53-917fdd39ae61', embedding=None, metadata={'page_label': '363', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  15 | 18 \\n \\n \\nFigure 1:  The D3S tracker represents the target by two models with complementary geometri c \\nproperties, one invariant to a wide range of transfo rmations, including non -rigid deformations (GIM - \\ngeometrically invariant model), the other assuming a rigid object with motion well approximated by a \\neuclidean change  (GEM - geometrically constrained Euclidean model). The D3S, exploiting the \\ncomplementary  strengths of GIM and GEM, provides both state -of-the-art locali zation and accurate \\nsegmentation, even in the presence of substantial deformation.  \\nState -of-the-art template -based trackers apply an efficient brute -force search for target localization. \\nSuch a strategy is appropriate for low -dimensional transformations like translation and scale change but \\nbecomes inefficient for more general situations , e.g. such that induce an aspect ratio change and \\nrotation. As a compromise, modern trackers combine approxi mate exhaustive search with sampling and \\nbounding box refinement/regression networks  for aspect ratio estimation. However, these approaches \\nare restricted to axis -aligned rectangles.  \\nEstimation of high -dimensional template -based transformation is unreliabl e when a bounding box is a \\nsparse  approximation of the target. This is common – consider , e.g. elongated, rotating, deformable \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a5ec3034-29d5-49a8-abff-88c98421b2e8', embedding=None, metadata={'page_label': '364', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  16 | 18 \\n \\nobjects, or a person with spread out hands. In these cases, the most accurate and well -defined target \\nlocation model is a binary per-pixel segmentation mask. If such output is required, tracking becomes \\nthe video object segmentation task recently popularized by DAVIS  and YoutubeVOS  challenges.  \\nUnlike in tracking, video object segmentation challenges typically consider large target  observed for \\nless than 100 frames with low background distractor presence. Top video object segmentation \\napproaches thus fare poorly in short -term tracking scenarios  where the target covers a fraction of the \\nimage, substantially changes its appearance ove r a more extended  period , and moves over a cluttered \\nbackground. Best trackers apply visual model adaptation, but in the case of segmentation errors , it \\nleads to irrecoverable tracking failure. Because of this, in the past, segmentation has played only an \\nauxiliary role in template -based trackers, constrained DCF le arning  and tracking by 3D model \\nconstruction.  \\nRecently, the SiamRPN  tracker has been extended to produce high -quality segmentation masks in two \\nstages  – SiamRPN branches first localize the targe t bounding box,  and then segmentation mask is \\ncomputed only within this region by another branch. The two -stage processing misses the opportunity \\nto treat localization and segmentation jointly to increase robustness. Another drawback is that a fixed \\ntempla te is used that cannot be discriminatively adapted to the changing scene.  \\nWe propose a new single -shot discriminative segmentation tracker, D3S, that addresses the limitations \\nas mentioned above . Two discriminative visual models encode the target  – one is adaptive and highly \\ndiscriminative but geometrically constrained to a euclidean motion (GEM), while the other is invariant \\nto a broad range of transformation (GIM, geometrica lly invariant model), see above Fig . \\nGIM sacrifices spatial relations to allow tar get localization under significant deformation. On the other \\nhand, GEM predicts the only position but discriminatively adapts to the target and acts as a selector \\nbetween possibly multiple target segmentations inferred by GIM. In contrast to related \\ntracke rs [siammask_cvpr19 , siamrpn_cvpr2019 , atom_cvpr19 ], the primary output of D3S is the \\nsegmentation map computed in a single pass through the network, which is trained end -to-end for \\nsegmentation only . \\nSome applications and most tracking benchmarks require reporting the target location as a bounding \\nbox. As a secondary contribution, we propose an effective method for interpreting the segmentation \\nmask as a rotated rectangle. This avoids an error -prone greedy search and naturally addresses changes \\nin location , scale, aspect ratio , and rotation.  \\nD3S outperforms all state -of-the-art trackers on most of the significant  tracking \\nbenchmarks  [kristan_vot2016 , kristan_vot2018 , got10k , muller_trackingnet ] despite not being trained \\nfor bounding box tracking. In video o bject segmentation benchmarks  [davis16 , davis17 ], D3S ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='975750ad-52de-4a1c-a807-2aebe69cdb84', embedding=None, metadata={'page_label': '365', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  17 | 18 \\n \\noutperforms the leading segmentation tracker  [siammask_cvpr19 ] and performs on par with top video \\nobject segmentation algorithms (often tuned to a specific domain), yet running orders of magnitude \\nfaste r. Note that the D3S is not re -trained for different benchmarks – a single pre -trained version shows \\nremarkable generalization ability , and versatility .PyTorch implementation will be made available.  \\n \\nQ9. What is DRNet?  \\nAnswer:  \\nDRNet stands for Dissect and Reconstruct the Convolutional Neural Network via Interpretable \\nManners Convolutional neural networks (CNNs) have been broadly applied on various visual tasks due \\nto its superior performance ( [vgg], [resnet ], [densenet ]). But  the huge computation burden prevents \\nconvolutional neural networks from running on mobile devices. Some works had been done to prune \\nneural networks into smaller ones ( [slimming ], [pruning1 ], [pruning2 ]). Also , there are too many \\nlightweight network struc tures ( [mobilenet ], [mobilenetv2 ], [shufflenet ]) were proposed to adapt \\nconvolutional neural networks to computational limited mobile devices. However, these methods \\nusually require running a whole pre -trained network , whatever the task is. i.e. , the first  task requires the \\ndiscrimination power of cats and dogs, and the second task requires the discrimination power of apples \\nand watermelons. If one has a CNN which was pre -trained on ImageNet, he must run the whole CNN \\non each task, which is usually time -consuming and computation wasted.  \\nOur work focuses on a n underlying  problem, i.e. , can we run only parts of a CNN? To achieve this \\ngoal, we need to find a method to dissect the whole network into pieces and reconstruct some of these \\npieces according to specif ic tasks. The reconstructed CNN should have a smaller computation cost and \\nbetter performance. Meanwhile, the process of generating  this substructure should be quick and easy. \\nTherefore this technology can be applied on mobile devices and small robots such  as cell -phones and \\nuncrew ed aerial vehicles. Using these technologies, these devices only need to store one complete \\nCNN and some information about  the substructure generating program. When specific tasks come, \\nthese devices can create a smaller substructure in an instant and run on it, rather than run the whole \\noriginal CNN.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bcb36e19-0fdd-4e05-9ac0-e1e45fc8c9a5', embedding=None, metadata={'page_label': '366', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  18 | 18 \\n \\n \\nIn this paper, we proposed a novel and interpretable algorithm to generate these smaller substructures. \\nAn interpretable way of CNN inspires our method . As sh own in Figure 1: the original CNN ha s many \\nchannels, but not every channel is useful for the discrimination of every class. What we need to do is to \\nfind the channels relev ant to every type and combine them for the specific task. This method looks \\nsimilar to the previous work: structured network pruning ( [slimming ], [pruning3 ], [pruning4 ]). \\nHowever, all of these pruning methods need fine -tuning, which is time -consuming and not allowed on \\nmobile devices. And these pruning methods are usually lack of interpre tability which is much  needed \\nby human -beings when using CNNs. Therefore, we do not mean to propose a pruning method and \\nmake CNN smaller, but to find the best channels for each class, and combine them for specific tasks. \\nOur approach  not only can be used on VGG and ResNet but also on some light structures such as \\nMobileNetV2. Also , we make this process quick and interpretable.  \\n \\n \\n \\n \\n \\n \\n \\n  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ee0ecfd3-3551-4a10-99c8-b392b188c210', embedding=None, metadata={'page_label': '367', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  1 | 16 \\n \\n \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW  \\nPREPARATION  \\n(30 Days of Interview Preparation)  \\n \\n# Day28  \\n    ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='600878fe-4b3f-4cf9-a8d2-4871d41372b6', embedding=None, metadata={'page_label': '368', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  2 | 16 \\n \\nQ1. Explain StructEdit(Learning Structural Shape Variations) . \\nAnswer:  \\nThe shapes of the 3D objects exhibit remarkable diversity, both in their compositional structure in t erms \\nof parts, as well as in geometry  of the elemen ts themselves. Yet human  are remarkably skilled at \\nimagi ning meaningful shape variation  even from the isolated object instances. For example, having seen \\na new chair, we can easily imagine its natural  change s with the different height back, a wider seat, with \\nor without armrests, or wit h a di verse  base. In this article, we investigate how to learn such shape \\nvariations directly from the 3D data. Specifically, given the  shape collection, we are interested in two \\nsub-problems: first, for any give n shape, we want to discover  main modes of edi ts, which c an be inferred \\ndirectly from  shape collection; and second, given an example edit on on e shape, we want to transfer  edit \\nto another shape in the group , as a form of analogy -based edit transfer. This ability is useful in several \\nsettings, including the design of individual 3D models, the consistent modification of the 3D model \\nfamilies, and the fitting of CAD models to noisy and incomplete 3D scans.  \\n \\nAbove Fig: Edit generation and transfer with StructEdit.   \\nWe present the StructEdit, a method  that learns the distribution of  shape differences  between structured \\nobjects that can be used to generate an ample  variety of edits ( in a first row); and accurately transfer edits \\nbetween different purpose s and across different modalities ( on the second row). Edits can b e both \\ngeometric and topological.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='286488b0-082e-46df-a376-9ee6726f88f1', embedding=None, metadata={'page_label': '369', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  3 | 16 \\n \\nThere are many challenges in capturing  space of shape vari ations. First, individual shape  can have \\ndifferent representations as image , surface meshes , or point clouds; second, one needs th e unified setting \\nfor repre senting both continuous deformations as well as structural changes ; third, shape edits are not \\ndirectly expressed but are only implicitly contained in shape colle ctions; and finally, learning the space \\nof structural v ariations t hat is applicable to more than t he single shape amounts to learning mappings \\nbetween different shape edit distributions, since diffe rent shapes have various  types and numb ers of parts \\n(like tables with or without leg bars).  \\nIn much of the ex isting  literature on 3D machine learning (ML), 3D s hapes are mapped to poin ts in t he \\nrepresentation space whose coordinates encode latent f eatures of each shape. In such  representation, \\nshape edits are encoded as vectors in that same space – in other words , as differenc es between points \\nrepresenting shapes. Equivalently, we can think of form s as “anchored” vectors rooted at  origin, while \\nshape differences are “floating” vectors that can b e transported around in shape space. This type of vector \\nspace arithmetic i s commonly \\nused [wu2016learning , achlioptas2017learning , wang2018global , gao2018automatic , xia2015realtime , \\nVillegas_2018_CVPR ], for example, in performing analogies, where the vector that is the difference of \\npossible  point  A from point  B is added to point  C to produce an analogous point  D. The challenge with \\nthis view in our setting is that while Euclidean spaces are perfectly homogeneous and vectors can be \\ncomfortabl y transported and added to points anywhere, shape spaces are far or less so. While for \\ncontinuous variations , a vector space model has some plausibility, this is not so for structural variations: \\nthe “add arms” vector does not make sense for the point representing a chair that already has arms.  We \\ntake the different approach. We consider embedding  shape s differences or deltas  directly in their own \\nlatent space , separate from  general shape embedding space. Encoding and decoding such shape \\ndifferences is always done through a VAE ( variational autoencoder ), in the context of the given source \\nshape, i tself encoded through the  part hierarchy. This has the number of key advantages: (i)  allows \\ncompact encodings of shape deltas, since in general , we aim to describe local variation ; (ii) encourages  \\nnetwork to abstract commonalities in shape variations acros s shape s pace; and (iii)  adapts the edit  to the \\nprovided sou rce shape, suppressing the mode  that are semantically implausible.  \\nWe have extensively evaluated  the StructEdit  on publicly available  shape data sets. We introduce the  \\nnew synthetic dataset with g round truth shape edits to quantitatively evaluate our method and compa re \\nit against baseline alternative . We then p rovide evaluation results on  PartNet \\ndataset  [mo2019partnet ] and provide ablation studies. Finally, we demonstrate s that extension  of our \\nmethod allow s the handling  of both images and point cloud  as shape sources, can predict plausible edit \\nmodes from single shape examples, and can a lso transfer example shape edit  on one shape to  other shapes \\nin the collection . \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dcc3deff-bbeb-4992-bc19-9d9f4824dcc6', embedding=None, metadata={'page_label': '370', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  4 | 16 \\n \\n \\nQ2. EmpGAN: Multi -reso lution Interactive Empathetic Dialogue \\nGeneration  \\nAnswer:  \\nAs a vital part of human intelligence, emot ional perceptivity is playing  elemental role in various social \\ncommunica tion scenarios, such as ., education  and healthcare systems . Recently, sensitive  conversation \\ngeneration has received an increasing amount of attention to address emotion factors in an end -to-end \\nframework . However, as  li2018syntactically   revealed , that conventional  emotional conversation system  \\naims to produce more emoti on-rich responses according to the specific user -input emot ion, which \\ninevitably leads to th e psychologic al inconsistency problem.  \\nStudies on social psychology suggest that empathy is the crucial step towards a more humanized human -\\nmachine conversa tion, which improves  emotional perceptivity in em otion -bonding social activities . To \\ndesign th e intelligent automatic dialogue system, it is essential  to make a chatbot empathetic within \\ndialogues . Therefore , in this paper, we focus on  a task of  empathetic dialogue generation , which  \\nautomatically tracks and understands the user’s emotion at each turn in multi -turn dialogue scenarios.  \\nDespite the achieved successes , obstacles to es tablishing the  empathetic conversational system are still \\nfar beyond current signs of progres s: \\n\\uf0b7 Mere ly conside ring the sentence -level emotion  while neglect ing more precise token -level \\nfeeling s may lead  to insufficient emotion perceptivity . It is challenging  to capture  nuances of \\nhuman emotion accurately without mode ling multi -granularity emotion factors in the dialogue \\ngeneration.  \\n\\uf0b7 Merely relying on the di alogue history but overlooking the potential of user feedback for the \\ngenerated responses further aggravates the deficiencies above , which causes undesirable \\nreaction s. \\nIn this paper, we propose the multi -resolution adversarial empathetic dialogue generation model, named \\nEmpGAN , to address the abov e challenges through generating more empathetic and appropriate  \\nresponse s. To capture  nuances of user feelings sufficiently, EmpGAN make responses by taking both \\ncoarse -grained sentence -level and fine -grained token -level emotions into account. The  response \\ngenerator in  EmpGAN dynamically understands sentiment s along wit h a conversat ion to perceive  a \\nuser’s emotion states in multi -turn conversations. Furthermore, an interactive adversarial learning \\nframework is augmented to take  user feedbac k into account thoughtfully, where two interactive \\ndiscriminators identify whether t he generated responses evoke  emotion perceptivity regarding b oth the \\ndialogue history and  user emotions.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6a079953-cfcd-4b53-8897-0315a6945b42', embedding=None, metadata={'page_label': '371', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  5 | 16 \\n \\nIn pa rticular, the EmpGAN contains the empathetic generator and two interactive inverse discriminators. \\nThe empathetic generator is composed of three components: ( i) A semantic understanding module based \\non Seq2Seq (sequence to sequence) neural networks that maintain the  multi -turn semantic context. ( ii) A \\nmulti -resolution emotion percep tion model captures the fine  and coarse -grained emotion factors of each \\ndialogue turn to build the emotional framework . (iii) An empathet ic response decoder combines  semantic \\nand emotional context to produce appropriat e responses in terms of both  meanin g and emotion. The two \\ninteractive inverse discriminator  additionally incor porate the user feedback and  corresponding emotional \\nfeedback as inverse supervised signal to induce the generator to produce a more empathetic response.  \\n \\nQ3. G-TAD: Sub -Graph Localization for Temporal Action Detection  \\nAnswer : \\nVideo understanding has gained much attention from both academia and industry over recent years, given  \\nthe rapid growth of videos published in online platforms. Tempora l action det ection is one of  exciting \\nbut challenging tasks in this  area. It involves detecting  start and the end frames of action instances, as \\nwell as predicting their class label . This is onerous , especially in long untrimmed videos.  \\n \\nVideo context is an important  cue to detect actions effectively . Here, we refer to mean as frames that are \\noutside the target action but carry valuable indicative information of it. Using video context to infer \\npotential actions is natural for human beings. Empirical evidence shows that human  can reliably predict \\nor guess the occu rrence of the specific  type of work  by only looking at short video snippets wh ere the \\naction does not happen . Therefore, incorporating context into tempora l action detection has become  \\nimportant strategy to boost detection acc uracy in the recent literature . Researchers have proposed various \\nways to take advantage  of the video context, such as extending temporal action bou ndaries by t he pre-\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0a0a350a-ecdd-42a6-bfdf-3c084c2f0bfa', embedding=None, metadata={'page_label': '372', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  6 | 16 \\n \\ndefined ratio , using dilated convolution to encode meaning  into features , and aggregating definition  \\nfeature  implici tly by way of the  Gaussian curve . All these methods only utilize temporal context, which \\nfollows  or precedes  an action instance in its immediate secular  neighborhood. Howev er, real -world \\nvideos vary dramatically in temporal extent, action content, and even editing preferences. The use of \\nsuch temporal context s does not fully exploit  precious  merits of the video context, and it may also impair \\ndetection accuracy if not adequate ly de signed for underlying videos.  \\nSo, what properties characterize the desirable video context for accurate action detection? First, setting  \\nshould be semantically  or gra mma tically correlated to the target action other than merely temporally \\nlocated in its vicinity.  Imagine  a case where we manually stitch an action clip into some irrelevant frames ; \\nthe abrupt scene change surrounding the action would not benefit the action detection. On the other hand, \\nsnippets located at a distance from an opera tion but co ntaining similar semantic content might provide \\nindicative hints for detecting the action. Second, context should be content -adaptive rather than manually \\npre-defined. Considering the vast variation of videos, a framework  that helps to identify  different action \\ninstances could be changed  in lengths and locations based on the video content. Third, context should be \\nbased on multiple semantic levels, since using only one form/level of meaning  is unlikely to generalize \\nwell.  \\nIn this paper, we endow video context w ith all the above properties by casting action detection as a sub -\\ngraph localization problem based on a gra ph convolutional network (GCN) . We re present each video \\nsequence as the graph, each snippet as a node, each snippet -snippet correlation as an edge, and target \\nactions associated with context as sub -graphs, as  shown in Fig.  1. The meaning  of a snippet is considered \\nto be all snippets connected to it by an edge in a video graph. We define two types of edges — temporal \\ncorner s and semantic edges, each corresponding to temporal context and grammatical  context, \\nrespectively. Temporal edges exist between each pai r of neighboring snippets, whereas semantic edges \\nare dynamically learned from the video features at each GCN layer. Hence, the multi -level context of \\neach snippet is gradually aggregated into the features of the snippet throughout the entire GCN. ResNeXt \\ninspires the structure of each GCN block , so we name this G CN-based feature extractor GCNeXt.  \\nThe pipeline of our proposed Graph -Temporal Action Detection method, dubbed G -TAD , is analogous \\nto faster R -CNN  in objec t detection. There are two critical designs in G -TAD. First, GCNeXt, which \\ngenerates context -enriched features, corresponds to the backbone network, anal ogous to a series of \\nConvoluti onal Neural Network (CNN ) layers in faster R -CNN. Second, to mimic RoI(region of interest ) \\nalignment  in faster R -CNN, we design the sub-graph alignment (SGAlign) layer to generate a fixed -size \\nrepresentation for each sub-graph a nd embed all sub -graphs into  same Euclidean space. Finally, we apply \\na classifier on the features of each sub -graph to obtain detection results. We summarize our contributions \\nas follows.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6b2ac852-1a54-4f36-bbde-299e19835f43', embedding=None, metadata={'page_label': '373', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  7 | 16 \\n \\n(1) We present a novel GCN -based video model to exploit video context for effective temporal action \\ndetection fully . Using this video GCN representation, we can adaptively incorporate multi -level semantic \\nmeaning  into the features of each snippet.  \\n(2) We propose G -TAD, a new sub -graph detection frame work , to localize actions in video graphs. G -\\nTAD includes two main modules: GCNeXt and SGAlign. GCNeXt performs graph convolutions on \\nvideo graphs, leveraging both temporal and semantic context. SGAlign re -arranges sub -graph features in \\nthe embedded space su itable for detection.  \\n(3) G-TAD achieves state -of-the-art(SOTA ) performance on two popular action detection benchmarks. \\nOn ActityNet -1.3, it achieves an average mAP of  34.09% . On THUMOS -14, it \\nreaches  40.16% mAP@0.5, beating all contemporary one -stage methods.  \\n \\nFig: Overview of G -TAD architecture.  The input of G -TAD is t he sequence of snippet features. We \\nfirst extract features using  b=3 GCNeXt blocks, which gradually aggregate both temporal and multi -level \\nsemantic context. Semantic context, encoded in semant ic edges, is dynamically learned from element s \\nat eac h GCNeXt layer. Then we feed extracted features into the SGAlign laye r, where sub -graphs defined \\nby the set of anchors are transformed to a fixed -size representation in the Euclidean space. Finally, th e \\nlocalization module scores and ranks the sub -graphs for detection.  \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a9051aaa-fd5b-42d6-ae5e-08fad140c374', embedding=None, metadata={'page_label': '374', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  8 | 16 \\n \\nQ4. What is F3Net? \\nAnswer:  \\n \\nF3Net is a combination of Fusion , Feedba ck, and Focus for Salient object detection (SOD) aims to \\nestimate the significant visual  regions of images or videos and  often serves as the pre -processing step f or \\nmany downstream vision tasks . Earlier SOD algorithms mainly rely on heuristic priors ( e.g., color, \\ntexture and contrast) to generate saliency maps. However, these hand -craft features can hardly capture \\nhigh-level semantic relations and context information . Thus they are not robust enough to complex \\nscenarios. Recently, convolutional neural networks (CNNs) have demonstrated its powerful feature \\nextraction capability i n visual feature representation.  Many CNNs -based models  have achieved \\nremarkable progress and pushed the performance of SOD to a new level. These models adopt the \\nencoder -decoder architecture, which is simple in structure and computationally efficie nt. The encoder \\nusually is made up of a pre -trained classification model (e.g. , ResNet  and VGG ), which can extract \\nmultiple features  of different semantic levels and resolutions. In the decoder, extracted features are \\ncombined to generate saliency  maps.  \\nHowever, there remain  two significant  challenges in accurate SOD. First, features of different levels have \\ndifferent distribution chara cteristics. High -level features have rich semantics but lack precis e location \\ninformation. Low -level features have rich details but full of background noises. To generate better \\nsaliency maps, multi -level features are combined. However, without delicate c ontrol of the information \\nflow in the model, some redundant features, including noises from low -level layers and coarse boundaries \\nfrom high -level layers , will pass in and possibly result in performance degradation. Second, most of the \\nexisting models use binar y cross -entropy that treats all pixels equally. Intuitively, different pixels deserve \\ndifferent weights,  e.g., pixels at the boundary are more discriminative and should be attached with more \\nimportance. Various boundary losses  have been proposed to enhance the boundary detection accuracy, \\nbut considering only the boundary pixels is not comprehensive e nough  since there are lots of pixels near \\nthe boundaries prone to wrong predictions. These pixels are also essential  and should be assigned with \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e8d033c0-373e-417c-8853-4988106809fb', embedding=None, metadata={'page_label': '375', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  9 | 16 \\n \\nlarger weights. In consequence, it is essential to design a mechanism to reduce the impact of \\ninconsistency be tween features of different levels and assign larger weights to those signific ant pixels.  \\nTo address the above challenges, we proposed a novel SOD framework, named F 3Net, which achieves \\nremarkable performance in producing high -quality saliency maps. First,  to mitigate the discrepancy \\nbetween features, we design a cross -feature module (CFM), which fuses element s of different levels by \\nelement -wise multiplication. Different from addition and concatenation, CFM takes a selective fusion \\nstrategy, where redundant information will be suppressed to avoid the contamination between features , \\nand important features will complement each other. Compared with traditional fusion methods, CFM can \\nremove background noises and sharpen boundaries, as shown in Fig.  1. Second, due to downsampling, \\nhigh-level fea tures may suffer from information loss and distortion, which can not be solved by CFM. \\nTherefore, we develop the cascaded feedback decoder (CFD) to refine these features iteratively. CFD \\ncontains multiple sub -decoders, each of which include s both bottom -up and top -down processes. For the \\nbottom -up method , multi -level features are aggregated by CFM gradually. For the top-down process, \\naggregated features are feedback into previous features to refine them. Third, we propose the pixel \\nposition -aware loss (PPA) to imp rove the commonly used binary cross -entropy loss , which treats all \\npixels equally. Pixels located at boundaries or elongated areas are more complicated  and discriminating. \\nPaying more attention to these hard pixels can further enhance model generaliz ation. PPA loss assigns \\ndifferent weights to different pixels, which extends binary cross -entropy. The weight of each pixel is \\ndetermined by its surrounding pixels. Hard pixels will get larger weights , and easy pixels will get smaller \\nones.  \\nTo demonstrate t he performance of F 3Net, we report experiment al results on five popular SOD datasets \\nand visualize some saliency maps. We conduct a series of ablation studies to evaluate the effect of each \\nmodule. Quantitative indicators and visual results show that F 3Net c an obtain  significantly better local \\ndetails and improved saliency maps. Codes ha ve been released. In short, our main contributions can be \\nsummarized as follows:  \\n\\uf0b7 We introduce the cross feature module to fuse features of different levels, which can extract the \\nshared parts between  features  and suppress each other’s background noises and complement each \\nother’s missing parts.  \\n\\uf0b7 We propose the cascaded feedback decoder for SOD, which can feedback features of both high \\nresolutions and high semantics to pr evious ones to correct and refine them for better saliency \\nmaps generation.  \\n\\uf0b7 We design pixel position -aware loss to assign different weights to different positions. It can better \\nmine the structure information contained in the features and help the network focus more on \\ndetail regions.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6c8dd6fe-1731-496b-b54c-0c2f4bdd5019', embedding=None, metadata={'page_label': '376', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  10 | 16 \\n \\n\\uf0b7 Experimental results demonstrate that the proposed model F 3Net achieves the state -of-the-art \\nperformance on five datasets in terms of six metrics, which proves the effectiveness and \\nsuperiority of the proposed method.  \\n \\nQ5.Natural Language Generation using Reinforcement Learning with \\nExternal Rewards  \\nAnswer:  \\nWe aim to develop models that are capable of generating language acro ss several genres of text , \\nconversational text s, and restaurant re views. After all, humans are ad ept at both. Extant NLG(natural \\nlanguage generation ) models work on either conversational text ( like movie dialogues) or longer text \\n(e.g., stories, reviews) but not both.  Also , while the state -of-the-art(SOTA ) in this field has advanc ed \\nquite rapidly, current model  is prone to generate language that is s hort, dull, off -context . More \\nimportantly,  a generated languag e may not adequately reflect  affective content of the input. Indeed, \\nhumans are already adept at t his task , as well. To address these re search challenges, we propose the \\nRNN -LSTM architecture that uses an encoder -decoder network. We also use reinforcement learning (RL) \\nthat incorporates internal and external rewards. Specifically, we use emotional appropriate ness as an \\ninternal reward for the NLG (Natural Language Generation) system – so that the emotional tone of \\ngenerated language is consistent with the emotional tone of p rior context fed as input to  model. We also \\neffectively incorporate usefulness scores as external rewards in our model. Our main contribution is the \\nuse of distantly labeled data in  architecture that generates coherent, affective content and we test the \\narchitecture across two different genres of text.  \\nWhat are the problem statement and their  intuition?  \\nOur aim is to take advantage of reinforcement learning (RL) and external rewards during the process of \\nlanguage generation. Complementary to this goal, we also aim  to generate language that has  same \\nemotional tone as the other  input. Emotions are recog nized as functional in decision -making by \\ninfluencing motivation and action selection.  However, the external fe edback and rewards are hard to \\ncome by for language generation; these would need to be provided  through crowdsourcing judgment  on \\ngenerated responses  during  generation proce ss, which makes  process time -consuming and impractical. \\nTo overcome th is problem, we look for distance labeling and use labels provided in  training set as a \\nproxy for human ju dgment  on generated responses. Particularly, we inc orporate usefulness scores in a \\nrestaurant review corpus as the proxy for external feedback.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='240b5e70-287b-4696-bd04-b87251cecf8a', embedding=None, metadata={'page_label': '377', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  11 | 16 \\n \\n \\nFig. 1:  Overall Architecture of the system showing internal and external rewards using reinforcement \\nlearning  \\nQ6. LaFIn: Generative Landmark Guided Face Inpainting  \\nAnswer:  \\nImage inpainting ( a.k.a.  image completion) refers to the process of reconstructing lost or deteriorated \\nregions of images, which can be applied to, as a fundamental component, various tasks suc h as image \\nrestoration and editing.  Undoubtedly, one expects the completed result to be realistic  so that the \\nreconstructed regions can be hardly perceived. Compared with natural scenes like oceans and lawns, \\nmanipulating faces, the focus of this work, is more challenging. Because the faces have much stronger \\ntopological structure and attribute consistency to preser ve. Figure  1 shows three such examples. Very \\noften, given the o bserved clues, human beings can easily infer what the lost parts possibly, although \\ninexactly, look like. As a consequence, a slight violation o f the topological structure and  the attribute \\nconsistency in the reconstructed face highly likely leads to a significant perceptual flaw. The following \\ndefines  the problem:  \\nDefinition:  \\nFace Inpainting. Given a face image , I with corrupted regions masked by  M. Let ¯¯¯¯¯¯M designate the \\ncomplement of  M and ∘ the Hadamard  product. The goal is to fill the target part with semantically \\nmeaningful and visually continuous information to the observed part. In other words, the completed \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='59a45055-2844-43ac-98a9-94a0e460571c', embedding=None, metadata={'page_label': '378', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  12 | 16 \\n \\nresult  ^I:=M∘^I+¯¯ ¯ ¯ ¯¯M∘I should preserve the topological structure among face components suc h as \\neyes, nose , and mouth, and the attribute consistency on like pose  gender, ethnicity , and expression.  \\n           \\nFigure 1:  Three face completion results by our method. From left to right: corrupted inputs, plus \\nlandmarks predicted from the inputs, and our final results, respectively.  \\n       \\nQ7. Image2StyleGAN++: How to Edit the Embedded Images?  \\nAnswer:  \\n \\n \\n(i)                             (ii)                              (iii)                                (iv)  \\nFrom above fig: (i) and ( ii): input images; ( iii): the “two -face” generated by naive ly copying the left half \\nfrom ( i) and the right half from (i i); (iv): the “two -face” created by our Image2StyleGAN++ framework.  \\nRecent GANs demonstrated that synthetic images c ould be generated with very high q uality. This \\nmotivates research into embedding algorithms that embed a given photograph into a GAN latent space. \\nSuch embedding algorithms can be used to analyze the limitations of GANs , do image inpainting, local \\nimage editing, global image transformations such as image m orphing and expression transfer, and few -\\nshot video generation.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='57fa46b5-5d61-4158-b7c8-545f56697af6', embedding=None, metadata={'page_label': '379', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  13 | 16 \\n \\nIn this paper, we propose to extend a very recent embe dding algorithm, Image2 StyleGAN . In particular, \\nwe would like to improve this previous algorithm in three aspects. First, we noticed that the embedding \\nquality c ould be further improved by including No ise space optimization into  embedding framework. \\nThe key insight here is that stable Noise space optimizat ion can only be conducted if optimization is \\ndone sequentially with  W+ space and not jointly. Seco nd, we would like to improve  capabilities of the \\nembedding algorithm to increase the local control over the embedding. One way to improve local \\nauthority  is to include  mask  in embedding algorithm with undefined content. T he goal of the embedding \\nalgorithm should be to find a plausible embedding for everything outside the mask, while filling in \\nreasonable semantic content in the masked pixels.  \\nSimilarly, we would like to provide the option of approximate embeddings, where t he specified pixel \\ncolors are only a guide for the embedding. In this way, we aim to achieve high -quality embeddings that \\ncan be controlled by user scribbles. In the third technical part of the paper, we investigate the combination \\nof embedding algorithm a nd direct manipulations of the activation maps (called activation tensors in our \\narticle ). \\nQ8. oops!  Predicting Unintentional Action in Video  \\nAnswer:  \\nFrom just a glance at the video, we can  often tell whether a person’s action is intentional or not. For \\nexample, the Below figure shows a person attempting to jump off a raft, but unintentionally tripping into \\nthe sea. In a classic series of papers, developmental psychologist Amanda Woodward demonstrated that \\nchildren learn this ability to recognize the intentionality of actio n during their first year. However, \\npredicting the intention behind action has remained elusive for machine vision. Recent advances in action \\nrecognition have primari ly focused on predicting the physical motio ns and atomic opera tions in the video, \\nwhich captures the means of action but not the intent of action.  \\nWe believe a key limitation for perceiving visual intentionality has been the lack of realistic data with \\nnatural variation of intention. Although there are now extensive video datasets for action recognition, \\npeople are usually competent, which causes datasets to be biased towa rds successful outcomes. However, \\nthis bias for success makes discriminating and localizing visual intentionality  challenging for both \\nlearning and quantitative evaluation.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1f202c2d-bf86-47e8-8cbf-33480bd2fc53', embedding=None, metadata={'page_label': '380', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\n \\nP a g e  14 | 16 \\n \\n \\nFig: The oops! Dataset:  Each pair of frames shows an example of intentiona l and unintentional \\naction in our dataset. By crawling publicly available ‘‘fail’’ videos from the web, we can create a \\ndiverse and in -the-wild dataset of accident al action. For example, at the bottom -left cor ner shows \\na man failing to see  gate arm, and at the top -right shows two children playing  competitive game s \\nwhere it is inevitable ; one person will fail to accomplish their goal.  \\nWe introduce a new annotated video dataset that is abundant with unintentional action, which we have \\ncollected by crawling publicly available ‘‘fai l’’ videos from the web. From the above figure shows s ome \\nexamples, which cover in -the-wild situations for both intentional and unintentional action. Our video \\ndataset, which we will publicly release, is both large (over 50 hours of video) and diverse (covering \\nhundreds of scenes and activities). We annotate d videos with the temporal location at which the video \\ntransitions from intentional to unintentional action. We define three tasks on this dataset: classifying the \\nintentionality of action, localizing the change  from intentional to unintentional, and for ecasting  onset of \\nunintentional action shortly into the future.  \\nTo tackle these problems, we investigate several visual clues for learning with minimal labels to \\nrecognize intentionality. First, we propose a novel self -supervised task to learn to predi ct the speed of \\nthe video, which is incidental supervision available in all unlabeled video s for learning the action \\nrepresentation. Second, we e xplore the predictability of  temporal context as a clue to learn features, as \\nunintentional action often deviate s from expectation. Third, we study an order of events as a clue to \\nrecognize intentionality, since intentional action usually precedes unintentional action.  \\nExperiments and visualizations suggest that unlabeled video has intrinsic perceptual clues to rec ognize \\nintentionality. Our results show that, while each self -supervised task is useful, and learning to predict the \\nspeed of video helps the most. By ablating model and design choices, our analysis also suggests that \\nmodels do not rely solely on low -level mot ion clues to solve unintentional action prediction. Moreover, \\nalthough human 's consistency in our dataset is high, there is still a large gap in performance between our \\nmodels and human agreement, underscoring that analyzing h uman goals from videos remains t he \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ffa540c5-029d-4b92-ba5e-2fc4f6a6ce6b', embedding=None, metadata={'page_label': '381', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  15 | 16 \\n \\nfundamental challenge in computer vision (OpenCV). We hope this dataset of unintentional and \\nunconstrained action can provide the pragmatic benchmark of progress.  \\nQ9. FairyTED: A Fair Rating Predictor for TED Talk Data  \\nAnswer:  \\nIn recent times, artificial intelligence is bei ng used  for inconsequential decision making. Governments \\nmake use of it in the criminal justice system to predict \\nrecidivism  [brennan2009evaluating , tollenaar2013method ], which affects the decision about bail, \\nsentencing , and parole. Various firms are also using machine learning algorithms to examine and filter \\nresumes of job applicants  [nguyen2016hirability , chen2017automated , naim2016automated ], which is \\ncrucial for the growth of a company. Machine learning algorithms are also being used to evaluate \\nhuman’s socia l skills , such as presentation performance  [Chen2017a , Tanveer2015 ], essay grading.  \\nTo solve such decision -making problems, machine learning algorithms are trained on massive datasets \\nthat are usually collected in the wild. Due to difficulties in the manual  curation or adjustment over large \\ndataset s, the data likely  capture unwanted bias towards the underrepresented group based on race, gender , \\nor ethnicity. Such bias results in unfair decision -making systems, leading to unwanted and often \\ncatastrop hic consequences to human life and society. For example, the recognition rates of pedestrians \\nin autonomous vehicles are reported to be not equally accurate for all groups of \\npeople  [wilson2019predictive ]. Matthew et al.  [kay2015unequal ]showed that societa l bias gets reflected \\nin the machine learning algorithms through a biased dataset and causes representational harm for \\noccupations. Face recognition is not as useful  for people with different skin tones. Dark -skinned females \\nhave  43 times higher detection error than light -skinned males.  \\nIn this work, we propose a predictive framework that tackles the issue of designing a fair prediction \\nsystem from biased data. As an application scenario, we choose the problem of fair rating predicti on in \\nthe TED talks. TED talks cover a wide variety of topics and influence the audience by educating and \\ninspiring them. Also , it consists of speakers from a diverse community with imbalances in age, gender , \\nand ethnic attributes. The ratings are pr ovided by spontaneous visitors to the TED talk website. A \\nmachine learning algorithm trained solely from the audience ratings will have a possibility of the \\npredicted score  being biased by sensitive attributes of the speakers.  \\nIt is a challenging problem because numerous factors drive human behavior  and hence have huge \\nvariability. It is challenging  to know the way these factors interact with each other. Also , uncovering the \\ntrue interaction model may not be feasible and often expensive. Even though the sharing platforms such \\nas YouTube, Massive Open Online Courses (MOOC), or  ted.com  make it possible to collect a large \\namount of observational data, these platforms do not correct for bias and unfair ratings.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7e91dd30-09af-4cc4-975f-6d588bb3085b', embedding=None, metadata={'page_label': '382', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  16 | 16 \\n \\nIn th is work, we utilize  causal models  [pearl2009causal]  to define possible dependencies between \\nattributes of the data. We then address the problem  of not knowing  true interaction model by averaging \\noutputs of predictors across several possible causes. Furth er, using these causal models , we \\ngenerate  counterfactual samples  of sensitive attributes. These counterfactual samples are the key \\ncomponents in our fair prediction framework (adapted \\nfrom  kusner2017counterfactual  russell2017worlds) and help reducing b ias in ratings wrt sensitive \\nattributes. Finally, we introduce the  novel metric to quantify  degree of fairness employed by our \\nFairyTED pipeline. To  best o f our knowledge, FairyTED is  first fair prediction pipeline for  public \\nspeaking dataset s and can be applied to any dataset of  similar grounds. Apart from  theoretical \\ncontribution, our work also has practical implications in helping both the viewers and  organizers make \\ninformed and unbiased choices for the selection of talks an d speakers.  \\n \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4e69c5c4-9539-420b-9aeb-407fbb2057b6', embedding=None, metadata={'page_label': '383', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  1 | 16 \\n \\n \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW  \\nPREPARATION  \\n(30 Days of Interview Preparation)  \\n \\n# Day29  \\n \\n  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b76646e8-0eb6-428c-8958-725f79515d36', embedding=None, metadata={'page_label': '384', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  2 | 16 \\n \\nQ1. What is SuperGlue ? \\nAnswer:  \\n \\n \\nSuperGlue is a  Learning Feature Matching with Graph Neural Networks . Correspondences between \\npoints in imag es are essential for estimating 3D structure and camera poses in geometric computer \\nvision (OpenCV)  tasks such as SLAM (Simultaneous Localization and Mapping ) and SfM(  Structure -\\nfrom -Motion ). Such correspondences are generally estimat ed by matching local fea tures, the  process \\ncalled  as data association. Broad  viewpoint and lighting changes, occlusion, blur, and lack of texture are \\nfactors that make 2D -to-2D data association particularly challenging.  \\nIn this paper, we present new way of thinking about  feature matching problem. Instead of learning better \\ntask-agnostic local features followed by simple matching heuristics and tricks, we propo se to determine  \\nthe matching process from pre -existing local features using a novel neural architecture called SuperGlue. \\nIn the c ontext of SLAM, which typically  decomposes the problem into the visual feature extraction  front -\\nend and the bundle adjustment or pose s estimation  back -end, our network lies directly in middle \\n– SuperGlue is a learnable  middle -end (see in above Figure ). \\nIn this work,  learning feature matching  is viewed as finding  partial assignment be tween two sets of local \\nfeature. We revisit  classical graph -based strategy of matching by solving the  linear assignment pr oblem, \\nwhich, when relaxed to the  optimal transport problem, can be solved differentiably. The cost function of \\nthis optimization is predicted by a GNN (Graph Neural Network ). Inspir ed by  success of the Trans former , \\nit uses self - (intra -image) and cross - (inter -image) attention to leverag ing both spat ial relationships of  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='91ce30fd-3940-4d18-a433-427fd7628055', embedding=None, metadata={'page_label': '385', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  3 | 16 \\n \\nkeypoints and their visual appearanc e. This formulation enforces  assignm ent structure of the prediction  \\nwhile enabling cost to learn complex priors, handling occlusion , and non -repeatable keypoints. Our \\nmethod is trained end -to-end from image s pair  – we learn p riors for pose estimation from  large annotated \\ndataset, enabli ng SuperGlue to reason about 3D scen e and  assignment. Our work can be applied to a \\nvariety of multiple -view geometry problems that require high -quality feature s correspondences  (see in \\nbelow Figure ). \\nWe show  superiority of SuperGlue compared to both handcrafted matches and learned inlier cla ssifiers . \\nWhen combined with SuperPoint , a deep front -end, SuperGlue advances the state -of-the-art on the tasks \\nof indoor and outdoor pose estimation and paves the way towards end -to-end deep SLAM.  \\n \\n \\n \\nQ2. What is MixNMatch?  \\nAnswer:  \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cbf33e2e-ce4b-46e0-925b-dbc4ae3b9cf2', embedding=None, metadata={'page_label': '386', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  4 | 16 \\n \\nIt is a Multifacto r Disentanglement and Encoding  for Conditional Image Generation . Consider the real \\nimage of the yellow bird in the above Fig , First column. What would a  bird look like in a diffe rent \\nbackground, say that of a duck? How about in the  different texture, perhaps that of the rainbow \\ntextured bird in the second  column? What if we wanted to keep its texture but change s its shape to that \\nof rainbow bird and ba ckground and pose to that of  duck, as in the 3rd column? How about sampling \\nshape,  pose, texture, and experience  from 4  different reference images  and combining them to create  \\nentirely new image (last column)  \\nProblem . \\nWhile research in conditional image generation has made tremendous progress , no actual  work can \\nsimultaneously disentang le background , object pose , shape , and  texture  with minimal supervision, so \\nthat these factors can be combined from  multiple real images  for fine -grained controllable image \\ngeneration s. Learning disentangled representations with minimal sup ervision is the extremely \\nchallenging problem since the underlying factors that give rise to the data are often highly correlated and \\nintertwined. Work that disentangle s two such factors, by taking as input 2 reference images , e.g., one for \\nappearance and another for pose, do exist  [huang -eccv2018 , joo-cvpr18 , lee-eccv18 , lorenz -\\ncvpr2019 , xiao-iccv2019 ], but they cannot disentangle other factor  such as pose vs.  shape or foregro und \\nvs. background appearance . Since only two factors can be controlled, these ap proaches cannot arbitrarily \\nchange,e.g. , the object’s background, shape, and texture, while keeping its pose the same. Others require \\nintense  supervision in the form of keypoint or pose or mask annotations  [peng -iccv2017 , Balakrishnan -\\ncvpr2018 , ma-cvpr2018 , esser -cvpr2018 ], which limit  their scalability and still fall s hort of disentangling \\nall of  four factors outlined above.  \\nOur proposed conditional generative model,  MixNMatch , aim  to fill this void. MixNMatch learns to \\ndisentangle and encode ba ckground, object pose, shape, and texture latent factors from the real images, \\nand importantly, does so with minimal human supervision. This allows, e.g., each factor to be extracted \\nfrom a different actual image, and then combined for mix -and-match image generation; see in above fig.  \\nDuring training, MixNMatch only requires a loose bounding box around the object to the model \\nbackground but requires no other supervision for modeling the object’s pose, shape, and texture.  \\n \\n \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='646b0ded-a0da-4c76-b614-d07e45fd3c08', embedding=None, metadata={'page_label': '387', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  5 | 16 \\n \\n \\nQ3. FAN: Feature Adaptation Network  \\nAnswer:  \\n \\n \\nFigure : Visual results on  four datasets.  Vertically we show input in row  first and our results in row  third . \\nFor LFW and SCface  datasets, we show the ground truth and gallery images in second row , respectively. \\nFor WIDER FACE and QMUL -SurFace datasets which do not have ground truth high -resolution images, \\nwe compare with two state -of-the-art(SOTA) methods: Bulat et al.  [bulatyang2 018learn ] and \\nFSRGAN  [CT-FSRNet -2018 ] in row  2, respectively.  \\nIt is used  for Survei llance Face  Recognition and Normalization . Surveillance Face Recognition (FR) is \\na challenge and a signific ant problem yet less studied. The performance on conventional benchmarks \\nsuch as LFW  [LFWTech ] and IJB -A have been greatly improved by state -of-the-art (SOTA) (Face \\nRecognition( FR) methods  [wang2018cosface , wen2016discriminative , deng2019arcface ], which s till \\nsuffer when applied to surveillance Face Recognition( FR). One intuitive approach is to perform Face \\nSuper -Resolu tion (FSR) on surveillance face  to enhance facial details. However, existing Face Super -\\nResolution( FSR) methods are problematic to handle surveillance faces, because they usually ignore \\nthe identity  information and require  to paired  training data. Preserving identity information is mor e \\ncrucial for surveillance of all face  than recovering other information,  e.g., background, Pose, \\nIllumination, Expression (PIE).  \\nIn this work, we study surveillance face recognition (FR) and norma lization. Specifically, given the \\nsurveillance face image, we aim to learn robust identity features for Face recognition( FR). Meanwhi le, \\nthe feature  are used to generate a normalized face with enhanced facial details and neutral PIE. Our \\nnormalization is performed mainly on the aspect of the resolution. While sharing  same goal as traditional \\nSR, it differs in removing the pixel -to-pixel  correspondence between  original and super -resolved images, \\nas required by conven tional SR. Therefore, we term it as face  normalizatio n. For  same reason, we \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='01023637-bbdd-4521-b67f-8d1fa6372a47', embedding=None, metadata={'page_label': '388', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  6 | 16 \\n \\ncompare ours to FSR  instead of prior normalization methods operating on pose  or expression. To the \\nbest of our knowledge, this is a first work to study surveillance face normalization.  \\nWe propose the  novel Feature Adaptation Network (FAN)  to jointly perform face recognition and \\nnormalization, which has 3 advantages over conventional  FSR.  i) Our joint learning scheme can benefit \\neach other , while most  FSR methods do not consider a  recognition task.  ii) Our framework enables \\ntraining with both paired and unpaired data while conventional SR methods only support paired \\ntraining.  iii) Our approach  simultaneously improves  resolution and alleviates the background and PIE \\nfrom real surveillance faces while traditional methods only act on  recommenda tion. Examples in below \\nFig. One demonstrate s the superiority of FAN over SOTA SR methods.  \\n \\nOur Feature Adaptation Network  (FAN ) consists of 2 stages. In  first stage, we adopt disentangled feature s \\nlearning to learn s both identity and non -identity characteristics mainly from high -resolution(HR) images, \\nwhich are combined as input to the decoder for pi xel-wise face recovering. In  second stage, we propose \\nfeature adaptation to f acilitate the feature further  learning from the low -resolution ( LR) images by \\napproximating  feature distribution between th e low-resolution  and high-resolution  identity encoders. \\nThere are two advantages to use Feature Adaption Network( FAN ) for surveillance facial -\\nrecognition( FR) and normalization. First,  Feature Adaption Network ( FAN ) focuses on learning \\ndisentangled identity  features from Low-resolution( LR) images, which is better for facial recognition \\n(FR) than extracting features from super -resolved \\nfaces  [tran2017disentangled , zhang2018facesr , wu2016j ]. 2nd , our  adaptation is performed in \\ndisentangled identity feature space, which enables training with unpaired data without pixel -to-pixel \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d182b2b0-6ccc-47ec-af00-1d4721046ed0', embedding=None, metadata={'page_label': '389', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  7 | 16 \\n \\ncorrespondence s. As shown in the last fig. , the synthetic paired data used in prior \\nworks  [CBN_ECCV16 , CT-FSRNet -\\n2018 , bulatyang2018learn , wu2016j , zhang2 018facesr , DRRN , MemNet_ICCV17 , rad2019srobb ]can \\nnot accurately reflect  difference between real low-resolution( LR) and high-resolution( HR in -the-wild \\nfaces, which is also observed in  [cai2019toward ]. \\nFurthermore, to better handle surveillance faces with  the unknown and diverse resolution, we propose \\nthe Random Scale Augmentation (RSA) method that enables the network to learn all kinds of scales \\nduring training. Prior FSR  [CT-FSRNet -2018 , CBN_ECCV16 , URDGN_ECCV16 ] methods \\neither  artificially  generate the LR images from the HR ones by simple  down -sampling , or learn  the \\ndegradation mapping via a Convolutional Neural Network (CNN). However, their common drawback is \\nto learn reconstruction under  fixed  scales, which may greatly limit their applications to surveillance faces. \\nIn contrast, our RSA efficiently alleviates the constraint on scale variation.  \\nQ5. WSOD with PSNet and Box Regression  \\nAnswer:  \\nThe obje ct detection task is to find  objects belonging to specified cl asses and their locations in  images. \\nBenefiting from the rapid development of deep learning (DL)  in recent years, the fully supervised object \\ndetection task has made si gnificant progress. However, fully supervised task requires instance -level \\nannota tion for training, which costs  lot of time and resources. Unlabeled or image s labeled datasets \\ncannot be effectively used by fully supervised method. On  another hand, image -level annotated datasets \\nare easy to generate and can even be automatically generated by web search engines. To effectively \\nutilize these readily available datasets, we focus on weakly -supervised object detection(WSOD)  tasks. \\nThe WSOD task only takes  imag e-level annotations to train  instance -level object detection network, \\nwhich is different from the fully supervise d object detection task.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b2318aae-b56d-4b23-99c4-b808c8c80aa9', embedding=None, metadata={'page_label': '390', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  8 | 16 \\n \\n \\nFig.: Examples of PSNet outputs: ( i) a do g without proposal occlusion, (ii ) a dog whose head is occluded \\nby the proposal box, (iii ) a dog that proposal  covers part of the body, and (iv)  proposal completely cover \\nthe entire dog. If proposal does not completely include  the whol e dog, PSNet gives a high score. If \\nproposal ultima tely consists of  the whol e dog, PSNet gives a low score.  \\nThere are 3 main methods for weakly supervised object det ection: The fir st is to update detector and \\npseudo labels from inaccurate pseudo labels iteratively ; The second is to construct an end -to-end network \\nthat can take image -level annotation  as supervision to train this object detection network. The third two -\\nstage method is that taking an algorithm to optimize pseudo labels from other WSOD network s and \\ntraining a fully supervised object detection network. In addition, according to different m odes of \\nproposing proposals, each of  above methods can be divided int o 2 classes: one is to propose proposals \\nbased on feature map that predicts probability of each pixel belonging to each class, and then get the \\npossible i nstances and their locations in image; The second is de tector -based method that uses the trained \\ndetec tor to identify multiple proposals and determine s whether each proposal belongs to a specific object \\nclass or not. Comparing the effects of these methods, the end -to-end detector -based approach perform s \\nwell, and our wor k follows this series of process . \\nThe earliest end -to-end detector -based WSOD network is WSDDN  Bilen and Vedaldi ( 2016 ), which \\ntrains a two -streams network to predict the classification accuracy of each proposal and its contri butions \\nto each class. The results of the two streams are combined to get the image classification score  so that  \\nthe WSDDN can take advantage of image -label annotations for training. Subsequen t other work aims to \\nimprove  performance of this network, like a dding more classification streams, using the clustering \\nmethod, adding a fully supervised module, and so on. The end -to-end detector -based approach has 2 \\ndrawbacks: one is that context information cann ot be fully used to classify  proposal; The second is th at \\nthe most discriminative parts of the object may be detected instead of the entire object.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b38ad25a-a0aa-4b5d-ad72-12c1d49634ea', embedding=None, metadata={'page_label': '391', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  9 | 16 \\n \\n \\n \\n \\nTo make full use of the context information of the proposal and avoid finding only the most \\ndiscriminative part, we design a new network structure that add s a box re gression branch to the traditional \\nWSOD network. In the previous WSOD network, there is usually no box regression part, while this \\nbranch plays an essential  role in fully supervised object detection networks. The box regression network \\ncan adjust position and scale of proposal, make it closer to the ground truth. In the fully supervised object \\ndetection task, we can use the instance -level l abel as supervision to train  box regression network; but in \\nWSOD task, network cannot obtain the instance -level annotation and thus cannot train this branch. To \\nobtain reliable instance annotation  to train the r egression network, we designed the  proposal scoring \\nnetwork named P SNet that can detect whether  proposal completely covers the object. Th e PSNet is \\nspecial ly trained multi -label cl assification network. Even if the  object in the image is occluded or \\nincomplete, the PSNet can detect the presence of the object. The PSNet can be used to evaluate image s \\nwithout  proposal area. If the proposal  completely covers who le object,  rest of the image will not contain \\ninformation about it. We use  PSNet to evaluate the output of the WSOD network, and then select \\nappropriate proposals as pseudo labels to train box regression network. Examples of the outp ut of PSNet \\nare shown i n the above Figure . \\n \\nQ6. Autonomous Driving Assistance Systems (ADAS) and Vehicle \\nAutomation . \\nAnswer:  \\nVehicles are being equipped with increasingly complex autonomous driving assistance systems (A DAS) \\nthat take over parts of  driving task s previously performed by the human driver. There are several  different \\nADAS technologies in vehicles, starting from basics that have been in vehicles for several years, such as \\nautomatic windscreen wipers and anti -lock braking systems. More advanced techn iques are already on \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dfc8307b-c2a9-48b8-ab68-7ae31cde4201', embedding=None, metadata={'page_label': '392', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  10 | 16 \\n \\nthe road today, where both the longitudinal (braking/accelerating, e.g. , adaptive cruise control) and lateral \\n(steering, e.g. , assisted lane -keeping) control of the vehicle is shifting to ADAS. Further enhanced levels \\nof automated drivi ng functionality include autopilot (Tesla), intellisafe (Volvo), and Distronic plus \\nsteering assist (Mercedes). Overall this fast pace of market penetration of ADAS in vehicles has n ot \\nallowed drivers to develop  understanding of new systems over an extende d period.  \\nThe most common taxonomy to capture the development of ADAS technology in cars are SAE’s levels \\nof automation  sae. This approach is based on six levels of automation , ranging from no automation (level \\n0) to full automation (level 5). In particular, in levels 2/3, the automated system  can take partial control \\nof vehicle, where level 2 expectations of the human driver are to monitor the system and intervene \\nappropriately , while the level 3 expectation of the human driver is to intervene appropriately upon a \\nrequest from the system. Today most ADAS technology equipped cars are at level 1, in which \\nprogression to partial/semi -automation (level 2/3) with in -built ADAS techno logy in even lower -priced \\ncar models is becoming more common. Also , level 2 /3 automation will likely be  reality for some time to \\ncome, given that fuller automation ( 4/5) is emerging slowly without clear market deployment roadmap.  \\nOne of  main challenges tha t arise  in level 2/3 automation is transition of control from the ADAS to the \\nhuman driver, often referred to as the “handover problem .” This transition is, according to social  factors \\nand safety research, a phase where human attention and reliability is c ritical, but where humans tend to \\nunderperform in those respects  son2017situation . E.g. , research has indicated that automatic cruise \\ncontrol technology leads to a reduction in mental workload and , thus, to problems with regaining control \\nof the vehicle in  failure scenarios  stanton1998vehicle . Additionally, a common misconception \\nconcerning ADAS technology is that when more automation is introduced, human error will \\ndisappear  atlantic2015save , which may give rise to the problematic idea that driver training  is not \\nnecessarily needed. However, social  factors research advises against not training for the use of new \\nsophisticated  automation technology  lee2006human ; salas2006design ; saetren2015effects , as humans in \\nthe technology loop will still be needed for use, maintenance or design of the technology. It may even be \\nthat increased automation increases the level of competence require d for  the driver, as the  driver must \\nknow both how to handle  system manually, for instance , if the sensors in a car stop working  due to bad \\nweather, in addition to knowing how to control  and supervise the advanced automation technology.  \\nIn our previous work  rismani2018qualitative , we performed a qualitative survey and found that the \\nhandover problem is challenging , and it is unclea r to drivers how this could best be handled securely . \\nFurthermore , drivers were worried about the implications of vehicle automation due to lack of knowledge \\nand experience of level 2/3 systems and seemed concerned about the kind of training and licensing that \\naccompanies these developments in vehicle automation. The lack of certainty around tra ining and \\nlicensing concerning  emerging ADAS technologies is a relevant ethical concern, as it exposes a gap in \\nregulation and industry best practices that ha ve not been the focus of much research to date.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ea4270d9-fa08-4770-a7ba-40a2b0daa34e', embedding=None, metadata={'page_label': '393', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  11 | 16 \\n \\nThis lack of certainty around driver training and licensing wrt level 2/3 automation systems underscores \\nthe need to understand better  the following research questions: (i ) What are drivers’ awarene ss of ADAS \\nin the ir vehicles, (ii ) How knowledgeable are drivers abou t ADAS in their vehicles, and (iii ) How willing \\nare drivers to engage or use ADAS in their vehicles? Overall we expect to see peo ple’s engagement or \\nuse pattern  of ADA S technologies in their vehicle  corre late to their awareness and  knowledge of those \\ntechniques . \\nPrevious work has looked at driver perception  of ADAS and vehicle automation , including understanding \\nlearner drivers’ perspective of Blind Spot Detection (BSD)  and Adaptive Cruise Control (ACC)  systems. \\nThat work found that driver’s  awareness, use, and perceived safety of Blind Spot Detection( BSD ) was \\nhigher than that of ACC  tsapi_introducing_2015 , and contributed to a greater understanding of driver \\npreparation and acceptance of ADAS  crump2016differing , and how drivers learn and prefer to learn \\nabout ADAS, and what their expectations are regarding ADAS and vehicle \\nautomation  hoyos2018consumer . \\nTo answer our research questions , we performed a quantitative public survey of issues specifi c to the \\npublic’s awareness, knowledge , and use of ADAS technologies in level 2/3 automation. Also , based on \\nprevious work  tsapi_introducing_2015 ; crump2016differing ; hoyos2018consumer , we analy zed gender \\nand age relationships as well as income and type of  training with regards to our research questions above.  \\nQ7. Robot Learning and Execution of Collaborative Manipulation Plans \\nfrom YouTube Videos . \\nAnswer:  \\n We focus on  problem of learning collaborative action plans for  robot. Our goal is to have  robot “wa tch” \\nunconstrained videos on  web, extract the action sequences shown in the videos and convert them to an \\nexecutable plan that it can perform either independently or as part of a human -robot or robot -robot team.  \\nLearning from online videos is har d, particularly in collaborative settings: it requires recognizing the \\nactions executed, together with manipulated tools and objects. In many collaborative tasks , these actions \\ninclude ha nding objects over or holding  object for the other person to manipulate. There is a very large \\nvariation in how the actions are performed and collaborative actions may overlap spatially and \\ntemporally.  \\nIn our previous work  [hejia_isrr19 ], we proposed a system for learning activities p erformed by two \\nhumans collaborating at the  cooking task. The system implements a collabor ative action grammar built \\nupon action grammar initially proposed by Yang et al.  [yang2015robot ]. Qualitative analysis in 12 clips \\nshowed that parsing these clips wit h grammar results in human -interpretable tree structures representing ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6493f3e1-5e7f-4f08-9a9b-3ad8e0ebb5cc', embedding=None, metadata={'page_label': '394', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  12 | 16 \\n \\na variety of single and collaborative actions. The clips were manually segmented and were approximate \\n100 frames each.  \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='170dfba7-0d40-4d34-803a-2f9962dafb8e', embedding=None, metadata={'page_label': '395', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  13 | 16 \\n \\n \\nIn this paper, we generalize this work with a framework for  generating single and collaborative action \\ntrees from full -length YouTube videos lasting several minutes  and concatenating the trees in an action \\ngraph that is executable by one or more robotic arms . \\nThe framework takes as input YouTube video showing  collaborative task s from start to end. We assume s \\nthat objects in video are annotated with label  and bou nding boxes, e.g., by running the YOLOv3 \\nalgorithm. We also think  a skill library that associates a detected action with skill -specific motion \\nprimitiv es. We focus on cooking tasks because of the variety of manipulation actions and their \\nimportance in -home service robotics.  \\nIn second fig.  shows the components of  proposed framework. We rely on insight that hands are  main \\ndrivin g force of manipulation actions. We detect the human hands in the video and use the hand \\ntrajectories to split the video into clips. We then associate objects and hands spatially and temporally to \\nrecognize the actions and generate human -interpretable robo t commands. Finally, we propose an open -\\nsourced platform for creating and executing an action graph. We provide a quantitative analysis of \\nperformance in two YouTube videos of 13401 frames in total and a demonstration in the simulation of \\nrobots learning a nd perform ing the actions of the third video of 2421 frames correctly . \\nWhile the extracted act ion sequences are executed in the  open -loop manner and thus do not withstand \\nreal-world failures or disturbances, we find that this work brings us the step closer  to havi ng robots \\ngenerate and execute  variety of semantically meaningful plans from watching videos online.  \\nQ8. JEC -QA: A Legal -Domain Question Answering Dataset  \\nLegal Question  Answering ( LQA ) aims to provide explanations, advice , or solutions for legal issues. A \\nqualified LQA system can not only demonstrates  a professional consulting service for unskilled humans \\nbut also help professionals to improve work efficiency and analyze real cases more accurately, which \\nmakes LQA an import ant NLP application in the legal domain. Recently, many researchers attempt to \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5cbc28a2-cb75-4fbf-a110-80e240928ada', embedding=None, metadata={'page_label': '396', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  14 | 16 \\n \\nbuild LQA systems with machine learning techniques  and neural network s. Despite these efforts in \\nemploying advanced NLP models, LQA is still confronted with the following two significant  challenges. \\nThe first is that there is less qualified LQA dataset , which limits the research. The second is that the cases \\nand questions in the legal domain are very complex and rigorous. As shown in Table  1, most problem s \\nin LQA can be divided into two typical types: the knowledge -driven questions (KD -questio ns) and case -\\nanalysis questions (CA -questions). KD -questions  focus on the understanding of specific legal concepts, \\nwhile CA -questions  concentrate more on the analysis of real cases. Both types of questions require \\nsophisticated reasoning ability and text comprehension ability, which makes LQA a hard task in NLP.  \\n \\n \\nTo get a better understanding of these reasoning abilities, we show a question of JEC -QA in \\nFig. 1describing a criminal behavior that results in two crimes. The mo dels must understand \\n“Motivational Concurrence” to reason out further  evidence rather than lexical -level semantic matching. \\nMoreover, the models must have the ability of multi -paragraph reading and multi -hop reasoning to \\ncombine the direct evidence and the  additional  evidence to answer the question, while numerical analysis \\nis also necessary for comparing which crime is more danger ous. We can see that answering one question \\nwill need multiple reasoning abilities in both retrieving and answering, makes JEC -QA a challenging \\ntask. \\nTo investigate the challenges and characteristics of LQA, we design a unified OpenQA framework and \\nimplement seven representative neural methods of reading comprehension. By evaluating the \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f6075079-b140-4413-aa1b-2b705affcadc', embedding=None, metadata={'page_label': '397', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  15 | 16 \\n \\nperformance of these methods on JEC -QA, we sh ow that even the best approach  can only achieve \\nabout  25% and 29% on KD -questions  and CA -questions , respectively, while skilled humans and \\nunskilled humans can reach  81% and 64% accuracies on JEC -QA. The experimental results show that \\nexisting OpenQA metho ds suffer from the inability of complex reasoning on JEC -QA as they cannot \\nwell understand legal concepts and handle multi -hop logic . \\n \\n \\nQ9. SpoC: Spoofing Camera Fingerprints  \\nAnswer:  \\n \\nFigure 1 : SpoC  learns to spoof camera fingerprints. It can be used to insert camera traces to a generated \\nimage. Experiments show that we can fool state -of-the-art camera -model identifiers that were not seen \\nduring training.  \\n \\nFigure 2 : A digital image of a scene contains camera -related traces of the image formation process that \\ncould act as a fingerprint of a camera model. The used lenses and filters, the sensor , and the manufacturer -\\nspecific digital processing pipelines result in unique patterns. These patterns c an be used to identify \\ncamera models.  \\nThere have been astonishing advances in synthetic media generation in the last few years, thanks to deep \\nlearning, and in particular to Generative Adversarial Networks (GANs). This technology -enabled a \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dbc4de72-8345-454d-ac0c-82021e75c836', embedding=None, metadata={'page_label': '398', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  16 | 16 \\n \\nsignificant impr ovement in the level of realism of generated data, increas ing both resolution and quality . \\nNowadays, powerful methods exist for creating an image from scratch, and for changing its style  or only \\nsome specific  attributes . These methods are beneficial , espec ially on faces, and allow one to change the \\nexpression of a person easily  or to modify its identity through face -swapping . This manipulated visual \\ncontent can be used to build more effective fake news. It has been estimated that the average number of \\nrepos ts for a report  containing an image is  11 times larger than for those  without images . This raises \\nserious concerns about the trustworthiness of digital content, as testified by the growing attention to the \\nprofound fake phenomenon.  \\nThe research community h as responded to this threat by developing several  forensic detectors. Some of \\nthem exploit high -level artifacts, like asymmetries in the color of the eyes, or anomalies arising from an \\nimprecise estima tion of the underlying geometry . However, technology improves so fast that these visual \\nartifacts will soon disappear. Other approaches rely on the fact that any acquisition device leaves \\ndistinctiv e traces on each captured image , because of its hardware, or its signal processing suite. They \\nallow  associating a media with its acquisition device at various levels, from the type of source (camera, \\nscanner, etc.), to its brand/model (e.g. , iPhone6 vs . iPhone7), to the individual device . A primary  impulse \\nto this field has been given by the seminal wor k of Lukàs et al., where it has been shown that reliable \\ndevice identification is possible based on the camera photo -response non -uniformity (PRNU) pattern. \\nThis pattern is due to tiny imperfections in the silicon wafer used to manufacture the imaging sens or and \\ncan be considered as a type of device fingerprint.  \\nBeyond extracting fingerprints that contain device -related traces, it is also possible to re cover camera \\nmodel fingerprints . These are related to the internal digital acquisition pipeline, including  operations like \\ndemosaicing, color balancing, and compression, whose details differ according to the brand a nd specific \\nmodel of the camera  (See Fig. 2). Such differences help attribute images to their source camera, but can \\nalso be used to highlight better  anomalie s caused by image manipulations . The absence of such traces, \\nor their modification,  is a strong clue that the image is synthetic or h as been manipulated in some way . \\nDetection algorithms, however, must confront with the capacity of an adversary to fool them. This applies \\nto any classifier and is also very well known in forensics, where m any counter -forensics methods have \\nbeen proposed in the literature . Indeed, forensics and counter -forensics go hand in hand, a competition \\nthat contributes to improving the level of digital integrity over time.  \\nIn this work, we propose a method to synthesi ze traces of cameras using a generative approach that is \\nagnostic to the detector (i.e., not just targeted adversarial noise). We achieve this by training a conditional \\ngenerator to jointly fool an adversarial discriminator network as well as a camera embe dding network. \\nTo this end, the proposed method injects the distinctive traces of a target camera model in synthetic \\nimages, while reducing the first generation traces themselves, leading all tested classifiers to attribute \\nsuch images to the target camera  (’targeted attack’).  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='44989d2e-7b12-4190-9a9b-572563e6fa41', embedding=None, metadata={'page_label': '399', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  1 | 17 \\n \\n \\nDATA SCIENCE  \\nINTERVIEW  \\nPREPARATION  \\n(30 Days of Interview Preparation)  \\n                              \\n \\n \\n#Finale Day30  \\nMost important questions  \\nRelated to Project  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='81fe7248-42af-4321-a7cf-3418ccdcd6c2', embedding=None, metadata={'page_label': '400', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  2 | 17 \\n \\nDisclaimer : The answers given here are not generic ones. These answers are given based \\non the attendance system that we have developed to do face detection. The answers will vary \\nbased on the projects done, methodologies used and based on the person being interviewed.  \\n \\nFace Recognition and Identification  system  Project  \\n \\nQ1. Tell me about your current project . \\nAnswer:  \\nThe project is called  Attendance System using facial recognition.   \\nThe goal of the pr oject is to identify the person and mark their attendance . First, the user has to \\nregister himself/herself in the application providing the required details. The application takes \\nmultiple sn aps of the user and then stores it into the database. Once the same user comes before the \\ncamera again , the application captures the image, references it against the already stored images in \\nthe database , and then marks the attendance, if the user is prese nt in the database.  Reports can be \\ngenerated for a particular duration based on the user requirement.  \\nSome snaps from the project are as follows:  \\n1st-time registration:  \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='048c5679-4c6b-4065-a2bd-ed3d4ed894ef', embedding=None, metadata={'page_label': '401', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  3 | 17 \\n \\nMarking the Attendance:  \\nWith un -registered user:  \\n \\n \\nWith a registered user:  \\n \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f96ba0e2-f70e-4674-a06a-c7b1c7dadc06', embedding=None, metadata={'page_label': '402', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  4 | 17 \\n \\n \\nSeeing the reports:  \\n \\n \\n \\n \\n Features:  \\n\\uf0b7 Works with generic IP cameras with good quality.  \\n\\uf0b7 Works even with PC, you don’t need high -end systems.  \\n\\uf0b7 Works in both indoor as well as outdoor environments.  \\n\\uf0b7 Works with limited pose changes.  \\n\\uf0b7 Works with spectacles.  \\n\\uf0b7 Works  for people of different ethnicity.  \\n\\uf0b7 Works for tens of thousands of registered faces.  \\n\\uf0b7 Works with limited lighting conditions.  \\n\\uf0b7 Works with partial facial landmarks.  \\n\\uf0b7 Non-recognition of static input images when provided by the user.  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='441ac478-d433-4a70-b612-9e343ce05922', embedding=None, metadata={'page_label': '403', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  5 | 17 \\n \\nFunctionalities in the Atten dance System  \\n\\uf0b7 Registration of users in the system.  \\n\\uf0b7 Capturing the user details during registration using  Passport, Adhar Card, and Pan \\nCard.  \\n\\uf0b7 All details will be extracted using the in -house OCR technique.  \\n\\uf0b7 Tracking of the login and logout timings of the users  from  the system.  \\n\\uf0b7 Generation of user logs on a temporal basis.  \\n\\uf0b7 Generation of timely reports.  \\nDeployment/Installation  \\n\\uf0b7 The application can be easily installed as a web -based API on any cloud platform. This \\ninstallation is similar to a plug and play scenario.  \\n\\uf0b7 The application can also be installed in an edge device (like the Google Coral). This \\ninstallation provides realtime streaming capabilities to the application.  \\nQ2. What was the size of the data ? \\nAnswer:  \\nThe number of images used for training was 12,313.  \\nQ3. What was the data type?  \\nAnswer:  \\nThe data used for training this model consisted of thousands of images ; the images then are converted \\nto tensor objects , which have a float 32 representation.  \\nQ4. What wa s the team size and distribution ?  \\nAnswer:   \\nThe team consisted of:  \\n\\uf0b7 1 Product Manager,  \\n\\uf0b7 1 Solution Architect,  \\n\\uf0b7 1 Lead,  \\n\\uf0b7 2 Dev -Ops engineers,  \\n\\uf0b7 2 QA engineers ,  \\n\\uf0b7 2 UI developers , and  \\n\\uf0b7 3 Data Scientists.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='509eaa79-7b24-4cb7-83ee-d5c746522b33', embedding=None, metadata={'page_label': '404', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  6 | 17 \\n \\n \\nQ5.What Hadoop distribution were you  using ?  \\nAnswer:  \\nThe Hadoop distribution from Cloudera was used  as it provides many of the much -needed capabilities \\nout of the box like multi -function analytics, shared data experience with optimum security and \\ngovernance, hybrid capabilities for support to clouds, on -premise servers as well as multi -clouds.  \\nQ6.What is the version of distribution ?  \\nAnswer:  \\nCDH – 5.8.0  \\nQ7.What was the size of the cluster ?  \\nAnswer:  \\nThe cluster(production setup) consisted of  15 servers with  \\n\\uf0b7 Intel i7 pr ocessors  \\n\\uf0b7 56 GB  of RAM  \\n\\uf0b7 500 GB of Secondary storage each   \\n\\uf0b7 Mounted NAS locations  \\nQ8. How many nodes were there in all the Dev, UAT , and P rod \\nenvironments ? \\nAnswer:  \\nThe necessary  coding was done on one development server. But  as a standalone machine won’t give \\nenough speed to train the model in a short time, once we saw that the model’s loss is decreasing for \\na few numbers of epochs in the standalone machine, the same code was deployed to a cloud -based \\nGPU machine for training . Once the model was trained there, we used the saved model file for \\nprediction/classification. The same model file was deployed to the cloud UAT and Production \\nenvironments.   \\nIn total, we had :  \\n\\uf0b7 5 nodes in the dev environment,  \\n\\uf0b7 5 nodes in UAT , and  \\n\\uf0b7 15  nodes in production.  \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9f456226-4bfd-4bc7-909a-596f4ee94406', embedding=None, metadata={'page_label': '405', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  7 | 17 \\n \\nQ9.How  were  you creating and maintaining the logs?  \\nAnswer:  \\nThe logs are maintained using MongoDB. The logging starts with the start of the application. The \\nstart time of the application gets  logged. After that, there are loggings for entry and exits to the \\nindividual methods. There are loggings for the error scenarios and exception block as well.  \\n \\n \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='223f6219-1e56-4024-af2c-2c13e546060c', embedding=None, metadata={'page_label': '406', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  8 | 17 \\n \\nQ10. What techn iques were you using for  data pre-processing for \\nvarious data science use case s and visualization ? \\nAnswer:  \\nThere are multiple  steps that we do for data preprocessing, like data cleaning, data integration, data \\nscaling, etc. Some of them are listed as follows:  \\n\\uf0e8 For Machine Learning : \\n While preparing data for a model, data should be verified using multiple tables or files \\nto ensure data integrity.   \\n Identifying and removing unnecessary attributes.  \\nFor example,  \\n \\nHere, the user_ID column does not contribute to the customer behavior for \\npurchasing the products. So, it can be dropped fro m the dataset.  \\n \\n Identifying, filling or droping the rows/columns containing missing values based on the \\nrequirement.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='331f6833-874b-4993-8222-3a7beb684270', embedding=None, metadata={'page_label': '407', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\nP a g e  9 | 17 \\n \\n \\nHere, the Product_Category_3 has about 5.5 lac missing values. It can be \\ndropped using the command \\uf0e0 df.drop(‘Product_Category_3’,axis=1, \\ninplace=True)  \\nOr, if the count of null values have been lower, they could have been imputed \\nusing\\uf0e0 \\ndf['Purchase'] = df['Purchase'].fillna(df['Purchase'].mean())  \\n \\n \\n Identifying and removing outliers  \\n \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dbbde5f5-4f0c-495f-84e4-34a76177bdc8', embedding=None, metadata={'page_label': '408', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  10 | 17 \\n \\n In the image above, one point  lies very far from the other data points, i.e., it’s an outlier \\nthat is not following the general trend of the data. So, that point can be dropped.  \\n Based on the requirement, form clusters of data to avoid an overfitted model.  \\n \\nContrary to the example in the previous point , there can be several points that do \\nnot follow a particular pattern or which have a pattern of their own. If those points \\nare too many, they can’t be considered as outliers.  Then we need to consider \\nthose points separately. In that kin d of scenario, we create the clusters of similar \\npoints, and then we try and train our model on those clusters.  \\n Scaling the data so that the difference between the magnitudes of the data points in \\ndifferent columns are not very big.  \\n \\nIn the diagram above,  the magnitude of the values in the ‘Purchase’  column is \\nway larger than the other columns. This kind of data makes our model \\nsensitive. To rectify this, we can do \\uf0e0 \\n# Feature Scaling So that data in all the columns are to the same scale  \\nfrom sklearn.prepr ocessing import StandardScaler  \\nsc = StandardScaler()  \\nX_train = sc.fit_transform(X_train)   \\nAfter scaling the data looks like:  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='74ce2f9a-fc1d-47b9-bd8d-dd2d65c71c06', embedding=None, metadata={'page_label': '409', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\nP a g e  11 | 17 \\n \\n \\n \\n Converting the categorical data into numerical data . \\nFor example, gender data (Male or Female) is a categorical one. It can be \\nconverted to numeric values , as shown below:  \\ndf['Gender']=df['Gender'].map({'F':0, 'M':1})  \\n Replacing or combining two or more attributes to generate a new attribute which serves \\nthe same purpose.  \\nFor example, if we use one -hot encoding in the example above, it will generate \\ntwo separate columns for male s and female s. But if we observe, a person who is \\nnot a male is automatically a female(if we co nsider only two genders). So, the \\ntwo columns essentially convey the same information in that case. This is called \\nthe dummy variable trap . So, one column can be conveniently dropped.  \\n \\n Trying out dimensionality reduction techniques like PCA(Principal Component \\nAnalysis), which tries to represent the same information but in a space with reduced \\ndimensions.  \\n \\n\\uf0e8 For Deep Learning:  \\n Data augmentation strategies followed by image annotation. Data augmentation \\nconsists of image rota tion, contrast, and color adjustments, lighting variations, \\nrandom erasing, etc.  \\n Then all the images are made of identical size.  \\n Then image annotation is done.  \\n \\n \\n \\n \\n \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d5a5b5d7-fd48-4625-abc7-f7f950b56f6c', embedding=None, metadata={'page_label': '410', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  12 | 17 \\n \\nQ11. How were you  maintaining the failure cases ? \\nAnswer:  \\nLet’s say that our model was not able to make  a correct prediction for an image. In that case, that \\nimage gets stored in the database. There will be a report triggered to the support team at the end of \\nthe day with all the failed scenarios where they can inspect the cause of failure. Once we have a \\nsufficient number of cases, we can label and include those images while retraining the model for \\nbetter model performance.  \\nQ12. What kind of automation  have  you done for data processing ? \\nAnswer:  \\nWe had a  full-fledged ETL pipeline in place for data extraction . Employers already have images of \\ntheir employees. That data can be easily used after doing pre -processing for training the image \\nidentification model.  \\n \\nQ13. Have you used any scheduler?  \\nAnswer:  \\nYes, a scheduler was used for ret raining the model after a fixed time (20 days) .  \\nQ14. How are you  monitoring your job ? \\nAnswer:  \\nThere are logging set -ups done. We regularly monitor the logs to see for any error scenarios. For fatal \\nerrors, we had email notifications in place. Whenever a specific  error code, which has been classif ied \\nas a fatal error occurs, email gets triggered to the concerned parties.  \\nQ15. What w ere your roles and responsib ilities  in the project ? \\nAnswer:  \\nMy responsibilities consisted of gathering the dataset, labeling  the images for the model training, \\ntraining the model on the prepared dataset, deploying the trained model to the cloud, monitoring the \\ndeployed model for any issues , providing QA support before deployment and then providing the \\nwarranty support post -deployment.  \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fb407b8c-f6a0-46a1-9406-77151119bc7b', embedding=None, metadata={'page_label': '411', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  13 | 17 \\n \\nQ16. What was  your day to day task ? \\nAnswer:  \\nMy day to day tasks involved completing the JIRA tasks assigned to me, attending the scrum \\nmeetings, participating in design discussions and requirement gathering, doing the requirement \\nanalysis, data vali dation, image labeling, Unit test for the models, providing UAT support , etc. \\nQ17. In which area you have contributed the most ? \\nAnswer:  \\nI contributed the most to image labeling and model training areas.  Also, we did a lot of brainstorming \\nfor finding and selecting the best algo rithms for our use cases.  After that, we identified and finalized \\nthe best practices  for implementation , scalable deployment  of the model , and best practices for \\nseamless deployments as well.  \\n \\nQ18. In which tech nology you are most  comfortable?  \\nAnswer:  \\nI have worked in almost all the fields viz. Machine Learning, Deep Learning, and Natural Language \\nProcessing , and I have nearly equivalent knowledge in these fields. But  if you talk about personal \\npreference, I have loved working in Deep Learning and NLP the most.  \\n \\nQ19. How you rate yourself in big data technology ? \\nAnswer:  \\nI gave worked often in the big data computing technology with ample knowledge in distributed and \\ncluster -based computing. But my focus and extensive contribution have been as a data scientist.  \\n \\n \\nQ20.  In how many projects you have already worked ? \\nAnswer:  \\nIt’s difficult to give a number. But I have worked in various small and large scale projects , e.g., object \\ndetection, object classification, object identification, NLP projects, chatbot building, machine \\nlearning regression , and classification problems.  \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6c014325-6560-409d-be9e-86c76bc20b6b', embedding=None, metadata={'page_label': '412', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\nP a g e  14 | 17 \\n \\nQ21. How were you  doing deployment ? \\nAnswer:  \\nThe mechanism of deployment depends on the client 's requirement. For example, some clients want \\ntheir models to be deployed in the cloud , and the real -time calls they take place from one cloud \\napplication to another. On the other hand, some client s want an on -premise deployment , and then \\nthey do API calls to the model. Generally, we prepare a model file first and then try to expose it \\nthrou gh an API for predictions/classifications. The mechanism in which he API gets called depends \\non the client requirement.  \\nQ22. What kind of challenges  have  you faced  during the project ? \\nAnswer:  \\nThe biggest challenge that we face is in terms of obtaining a good dataset, cleaning it to be fit for \\nfeeding it to a model , and then labeling the prepared datasets. Labeling is a rigorous task and it burns \\na lot of hours.  Then comes the task of finding the correct algorithm to be used for that business case. \\nThen th at model is optimized. If we are exposing the model as an API, then we need to work on the \\nSLA for the API as well, so that it responds in optimum time.  \\n \\nQ23. What will be your expectations ? \\nAnswer:  \\nIt’s said that the best learning is what we learn on the job with experience. I expect  to work on new \\nprojects which require a broad  set of skills so that I can hone my existing skills and learn new things \\nsimultaneously.  \\n \\nQ24. What is your future objective ? \\nAnswer:  \\nThe field of data science is continuously changing. Almost daily, there is a research paper that \\nchanges the way we approach an AI problem. So, it really makes it exciting to work on things that \\nare new to the entire world. My objective is to learn  new things as fast as possible and try and \\nimplement that knowledge to the work that we do for better code, robust application and in turn, a \\nbetter user/customer experience.  \\n \\n \\n \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ed114138-9b74-416c-8f86-077cb858e634', embedding=None, metadata={'page_label': '413', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  15 | 17 \\n \\nQ25. Why are you  leaving you r current organi zation ? \\nAnswer:  \\nI was working on similar kinds of proje cts for some time now. But the market is rapidly changing, \\nand the skill set required to be relevant in the market is changing as well. The reason for searching a \\nnew job is to work on several kinds of projects and improve my skill se t. <Mention about the \\ncompany profile and if you have the project name that you are being interviewed for as new \\nlearning opportunities for you> . \\n \\nQ26. How did you do Data validation ? \\nAnswer:   \\nData validation is done by  looking at the images gathered . There should be ample images for the \\nvaried number of cases like change in the lighting conditions, distance from the camera, movement \\nof the user, the angle at which camera is installed, the position at which the camera is installed, the \\nangle at which the sna p of the user has been taken, the alignment of the image, the ratio of the face \\nand the other areas in the image etc . \\n \\nQ27. How did you do Data  enrichment ? \\nAnswer:  \\nData enrichment in vision problems mostly consists of image augmentation. Apart from image \\naugmentation, we tried to train the model with images with different lighting conditions, with b/w \\nand colored images, images from different angles , etc.   \\n \\nQ28. How would you rate yourself in machine learning?  \\nAnswer:  \\nWell, honestly , my 10 and your 10 will be a lot different as we have dif ferent kinds of experiences. \\nOn my scale of 1 to 10 , I’ll rate myself as an 8.2.  \\n \\nQ29. How would you rate your self  in distributed computation ?  \\nAnswer:  \\nI’d rate myself a 7.7 out of 10.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8fd177c0-d13c-4f9d-a7c2-f73bec904267', embedding=None, metadata={'page_label': '414', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  16 | 17 \\n \\nQ30. What are the areas of machine learning algorithm s that you  \\nalready  have  explored ? \\nAnswer:  \\nI have explored various machine learning algorithms like Linear Regression, Logistic Regression, L1 \\nand L2 Regression, Polynomial Regression, Multi Linear Regression, Decision Trees, Random \\nForests, Extra Trees Classifier, PCA, TSnE, UMAP, XG Boost, CAT Boos t, ADA Boost, Gradient \\nBoosting, Light Boost,  K-Means, K-Means ++,LDA, QDA,  KNN, SVM, SVR, Naïve Bayes , \\nAgglomerative clustering, DBScan , Hierarchical clustering, TFIDF, Word to Vec, Bag of words, Doc \\nto Vec, Kernel Density Estimation  are some of them.  \\n \\nQ31.  In which part of machine learning  have  you already worked \\non? \\nAnswer:  \\nI have worked on both supervised and unsupervised machine learning approaches and building \\ndifferent models using the as per the user requirement . \\nQ32. How did you optimize  your solution ? \\nAnswer:  \\nWell, model optimization depends on a lot of factors.  \\n\\uf0b7 Train with better data(increase the quality), or do data pre -processing steps more \\nefficiently.  \\n\\uf0b7 Keep the resolution of the images identical.  \\n\\uf0b7 Increase the quantity of data used for training.  \\n\\uf0b7 Increase the number of epochs for which the model was trained  \\n\\uf0b7 Tweak the batch input size, the number of hidden layers, the learning rate,  rate of \\ndecay , etc. to produce the best results.  \\n\\uf0b7 If you are not using transfer learning, then you can alter the numbe r of hidden layers, \\nactivation function.  \\n\\uf0b7 Change the function used in the output layer based on the requirement. The sigmoid \\nfunctions work well with binary classification problems, whereas for multi -class \\nproblems, we use a sigmoid model.  \\n\\uf0b7 Try and use multi threaded approaches, if possible.  \\n\\uf0b7 Reduce Learning Rate in plateau reasons optimizes the model even further.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7b023ecf-63ec-496d-a5f2-123d1e8dd0c6', embedding=None, metadata={'page_label': '415', 'file_name': 'Interview preparation .pdf', 'file_path': '/content/Data/Interview preparation .pdf', 'file_type': 'application/pdf', 'file_size': 26862185, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02', 'last_accessed_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\nP a g e  17 | 17 \\n \\nQ33. How much time did your model take  to get trai ned? \\nAnswer:  \\nWith a batch size of 128 and the number of epochs 100000 with 7000 images,  it took around 110 \\nhours  to train the model using Nvidia Pascal Titan GPU.  \\n \\nQ34. At what frequency  are you retraining and updating your \\nmodel ? \\nAnswer:  \\nThe model gets retrained every 20  days.  \\n \\nQ35. In which mode  have  you deployed your model ? \\nAnswer:  \\nI have deployed the model both in cloud envi ronments as well in the on -premise ones based on the \\nclient and project requirements.  \\n \\nQ36. What is your area of speciali zation in machine learning ? \\nAnswer:  \\nI have worked on various algorithms. So, It’s difficult to point out one  strong area . Let’s have a \\ndiscussion on any specific requirement that you have, and then we can take it further from there.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"You are a Q&A assistant. Your goal is to answer questions as\n",
        "accurately as possible based on the instructions and context provided.\"\"\"\n",
        "\n",
        "\n",
        "# This will wrap the default prompts that are internal to llama-index\n",
        "query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")"
      ],
      "metadata": {
        "id": "-QsIE7rmS9ou"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPC9q1eBS_cj",
        "outputId": "ef382752-9456-43a3-8bb4-26a99475a97f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: read).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    device_map=\"auto\",\n",
        "    # uncomment this if using CUDA to reduce memory usage\n",
        "    model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567,
          "referenced_widgets": [
            "7cbbe9e549bd49159be82cc2d5bba379",
            "6268df3dce414dd89a447af4da09b590",
            "3ce98b861123405fba6b5b4cb70caa2d",
            "1183d4b3680e4f7faeac601da5f00bc0",
            "f7bcbcc5e09e4dcba6c1c02722a5dc67",
            "967e200267ac493c96188c3c8019a739",
            "8ba2542b120641bfaa40d41f8f19b49a",
            "b24a4c94ecff467482511cd27a67602f",
            "618741b0cd22419aa4e14d198867a6e4",
            "df62a5c02d514fa2a53043d5024ff434",
            "8128bbfb2e3542c68ac19cb04a8b237b",
            "3aebbeacf5e04e13851161c911f25169",
            "912c73b0844e4ce696e6898ee3472570",
            "3deb489a669e4dbeba1a821ae1d314f0",
            "01e3c80f29834de78859d54726383c4a",
            "7fc4c920fe1d429a93410ab927630a1d",
            "48bb0dc16ee446a0a334f00e0b247dba",
            "e8a374b9bef04b22ab7049c0055684e0",
            "f7e35cd39480477e9a277d7092eca4f0",
            "5204421c1ee942d9bd6552bff668612b",
            "c7a0089e51c24a0ea32ebc672217df35",
            "a334fc0e5e2241a6845158b23e24a4a5",
            "cada55b815714a13b94d20d58271061f",
            "013c0bcdacc84da99870533d581c1768",
            "e2b85ef9df43481d8c6bec58fdd92e9a",
            "84d22453b3984b18936f2a34b34dba7d",
            "9403214d851a4c6c8eff2dd0e46004ad",
            "6cf8174e32c1425d83bc9af0e34f4e17",
            "c07220e54b694c4cb05c455ab17684a8",
            "3e2e6386d7724308b3b9e4d1eaded719",
            "5bfb3f09f3e045e38605c969e90a47be",
            "c2828b5e42f9475b9ccb7fc9fd574596",
            "577f6cce8fa543e1b406de9a208cbe61",
            "8a5612c45a3e42548c220368f036174a",
            "446e96d4d7214553b03b243b5f39819f",
            "9fea9ace7fb649be888619958a5f14c4",
            "6f03570810284c62ba6c14f680e6cc8a",
            "6968bbb2ff434bea8be81cbe54748cba",
            "3ecf381e226749e8bce8a4dde52c0644",
            "7e15d664ca074a7ab6123d2f699f7c51",
            "3fa3a152edfc4801a8574086d981bcc8",
            "b1139674596f4a08b610b135f762d9ad",
            "d9590ef6708c4d98b33ef6ab09b3dccd",
            "8d20c314b8f34c2cb60d51e2713da621",
            "4292d1ae31be4a22bec049849b393d25",
            "984cf51a1f5d415cb540e5375922308d",
            "3c75c2291d6d44bdb9ea9c8730edc699",
            "416c279d22814cb3ac8518db4a18efbf",
            "a7388451dcc246b191cd4af3650510d5",
            "33bd8f3e20484b91b263f1e7591e10d4",
            "0db6b1d8b9c44200a20b5c87bb5ad2c4",
            "3dad41b454414cf285fe46ad1bbabdad",
            "fcbc7cdb377f4227b3dfc5a5eac084f9",
            "e42b7a91f5db457cab7dfc89f28016d3",
            "a7759ab63ace477b8778468fbdc762a8",
            "dc97d12f3e814cc2a437cc5ad69699e6",
            "5ddbbbc633e54d74b1581f8323435238",
            "9485b734e8454e5582928bd94506811b",
            "75a010d615fe41d2a2ad856b2f212e05",
            "9e75032464c341738c6f6ea01f32887c",
            "04594ad2f37b4c0ebe133c74760cee33",
            "dee14fdc02e7497ab575b06896be5efa",
            "3534393dc1234c1594d6ac84b266a1f4",
            "ebca62fa14de4e93b3b26f08caff5c48",
            "65cee2e9c6514d93a217e2afdbb61ad8",
            "1f7bbd4a3971437f85310246204eece2",
            "3def84fdcf5b471695488cef56714e02",
            "474d8a2baf1946768c4779e8098d022c",
            "69fd5823ab42459b827035f2c88289da",
            "18fbc696b67149ae9a98ec2e7e57574c",
            "5d843f4e673247acb6835227a34da926",
            "c444b8cded324b29aef02a5ac81995a9",
            "a36e49507977421f949785cba5f8178a",
            "99763dc642134a1493952ad5d7c81e69",
            "6e64e06556754a49a12969dee9b64599",
            "62ec4c13bccb431290c192bb9132cfed",
            "69b891e613994f4b81bda14c7f5b4b82",
            "cdee1c16a95c403c80b6c2f2fba17c79",
            "a6c5c87564bc4713863aa7db3e128f56",
            "dc6bf8d3c8114941908f1e4d5ad97910",
            "29423beb8fb94e5580b8348d5742d88b",
            "0e8a4fc8c5164977914a51e7d463bff8",
            "194cac9e751549eea840cffdc6460149",
            "2c797bc18da34b2d8a1cb30dcc9e1647",
            "1382fec77fcc4478ab6f8862a04bd6cc",
            "5dbcf5586a3442da8bb30e5d2ee0adc0",
            "3f2c1ce527c243c0b21495e97f32aa06",
            "033b67383235422eac4a43e2355722c0",
            "6485864cad0b47c8b67e45a11fecd7d9",
            "2c2d1a34dfa5402dbeb0752d76d8ee42",
            "4cfb4c3b07e54394afec57afa06e5666",
            "77a942b230c74004aca2906db77eaf3f",
            "c9a5848c4fec43c39eca27da3b9f351e",
            "bad5a981e939415f8e404564e8f4ac2a",
            "e7b206c9aed140adb9cfd0aeab61d258",
            "772b43333c98428d997ea11fe8096731",
            "d2bdf3790b03437eaf5a014112ae0d76",
            "232ad7113fab4522a4edd8e80e4b15e4",
            "3aa6ed29a45a44eb9cfaade19b8af43e",
            "d2ec137769c54dc38469ca1bba8f0809",
            "e069d93550a44492ac741c66461aacf9",
            "76727604503f4217ad482b10f88f2dd1",
            "6937fdce33d940c09fdda69dd0dc7b9d",
            "d6f79736397344438c5fa905abdec69e",
            "8107738c03bd4970861460d868d4c300",
            "cf4d7c1070e649c5ad04fdcbfe48cbb2",
            "787a792e2cec449294db6e8a428be186",
            "c4391f9ef18140bea154cfb1035ce974",
            "a2c125cb343c41829e5404e73658250a",
            "78158c4c10074bd38d3c73b1e04b1ab8",
            "0fe9a1c3c9814bfb817f1d1cbeae3f04",
            "26b0d1dbdaf645d28b7eaac2d2d923b0",
            "5de6998ffabc4ba5b73fafdc0b4ffee1",
            "6f392d5b548b4c2693f283a850f6d952",
            "4418d4774e4449f197fc614404dd49a1",
            "7f96f1b791e74b9fab331df15d8b9ad3",
            "c163db1b52994e318ed80cc93082e559",
            "c77de697fbd44432a918608f8fce42c7",
            "8aa2a040e9464b2383d93804b4b81071",
            "e8b84922d40548319b4e387fbd88ee2a",
            "f4ab8ce4242b4993be21a4ae136db94e"
          ]
        },
        "id": "AcP5k8f_TCuc",
        "outputId": "4c7b566b-f1c3-436d-ec1f-4a42a5ccdf11"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7cbbe9e549bd49159be82cc2d5bba379"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3aebbeacf5e04e13851161c911f25169"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cada55b815714a13b94d20d58271061f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a5612c45a3e42548c220368f036174a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4292d1ae31be4a22bec049849b393d25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc97d12f3e814cc2a437cc5ad69699e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3def84fdcf5b471695488cef56714e02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdee1c16a95c403c80b6c2f2fba17c79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6485864cad0b47c8b67e45a11fecd7d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2ec137769c54dc38469ca1bba8f0809"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0fe9a1c3c9814bfb817f1d1cbeae3f04"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index import ServiceContext\n",
        "from llama_index.embeddings import LangchainEmbedding\n",
        "\n",
        "embed_model = LangchainEmbedding(\n",
        "  HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "7bd9e18eb2c348f8949811e1a242a7c1",
            "155476404cda4347bdcb948827ffe309",
            "983e589cf3134df98b9cdf84a7bb8d29",
            "0ecd28eafb8943aab07b8f9f7d1767da",
            "6caf33a8567f4c818b0ed9ff1a10e4bd",
            "53f0f0c8af584db0a2c1fea9f4b4c9f9",
            "3a2dafff10814f67a0ff42823ed0f89b",
            "e75a4316c2834663ba59d1624df86f9a",
            "d731d9f70aeb437298dd24e968a2c19a",
            "7d812a71e5c441c88f7061c55ca33ba9",
            "a9777758358041efbcf41daa738df0cd",
            "ad87365f8e4846568932b206aa994eee",
            "4fcc4f722b5d43b7aadb4e936070be9c",
            "08e9f66a33d94b7b9f980f2163f184d0",
            "32aff72d3a7643c280547cd3d2250c22",
            "a0217e03e52f4c3b80b6be3972e4015d",
            "228b3349e73146a68a25bd225e342fb0",
            "faae43aa8c074283be7bc55631de6884",
            "5db2e952aa044ca9962db2200b126683",
            "23d8266addf4486ca3617e7a64ff5db7",
            "5de15bb3e56b4a9bb24c7b80dff8cf6f",
            "acb54cb6d612400282037daa0f7d286e",
            "e212c4bcec0f4edbbfb2e1d27a382424",
            "e751f565c48c45858af4901822aaaa70",
            "cb37b3dd008d40a890758a48702209de",
            "f85230f3fec241e086ad0e5129644761",
            "7f06a7e46e474c5d959be8215d34c113",
            "88754644da1d43af9c959acb5def0897",
            "8e77b1cab64c43689051d4ce76469d61",
            "e08aa031db1149d6a733bee403df8e9a",
            "9e5e37fb980e490094c08f9aebd2e493",
            "bf967e6723df4c608fd12fda9be8d2e4",
            "77505cbfa9a24754b20a88118e223007",
            "6cf91330c0f740be9b984c3b0d16854a",
            "3ea2eef481f64176ad195ede7c1d0db4",
            "b543ed67fb0b47e9a7990214dfbe6457",
            "d17172d8c3784b45a5311d6fd79b4f47",
            "104a156af21048b09d8fb8bcd2d1e529",
            "b6e19febd35040a693cad645a40b5673",
            "eef0f9396a97492db8c02094ba9d17c3",
            "f02de5225ecc4c198c7da7a2136c9a47",
            "64e50042a9124a94af05c47b0c91f5a8",
            "3594d498c6054721a9b470dc03c357ac",
            "1b4387d5eb7b47508ca909600bdc863f",
            "27d97baa684e4372ac6ddb565310a678",
            "4740ae97df8f4539978ccac20bd01009",
            "4553b20d654243c89c490e7160d7b019",
            "b9fce70d534840bdbaf5f3cd12e9c4ad",
            "fdd170312e0c403899c22bc8fcfddba8",
            "731c89aef8d840bdbd27287da37e6832",
            "22ff04166293409a8eaf9fe3230a8781",
            "63fbfd3180084e69a3cd2f6787e65bf2",
            "9056e8a331a1406aa3bbd5d843b31c9a",
            "1b25f69b3a2e429089f58b65a4c9ad86",
            "d5e429ded0bf4d479d844a030f52cec1",
            "ba49cfb7b61a488da1da7df01ff84a2f",
            "545ddf85ff9c4500b95c1642f2498c6f",
            "8c5ab7ad4aaf44f99ed9bd8b97ec1290",
            "96f7ddbfe4d641c89dd97878c2f7aa75",
            "5d95bb888b6d4e839b8e82104cc10b58",
            "588398204dfd4533af06587f4f372b8e",
            "a50bc4918d7c4d2abd5511aeace5d2ae",
            "0c791822afe944b486285b380a9cff0b",
            "f7578bbff24f48a0860a82512185166b",
            "1644898f5b9241b8bda8d3b9fe690b27",
            "442d80fdeb5047c6a9f83acb18d0f3f6",
            "1f7f39ba00dc443fb1513a1bbeb96594",
            "57b09635215943bfb0563c791d9797e4",
            "83c01cb8b0fb45abab66c2a2fe4e48ee",
            "56048865724441e3afcdcab2db05bc34",
            "23e60f766a8b48aaa4fdb4b8b17775a7",
            "8dda43013c1942cfabcbeb2aa3ba9a1e",
            "c6ad5b0c5ee7435595015da8aab8a9e3",
            "a22eb13d7d824f8990a96ea4983dc9ff",
            "0d549b393fcc42cba337e5e3b3684719",
            "dc6c1ab08a49427a82a41c9338881dec",
            "e5489d19fe934321b728d538299666ab",
            "c64a4de49b92439d89e87079899b9727",
            "e111bdec0c0a44009090cb4e4ba27a1f",
            "e917a82641b745f1abc7da6fcf9ae82a",
            "8d08c8fb1ee94cd3809715a672e30299",
            "173ce66019b2437684040c3810f92f9e",
            "1ea5f982d6784fec889e0b07eb6f2cb6",
            "a21673aed82a4d169c8d6a92f4354acd",
            "b31d5eddd2a84606a57f085a6b948ae3",
            "31df46a871bd4770adc513726fe6a27b",
            "8ddd001fca7d4bdb9220ecf8c9185899",
            "07af7b4353f848d785e80af2fe5dbcf1",
            "9c0d98d7696d42d7a904db8c9136266b",
            "ad7aea75b60145c798f0ed55948c1533",
            "31bda88745b942e49d21a43c824589cc",
            "19e1d58bb441401f83136d049cf08264",
            "6dee69b6333e4462a30f98f174f41b75",
            "7dd183a012a94cabb8bc917819539dab",
            "d0f17f57ee1e4df4b4b4770c8981a7eb",
            "ee9096b7be6a43dd8e33595e1a4b0e80",
            "d46304da37c243faa159f008acbb058e",
            "570e6e2c317d4a04a6e3c25f0488cbcd",
            "7e2a90712afa4687a85f2c343595a6fd",
            "4e0e92deaccb4a28bac2b96aa8365a29",
            "e8e014e09b2549108002c9777ba3e628",
            "b430b09dd8184fc8a62992c5427caf0f",
            "511a5d77019042b1aa9f9a2631b6da1a",
            "76eef1897f3e422eb4c00631d92955cf",
            "9be4914e5cfa4f54a4d5ac19018deb8c",
            "505019e0f70141ef8b9ff40800fae0ec",
            "3f809021de5d4a82908d5e65b71b358c",
            "140e288669c14d1e93c3cba403310a0c",
            "e07e002ddbf6450791a866f434794b77",
            "b6bc9de3b8994b0ea9a1851f6ccb7894",
            "a301f58049e44206803949c03151cc3c",
            "d754e5e06fbf4b4091af56e97d3abc88",
            "2d63f13124024f4dafd17af590b16dd6",
            "2790bd7501e741fba96e489ab2bc7ba5",
            "b341540b8aaf4c59925d6c34c3506751",
            "00d5bb7967974585a861fe943b00d6e5",
            "00f993bcb1fe4fb3ba2df34f024daab0",
            "59a44eb97ff046cda2a2cb1879d4a6d4",
            "1c6d211edd834529b0a030ae5affcaee",
            "84e28c346c8840bbbc9a27da72f5130e",
            "67da5cb7da714ab59323f2ceb8213f9f"
          ]
        },
        "id": "RMexJLkgTCqe",
        "outputId": "7bee3c9f-6e24-4b56-c673-717b136b10ea"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bd9e18eb2c348f8949811e1a242a7c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad87365f8e4846568932b206aa994eee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e212c4bcec0f4edbbfb2e1d27a382424"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6cf91330c0f740be9b984c3b0d16854a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27d97baa684e4372ac6ddb565310a678"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba49cfb7b61a488da1da7df01ff84a2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f7f39ba00dc443fb1513a1bbeb96594"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c64a4de49b92439d89e87079899b9727"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c0d98d7696d42d7a904db8c9136266b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e0e92deaccb4a28bac2b96aa8365a29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a301f58049e44206803949c03151cc3c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "id": "IVyr4Xx0TCj0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
      ],
      "metadata": {
        "id": "gsllQ2gbTCgM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "MgEpOqb9U7np"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is correlation?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2PhJNacU7fI",
        "outputId": "ab2e6091-db02-46f3-dfe7-3bdff9d0a868"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation is the statistical technique that can show whether and how strongly pairs of variables are related. For example, height and weight are related; taller people tend to be heavier than shorter people. The relationship isn't perfect. People of the same height vary in weight, and you can easily think of two people you know where the shorter one is heavier than the taller one. Nonetheless, the average weight of people 5'5'' is less than the average weight of people 5'6'', and their average weight is less than that of people 5'7'', etc. Correlation can tell you just how much of the variation in people's weights is related to their heights.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is RNN?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dalodXtIbP3P",
        "outputId": "7883716b-1f25-4f61-856d-a983abbe9617"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN stands for Recurrent Neural Network. It is a type of artificial neural network that is designed to analyze and recognize patterns in sequences of data. Unlike feedforward neural networks, which only process the input data once and then discard it, RNNs have an internal memory that allows them to remember information from previous inputs, enabling them to make more informed predictions. This makes them particularly useful for tasks such as language modeling, speech recognition, and time series forecasting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"Tell me about pizza?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNtm7a0_biYa",
        "outputId": "29056186-6f22-41d8-b91e-3b96cb9eef51"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pizza is a delicious Italian dish consisting of a flat bread topped with various ingredients such as cheese, tomato sauce, vegetables, and meats. It is often baked in an oven and served hot. There are many different types of pizza, including classic margherita, meat-lovers, and vegetarian options. Pizza is a popular food around the world and is often enjoyed as a casual meal or as part of a social gathering.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"Who is Narendra modi?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyy6VB7fbolB",
        "outputId": "a9385cd4-ff92-4892-b0a7-dbb4fbd8cc10"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Narendra Modi is the current Prime Minister of India. He was born on September 17, 1950, in Vadnagar, Gujarat, India. He is known for his leadership in the Bharatiya Janata Party (BJP) and his efforts to promote economic growth and development in India. Modi has been the Prime Minister of India since 2014, and has been instrumental in implementing various policies and initiatives aimed at improving the lives of Indians and promoting India as a global economic powerhouse.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vDBApGE8dLMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-WF0mzCwdLJh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}